{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "\n",
    "def fmt_items(lines,max_lines=0):\n",
    "    max_width=max([len(line)for line in lines])\n",
    "    empty =' '*max_width\n",
    "    lines = [line.ljust(max_width)for line in lines]\n",
    "    lines += [empty]*(max_lines - len(lines))\n",
    "    return lines\n",
    "    \n",
    "def pp (*list):\n",
    "    lines = [ str(item).split('\\n') for item in list]\n",
    "    max_lines=max([len(item)for  item in lines])\n",
    "    lines = [fmt_items(item,max_lines=max_lines)for item in lines]\n",
    "    lines_t= np.array(lines).T\n",
    "    print('\\n'.join([' '.join(line) for  line in lines_t]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==GRAD_CHECK== at (0,)  num = 6.000000000039306 anal = 6.0\n",
      "Gradient check passed!\n",
      "==GRAD_CHECK== at (0,)  num = 1.0000000000065512 anal = 1.0\n",
      "==GRAD_CHECK== at (1,)  num = 1.0000000000065512 anal = 1.0\n",
      "Gradient check passed!\n",
      "==GRAD_CHECK== at (0, 0)  num = 1.0000000000065512 anal = 1.0\n",
      "==GRAD_CHECK== at (0, 1)  num = 1.0000000000065512 anal = 1.0\n",
      "==GRAD_CHECK== at (1, 0)  num = 1.0000000000065512 anal = 1.0\n",
      "==GRAD_CHECK== at (1, 1)  num = 1.0000000000065512 anal = 1.0\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(array_sum(np.array([5.0, 2.0]))[0]-array_sum(np.array([1.0, 2.0]))[0])/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max = [2.06106005e-09 4.53978686e-05 9.99954600e-01]\n",
      "[2.06106005e-09 4.53978686e-05 9.99954600e-01]\n",
      "soft max = [1. 0. 0.]\n",
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import linear_classifer \n",
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "print(probs)\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "print(probs)\n",
    "# assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max = [4.50940412e-05 6.69254912e-03 9.93262357e-01]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7e7aa37f4ed7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[1;34m(probs, target_index)\u001b[0m\n\u001b[0;32m     36\u001b[0m     '''\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mpp\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'log = '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "    \n",
    "linear_classifer.cross_entropy_loss(probs, np.array(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = linear_classifer.softmax(np.array([1, 0, 0]))\n",
    "linear_classifer.cross_entropy_loss(probs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [1. 0. 0.] 0\n",
      "soft max = [0.57611688 0.21194156 0.21194156]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6b65fea27065>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloss1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mloss2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss,grad = '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36msoftmax_with_cross_entropy\u001b[1;34m(predictions, target_index)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0msoftmax_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmax_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[1;34m(probs, target_index)\u001b[0m\n\u001b[0;32m     37\u001b[0m     '''\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# log = - np.log(probs[target_index])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mpp\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'log = '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "loss, grad= linear_classifer.softmax_with_cross_entropy(np.array([1., 0., 0.]), 0)\n",
    "loss1, grad1= linear_classifer.softmax_with_cross_entropy(np.array([3, 0, 0]), 0)\n",
    "loss2, grad2 = linear_classifer.softmax_with_cross_entropy(np.array([-1, 0, 0]), 0)\n",
    "print((loss1-loss2)/4)\n",
    "print('loss,grad = ',loss, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [1. 0. 0.] 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2837c8be80ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO Implement combined function orsoftmax and cross entropy and produces gradient #-2.0491237621641485\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcheck_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\gradient_check.py\u001b[0m in \u001b[0;36mcheck_gradient\u001b[1;34m(f, x, delta, tol)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0morig_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mfx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalytic_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Functions shouldn't modify input variables\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-2837c8be80ff>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO Implement combined function orsoftmax and cross entropy and produces gradient #-2.0491237621641485\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcheck_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36msoftmax_with_cross_entropy\u001b[1;34m(predictions, target_index)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0msoftmax_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmax_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[1;34m(probs, target_index)\u001b[0m\n\u001b[0;32m     37\u001b[0m     '''\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;31m# pp ('log = ',log)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "# TODO Implement combined function orsoftmax and cross entropy and produces gradient #-2.0491237621641485\n",
    "\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([ 1,0,0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[ 1.  2. -1.  1.]]\n",
      "targets =  [[2]]\n",
      "loss , grand (prediction) =  3.5797242232074917 [[ 0.20603191  0.56005279 -0.97211661  0.20603191]] \n",
      "                                                                                                    \n",
      "loss , grand (prediction) =  3.5797283438783922 [[ 0.20603518  0.56005049 -0.97211673  0.20603106]] \n",
      "                                                                                                    \n",
      "loss , grand (prediction) =  3.5797201026020242 [[ 0.20602864  0.5600551  -0.9721165   0.20603276]] \n",
      "                                                                                                    \n",
      "==GRAD_CHECK== at (0, 0)  num = 0.20603190920009948 anal = 0.20603190919001857\n",
      "loss , grand (prediction) =  3.579735424312667 [[ 0.2060296   0.56005772 -0.97211693  0.2060296 ]] \n",
      "                                                                                                   \n",
      "loss , grand (prediction) =  3.5797130222008735 [[ 0.20603422  0.56004787 -0.9721163   0.20603422]] \n",
      "                                                                                                    \n",
      "==GRAD_CHECK== at (0, 1)  num = 0.5600527948401712 anal = 0.5600527948339517\n",
      "loss , grand (prediction) =  3.5797047808806486 [[ 0.20603179  0.56005248 -0.97211607  0.20603179]] \n",
      "                                                                                                    \n",
      "loss , grand (prediction) =  3.5797436655451773 [[ 0.20603202  0.56005311 -0.97211716  0.20603202]] \n",
      "                                                                                                    \n",
      "==GRAD_CHECK== at (0, 2)  num = -0.9721166132181657 anal = -0.9721166132139888\n",
      "loss , grand (prediction) =  3.5797283438783922 [[ 0.20603106  0.56005049 -0.97211673  0.20603518]] \n",
      "                                                                                                    \n",
      "loss , grand (prediction) =  3.5797201026020242 [[ 0.20603276  0.5600551  -0.9721165   0.20602864]] \n",
      "                                                                                                    \n",
      "==GRAD_CHECK== at (0, 3)  num = 0.20603190920009948 anal = 0.20603190919001857\n",
      "Gradient check passed!\n",
      "predictions = [[ 2. -1. -1.  1.]\n",
      " [ 0.  1.  1.  1.]\n",
      " [ 1.  2. -1.  2.]]\n",
      "targets =  [[3]\n",
      " [3]\n",
      " [2]]\n",
      "loss , grand (prediction) =  3.3006203358964723 [[ 0.19515646  0.00971627  0.00971627 -0.26153928]  \n",
      "                                                 [ 0.02641156  0.07179405  0.07179405 -0.26153928]  \n",
      "                                                 [ 0.07179405  0.19515646 -0.32361707  0.19515646]] \n",
      "loss , grand (prediction) =  3.300624239057141 [[ 0.1951596   0.00971623  0.00971623 -0.26153956]  \n",
      "                                                [ 0.02641145  0.07179377  0.07179377 -0.26153956]  \n",
      "                                                [ 0.07179377  0.1951557  -0.3236171   0.1951557 ]] \n",
      "loss , grand (prediction) =  3.300616432798632 [[ 0.19515332  0.00971631  0.00971631 -0.261539  ]  \n",
      "                                                [ 0.02641166  0.07179433  0.07179433 -0.261539  ]  \n",
      "                                                [ 0.07179433  0.19515722 -0.32361703  0.19515722]] \n",
      "==GRAD_CHECK== at (0, 0)  num = 0.19515646273449147 anal = 0.19515646271894127\n",
      "loss , grand (prediction) =  3.3006205302237603 [[ 0.19515642  0.00971646  0.00971627 -0.2615393 ]  \n",
      "                                                 [ 0.02641155  0.07179404  0.07179404 -0.2615393 ]  \n",
      "                                                 [ 0.07179404  0.19515642 -0.32361707  0.19515642]] \n",
      "loss , grand (prediction) =  3.300620141573034 [[ 0.1951565   0.00971608  0.00971627 -0.26153927]  \n",
      "                                                [ 0.02641156  0.07179406  0.07179406 -0.26153927]  \n",
      "                                                [ 0.07179406  0.1951565  -0.32361706  0.1951565 ]] \n",
      "==GRAD_CHECK== at (0, 1)  num = 0.009716268156712005 anal = 0.00971626815181842\n",
      "loss , grand (prediction) =  3.3006205302237603 [[ 0.19515642  0.00971627  0.00971646 -0.2615393 ]  \n",
      "                                                 [ 0.02641155  0.07179404  0.07179404 -0.2615393 ]  \n",
      "                                                 [ 0.07179404  0.19515642 -0.32361707  0.19515642]] \n",
      "loss , grand (prediction) =  3.300620141573034 [[ 0.1951565   0.00971627  0.00971608 -0.26153927]  \n",
      "                                                [ 0.02641156  0.07179406  0.07179406 -0.26153927]  \n",
      "                                                [ 0.07179406  0.1951565  -0.32361706  0.1951565 ]] \n",
      "==GRAD_CHECK== at (0, 2)  num = 0.009716268156712005 anal = 0.00971626815181842\n",
      "loss , grand (prediction) =  3.300615105124143 [[ 0.19515618  0.00971625  0.00971625 -0.26153795]  \n",
      "                                                [ 0.02641152  0.07179395  0.07179395 -0.26153939]  \n",
      "                                                [ 0.07179395  0.19515618 -0.32361708  0.19515618]] \n",
      "loss , grand (prediction) =  3.3006255666954583 [[ 0.19515674  0.00971628  0.00971628 -0.26154062]  \n",
      "                                                 [ 0.02641159  0.07179415  0.07179415 -0.26153918]  \n",
      "                                                 [ 0.07179415  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (0, 3)  num = -0.2615392828864138 anal = -0.2615392828872938\n",
      "loss , grand (prediction) =  3.300620864132718 [[ 0.19515636  0.00971626  0.00971626 -0.26153932]  \n",
      "                                                [ 0.02641207  0.07179401  0.07179401 -0.26153932]  \n",
      "                                                [ 0.07179401  0.19515636 -0.32361707  0.19515636]] \n",
      "loss , grand (prediction) =  3.3006198076705124 [[ 0.19515657  0.00971627  0.00971627 -0.26153924]  \n",
      "                                                 [ 0.02641104  0.07179409  0.07179409 -0.26153924]  \n",
      "                                                 [ 0.07179409  0.19515657 -0.32361706  0.19515657]] \n",
      "==GRAD_CHECK== at (1, 0)  num = 0.02641155514293558 anal = 0.026411555157523366\n",
      "loss , grand (prediction) =  3.30062177179081 [[ 0.19515618  0.00971625  0.00971625 -0.26153939]  \n",
      "                                               [ 0.02641152  0.07179538  0.07179395 -0.26153939]  \n",
      "                                               [ 0.07179395  0.19515618 -0.32361708  0.19515618]] \n",
      "loss , grand (prediction) =  3.300618900028791 [[ 0.19515674  0.00971628  0.00971628 -0.26153918]  \n",
      "                                                [ 0.02641159  0.07179272  0.07179415 -0.26153918]  \n",
      "                                                [ 0.07179415  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (1, 1)  num = 0.07179405047130771 anal = 0.07179405044603954\n",
      "loss , grand (prediction) =  3.30062177179081 [[ 0.19515618  0.00971625  0.00971625 -0.26153939]  \n",
      "                                               [ 0.02641152  0.07179395  0.07179538 -0.26153939]  \n",
      "                                               [ 0.07179395  0.19515618 -0.32361708  0.19515618]] \n",
      "loss , grand (prediction) =  3.300618900028791 [[ 0.19515674  0.00971628  0.00971628 -0.26153918]  \n",
      "                                                [ 0.02641159  0.07179415  0.07179272 -0.26153918]  \n",
      "                                                [ 0.07179415  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (1, 2)  num = 0.07179405047130771 anal = 0.07179405044603954\n",
      "loss , grand (prediction) =  3.300615105124143 [[ 0.19515618  0.00971625  0.00971625 -0.26153939]  \n",
      "                                                [ 0.02641152  0.07179395  0.07179395 -0.26153795]  \n",
      "                                                [ 0.07179395  0.19515618 -0.32361708  0.19515618]] \n",
      "loss , grand (prediction) =  3.3006255666954583 [[ 0.19515674  0.00971628  0.00971628 -0.26153918]  \n",
      "                                                 [ 0.02641159  0.07179415  0.07179415 -0.26154062]  \n",
      "                                                 [ 0.07179415  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (1, 3)  num = -0.2615392828864138 anal = -0.2615392828872938\n",
      "loss , grand (prediction) =  3.30062177179081 [[ 0.19515618  0.00971625  0.00971625 -0.26153939]  \n",
      "                                               [ 0.02641152  0.07179395  0.07179395 -0.26153939]  \n",
      "                                               [ 0.07179538  0.19515618 -0.32361708  0.19515618]] \n",
      "loss , grand (prediction) =  3.300618900028791 [[ 0.19515674  0.00971628  0.00971628 -0.26153918]  \n",
      "                                                [ 0.02641159  0.07179415  0.07179415 -0.26153918]  \n",
      "                                                [ 0.07179272  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (2, 0)  num = 0.07179405047130771 anal = 0.07179405044603954\n",
      "loss , grand (prediction) =  3.300624239057141 [[ 0.1951557   0.00971623  0.00971623 -0.26153956]  \n",
      "                                                [ 0.02641145  0.07179377  0.07179377 -0.26153956]  \n",
      "                                                [ 0.07179377  0.1951596  -0.3236171   0.1951557 ]] \n",
      "loss , grand (prediction) =  3.300616432798632 [[ 0.19515722  0.00971631  0.00971631 -0.261539  ]  \n",
      "                                                [ 0.02641166  0.07179433  0.07179433 -0.261539  ]  \n",
      "                                                [ 0.07179433  0.19515332 -0.32361703  0.19515722]] \n",
      "==GRAD_CHECK== at (2, 1)  num = 0.19515646273449147 anal = 0.19515646271894127\n",
      "loss , grand (prediction) =  3.300613863557093 [[ 0.19515642  0.00971627  0.00971627 -0.2615393 ]  \n",
      "                                                [ 0.02641155  0.07179404  0.07179404 -0.2615393 ]  \n",
      "                                                [ 0.07179404  0.19515642 -0.32361687  0.19515642]] \n",
      "loss , grand (prediction) =  3.300626808239701 [[ 0.1951565   0.00971627  0.00971627 -0.26153927]  \n",
      "                                                [ 0.02641156  0.07179406  0.07179406 -0.26153927]  \n",
      "                                                [ 0.07179406  0.1951565  -0.32361726  0.1951565 ]] \n",
      "==GRAD_CHECK== at (2, 2)  num = -0.3236170651899073 anal = -0.3236170651815149\n",
      "loss , grand (prediction) =  3.300624239057141 [[ 0.1951557   0.00971623  0.00971623 -0.26153956]  \n",
      "                                                [ 0.02641145  0.07179377  0.07179377 -0.26153956]  \n",
      "                                                [ 0.07179377  0.1951557  -0.3236171   0.1951596 ]] \n",
      "loss , grand (prediction) =  3.300616432798632 [[ 0.19515722  0.00971631  0.00971631 -0.261539  ]  \n",
      "                                                [ 0.02641166  0.07179433  0.07179433 -0.261539  ]  \n",
      "                                                [ 0.07179433  0.19515722 -0.32361703  0.19515332]] \n",
      "==GRAD_CHECK== at (2, 3)  num = 0.19515646273449147 anal = 0.19515646271894127\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "pp('predictions =',predictions)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "pp('targets = ',target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "print('predictions =',predictions)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "print('targets = ',target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss , grand (prediction) =  2.1851824526038124 [[ 0.11245721 -0.48478057]  \n",
      "                                                 [ 0.0413707   0.33095266]] \n",
      "[[-0.11245721  0.48478057] \n",
      " [-0.07108652  0.81573323] \n",
      " [ 0.15382791 -0.15382791]]\n",
      "loss , grand (prediction) =  2.1851824526038124 [[ 0.11245721 -0.48478057]  \n",
      "                                                 [ 0.0413707   0.33095266]] \n",
      "loss , grand (prediction) =  2.1851802034795016 [[ 0.11245522 -0.48478054]  \n",
      "                                                 [ 0.04137079  0.33095453]] \n",
      "loss , grand (prediction) =  2.1851847017680486 [[ 0.11245921 -0.48478061]  \n",
      "                                                 [ 0.0413706   0.33095079]] \n",
      "==GRAD_CHECK== at (0, 0)  num = -0.11245721367458826 anal = -0.11245721367093255\n",
      "loss , grand (prediction) =  2.1851921482182326 [[ 0.11245725 -0.48478087]  \n",
      "                                                 [ 0.04137071  0.33095291]] \n",
      "loss , grand (prediction) =  2.1851727569953874 [[ 0.11245718 -0.48478027]  \n",
      "                                                 [ 0.04137068  0.33095241]] \n",
      "==GRAD_CHECK== at (0, 1)  num = 0.4847805711305497 anal = 0.48478057113584405\n",
      "loss , grand (prediction) =  2.185181030903233 [[ 0.11245512 -0.48478055]  \n",
      "                                                [ 0.04137158  0.33095384]] \n",
      "loss , grand (prediction) =  2.1851838743639025 [[ 0.1124593  -0.48478059]  \n",
      "                                                 [ 0.04136981  0.33095148]] \n",
      "==GRAD_CHECK== at (1, 0)  num = -0.07108651673970456 anal = -0.0710865167499724\n",
      "loss , grand (prediction) =  2.1851987673045965 [[ 0.11245538 -0.48478112]  \n",
      "                                                 [ 0.04137002  0.33095572]] \n",
      "loss , grand (prediction) =  2.1851661379753295 [[ 0.11245905 -0.48478002]  \n",
      "                                                 [ 0.04137137  0.3309496 ]] \n",
      "==GRAD_CHECK== at (1, 1)  num = 0.8157332316738318 anal = 0.8157332316797954\n",
      "loss , grand (prediction) =  2.1851855291880575 [[ 0.11245912 -0.48478062]  \n",
      "                                                 [ 0.0413714   0.3309501 ]] \n",
      "loss , grand (prediction) =  2.185179376071634 [[ 0.11245531 -0.48478052]  \n",
      "                                                [ 0.04137     0.33095522]] \n",
      "==GRAD_CHECK== at (2, 0)  num = 0.15382791058726752 anal = 0.15382791059189269\n",
      "loss , grand (prediction) =  2.185179376071634 [[ 0.11245531 -0.48478052]  \n",
      "                                                [ 0.04137     0.33095522]] \n",
      "loss , grand (prediction) =  2.1851855291880575 [[ 0.11245912 -0.48478062]  \n",
      "                                                 [ 0.0413714   0.3309501 ]] \n",
      "==GRAD_CHECK== at (2, 1)  num = -0.15382791058726752 anal = -0.1538279105918927\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "pp(dW)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of L2  (3, 2) 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "==GRAD_CHECK== at (0, 0)  num = 0.009999999999940612 anal = 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "==GRAD_CHECK== at (0, 1)  num = 0.02000000000022817 anal = 0.02\n",
      "enter of L2  (3, 2) 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "==GRAD_CHECK== at (1, 0)  num = -0.009999999999940612 anal = -0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "==GRAD_CHECK== at (1, 1)  num = 0.009999999999940612 anal = 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "==GRAD_CHECK== at (2, 0)  num = 0.009999999999940612 anal = 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "==GRAD_CHECK== at (2, 1)  num = 0.02000000000022817 anal = 0.02\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102666.15435186218\n",
      "Epoch 0, loss: 11.437980\n",
      "== W == 0.17624349407509313\n",
      "log loss =  102665.83476725097\n",
      "Epoch 1, loss: 11.437335\n",
      "== W == 0.17324946509479744\n",
      "log loss =  102665.51943287873\n",
      "Epoch 2, loss: 11.436703\n",
      "== W == 0.17030656280312612\n",
      "log loss =  102665.20827632082\n",
      "Epoch 3, loss: 11.436084\n",
      "== W == 0.16741389584116895\n",
      "log loss =  102664.90122679397\n",
      "Epoch 4, loss: 11.435476\n",
      "== W == 0.16457058893177176\n",
      "log loss =  102664.59821511057\n",
      "Epoch 5, loss: 11.434881\n",
      "== W == 0.16177578257281844\n",
      "log loss =  102664.29917363409\n",
      "Epoch 6, loss: 11.434297\n",
      "== W == 0.15902863273686396\n",
      "log loss =  102664.0040362362\n",
      "Epoch 7, loss: 11.433725\n",
      "== W == 0.15632831057697363\n",
      "log loss =  102663.71273825495\n",
      "Epoch 8, loss: 11.433164\n",
      "== W == 0.15367400213862537\n",
      "log loss =  102663.42521645449\n",
      "Epoch 9, loss: 11.432613\n",
      "== W == 0.15106490807753659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'THERE WERE 10 EPOCHS   !!!!!\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function \n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "'''THERE WERE 10 EPOCHS   !!!!!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.437980141859269, 11.43733544585312, 11.436703417823981, 11.436083805928662, 11.435476363380554, 11.434880848346916, 11.434297023848313, 11.433724657660111, 11.433163522216013, 11.432613394513575]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26ffc320>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "print(loss_history)\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102667.07378251232\n",
      "Epoch 0, loss: 11.438084\n",
      "== W == -0.14154594529651654\n",
      "log loss =  102663.88030036291\n",
      "Epoch 1, loss: 11.431914\n",
      "== W == -0.11774816524714836\n",
      "log loss =  102661.07950375811\n",
      "Epoch 2, loss: 11.426899\n",
      "== W == -0.09799123333119203\n",
      "log loss =  102658.6135801901\n",
      "Epoch 3, loss: 11.422821\n",
      "== W == -0.08159312001908484\n",
      "log loss =  102656.43558205798\n",
      "Epoch 4, loss: 11.419502\n",
      "== W == -0.06798528987406666\n",
      "log loss =  102654.50686999677\n",
      "Epoch 5, loss: 11.416799\n",
      "== W == -0.05669436627019166\n",
      "log loss =  102652.79525163464\n",
      "Epoch 6, loss: 11.414597\n",
      "== W == -0.04732657521886578\n",
      "log loss =  102651.27361187802\n",
      "Epoch 7, loss: 11.412802\n",
      "== W == -0.03955460935647236\n",
      "log loss =  102649.91889284196\n",
      "Epoch 8, loss: 11.411338\n",
      "== W == -0.03310657681850522\n",
      "log loss =  102648.71132432517\n",
      "Epoch 9, loss: 11.410142\n",
      "== W == -0.02775673159838126\n",
      "log loss =  102647.6338353205\n",
      "Epoch 10, loss: 11.409164\n",
      "== W == -0.023317716648616267\n",
      "log loss =  102646.67159757423\n",
      "Epoch 11, loss: 11.408364\n",
      "== W == -0.01963408527281437\n",
      "log loss =  102645.81166647981\n",
      "Epoch 12, loss: 11.407710\n",
      "== W == -0.01657689854387787\n",
      "log loss =  102645.04269455303\n",
      "Epoch 13, loss: 11.407173\n",
      "== W == -0.014039225715048988\n",
      "log loss =  102644.35469970728\n",
      "Epoch 14, loss: 11.406732\n",
      "== W == -0.011932400552237475\n",
      "log loss =  102643.73887545348\n",
      "Epoch 15, loss: 11.406370\n",
      "== W == -0.01018290921266553\n",
      "log loss =  102643.18743361192\n",
      "Epoch 16, loss: 11.406072\n",
      "== W == -0.008729804907816049\n",
      "log loss =  102642.69347258589\n",
      "Epoch 17, loss: 11.405827\n",
      "== W == -0.007522561389260702\n",
      "log loss =  102642.25086600512\n",
      "Epoch 18, loss: 11.405624\n",
      "== W == -0.006519291590916574\n",
      "log loss =  102641.85416781514\n",
      "Epoch 19, loss: 11.405457\n",
      "== W == -0.005685269860582345\n",
      "log loss =  102641.49853080764\n",
      "Epoch 20, loss: 11.405318\n",
      "== W == -0.004991706412058346\n",
      "log loss =  102641.17963626148\n",
      "Epoch 21, loss: 11.405203\n",
      "== W == -0.004414731196968277\n",
      "log loss =  102640.89363286247\n",
      "Epoch 22, loss: 11.405107\n",
      "== W == -0.003934551574160025\n",
      "log loss =  102640.63708344352\n",
      "Epoch 23, loss: 11.405028\n",
      "== W == -0.0035347541565294957\n",
      "log loss =  102640.40691836944\n",
      "Epoch 24, loss: 11.404961\n",
      "== W == -0.003201726224415568\n",
      "log loss =  102640.20039460732\n",
      "Epoch 25, loss: 11.404905\n",
      "== W == -0.0029241762695605827\n",
      "log loss =  102640.0150596915\n",
      "Epoch 26, loss: 11.404859\n",
      "== W == -0.0026927367089567135\n",
      "log loss =  102639.84871992441\n",
      "Epoch 27, loss: 11.404819\n",
      "== W == -0.0024996346981329626\n",
      "log loss =  102639.69941225999\n",
      "Epoch 28, loss: 11.404786\n",
      "== W == -0.002338419375185453\n",
      "log loss =  102639.56537940046\n",
      "Epoch 29, loss: 11.404758\n",
      "== W == -0.0022037358613758926\n",
      "log loss =  102639.44504770696\n",
      "Epoch 30, loss: 11.404734\n",
      "== W == -0.0020911379995770616\n",
      "log loss =  102639.33700758089\n",
      "Epoch 31, loss: 11.404714\n",
      "== W == -0.0019969331852626795\n",
      "log loss =  102639.23999601978\n",
      "Epoch 32, loss: 11.404697\n",
      "== W == -0.0019180537837559302\n",
      "log loss =  102639.15288109182\n",
      "Epoch 33, loss: 11.404683\n",
      "== W == -0.0018519505717866686\n",
      "log loss =  102639.07464810528\n",
      "Epoch 34, loss: 11.404670\n",
      "== W == -0.0017965044241439574\n",
      "log loss =  102639.0043872789\n",
      "Epoch 35, loss: 11.404659\n",
      "== W == -0.0017499531148696132\n",
      "log loss =  102638.94128274193\n",
      "Epoch 36, loss: 11.404650\n",
      "== W == -0.0017108306398976288\n",
      "log loss =  102638.88460271512\n",
      "Epoch 37, loss: 11.404642\n",
      "== W == -0.001677916913307395\n",
      "log loss =  102638.83369074023\n",
      "Epoch 38, loss: 11.404635\n",
      "== W == -0.001650196058200831\n",
      "log loss =  102638.7879578425\n",
      "Epoch 39, loss: 11.404629\n",
      "== W == -0.0016268218187256505\n",
      "log loss =  102638.74687552344\n",
      "Epoch 40, loss: 11.404624\n",
      "== W == -0.001607088872805788\n",
      "log loss =  102638.70996949358\n",
      "Epoch 41, loss: 11.404619\n",
      "== W == -0.0015904090347081219\n",
      "log loss =  102638.67681406431\n",
      "Epoch 42, loss: 11.404615\n",
      "== W == -0.0015762915101339226\n",
      "log loss =  102638.64702712867\n",
      "Epoch 43, loss: 11.404612\n",
      "== W == -0.0015643265102583985\n",
      "log loss =  102638.6202656668\n",
      "Epoch 44, loss: 11.404609\n",
      "== W == -0.001554171650175414\n",
      "log loss =  102638.59622172076\n",
      "Epoch 45, loss: 11.404606\n",
      "== W == -0.0015455406557829583\n",
      "log loss =  102638.57461878848\n",
      "Epoch 46, loss: 11.404604\n",
      "== W == -0.0015381939847831714\n",
      "log loss =  102638.55520859212\n",
      "Epoch 47, loss: 11.404601\n",
      "== W == -0.0015319310350814885\n",
      "log loss =  102638.53776818201\n",
      "Epoch 48, loss: 11.404600\n",
      "== W == -0.0015265836698650555\n",
      "log loss =  102638.52209733998\n",
      "Epoch 49, loss: 11.404598\n",
      "== W == -0.0015220108350168238\n",
      "log loss =  102638.50801625123\n",
      "Epoch 50, loss: 11.404597\n",
      "== W == -0.0015180940829342187\n",
      "log loss =  102638.49536341641\n",
      "Epoch 51, loss: 11.404595\n",
      "== W == -0.0015147338486381308\n",
      "log loss =  102638.48399377882\n",
      "Epoch 52, loss: 11.404594\n",
      "== W == -0.0015118463504139412\n",
      "log loss =  102638.47377704426\n",
      "Epoch 53, loss: 11.404593\n",
      "== W == -0.001509361009059637\n",
      "log loss =  102638.46459617378\n",
      "Epoch 54, loss: 11.404592\n",
      "== W == -0.0015072182979047747\n",
      "log loss =  102638.45634603093\n",
      "Epoch 55, loss: 11.404591\n",
      "== W == -0.0015053679507510632\n",
      "log loss =  102638.44893216781\n",
      "Epoch 56, loss: 11.404591\n",
      "== W == -0.0015037674673045137\n",
      "log loss =  102638.44226973568\n",
      "Epoch 57, loss: 11.404590\n",
      "== W == -0.001502380865960262\n",
      "log loss =  102638.43628250684\n",
      "Epoch 58, loss: 11.404590\n",
      "== W == -0.001501177642331498\n",
      "log loss =  102638.43090199676\n",
      "Epoch 59, loss: 11.404589\n",
      "== W == -0.001500131898983982\n",
      "log loss =  102638.42606667588\n",
      "Epoch 60, loss: 11.404589\n",
      "== W == -0.001499221617699732\n",
      "log loss =  102638.42172126181\n",
      "Epoch 61, loss: 11.404588\n",
      "== W == -0.0014984280504533125\n",
      "log loss =  102638.41781608392\n",
      "Epoch 62, loss: 11.404588\n",
      "== W == -0.0014977352093150122\n",
      "log loss =  102638.41430651286\n",
      "Epoch 63, loss: 11.404587\n",
      "== W == -0.0014971294388382947\n",
      "log loss =  102638.41115244782\n",
      "Epoch 64, loss: 11.404587\n",
      "== W == -0.0014965990572621936\n",
      "log loss =  102638.40831785658\n",
      "Epoch 65, loss: 11.404587\n",
      "== W == -0.0014961340551608914\n",
      "log loss =  102638.40577036198\n",
      "Epoch 66, loss: 11.404587\n",
      "== W == -0.0014957258420829334\n",
      "log loss =  102638.40348087077\n",
      "Epoch 67, loss: 11.404586\n",
      "== W == -0.0014953670333078544\n",
      "log loss =  102638.4014232401\n",
      "Epoch 68, loss: 11.404586\n",
      "== W == -0.0014950512701651854\n",
      "log loss =  102638.39957397798\n",
      "Epoch 69, loss: 11.404586\n",
      "== W == -0.0014947730684543746\n",
      "log loss =  102638.39791197423\n",
      "Epoch 70, loss: 11.404586\n",
      "== W == -0.0014945276904130677\n",
      "log loss =  102638.39641825866\n",
      "Epoch 71, loss: 11.404586\n",
      "== W == -0.0014943110364365592\n",
      "log loss =  102638.39507578406\n",
      "Epoch 72, loss: 11.404586\n",
      "== W == -0.00149411955337937\n",
      "log loss =  102638.39386923105\n",
      "Epoch 73, loss: 11.404586\n",
      "== W == -0.001493950156792552\n",
      "log loss =  102638.39278483269\n",
      "Epoch 74, loss: 11.404585\n",
      "== W == -0.001493800164884681\n",
      "log loss =  102638.39181021729\n",
      "Epoch 75, loss: 11.404585\n",
      "== W == -0.001493667242357053\n",
      "log loss =  102638.39093426667\n",
      "Epoch 76, loss: 11.404585\n",
      "== W == -0.001493549352564722\n",
      "log loss =  102638.39014698922\n",
      "Epoch 77, loss: 11.404585\n",
      "== W == -0.0014934447167064616\n",
      "log loss =  102638.38943940551\n",
      "Epoch 78, loss: 11.404585\n",
      "== W == -0.0014933517789562057\n",
      "log loss =  102638.3888034459\n",
      "Epoch 79, loss: 11.404585\n",
      "== W == -0.0014932691766234986\n",
      "log loss =  102638.38823185812\n",
      "Epoch 80, loss: 11.404585\n",
      "== W == -0.001493195714576219\n",
      "log loss =  102638.38771812455\n",
      "Epoch 81, loss: 11.404585\n",
      "== W == -0.001493130343281034\n",
      "log loss =  102638.38725638768\n",
      "Epoch 82, loss: 11.404585\n",
      "== W == -0.001493072139918829\n",
      "log loss =  102638.38684138336\n",
      "Epoch 83, loss: 11.404585\n",
      "== W == -0.0014930202921177153\n",
      "log loss =  102638.38646838043\n",
      "Epoch 84, loss: 11.404585\n",
      "== W == -0.0014929740839176925\n",
      "log loss =  102638.38613312694\n",
      "Epoch 85, loss: 11.404585\n",
      "== W == -0.0014929328836409362\n",
      "log loss =  102638.38583180158\n",
      "Epoch 86, loss: 11.404585\n",
      "== W == -0.0014928961333916862\n",
      "log loss =  102638.38556096985\n",
      "Epoch 87, loss: 11.404585\n",
      "== W == -0.001492863339952342\n",
      "log loss =  102638.3853175451\n",
      "Epoch 88, loss: 11.404585\n",
      "== W == -0.0014928340668772934\n",
      "log loss =  102638.38509875325\n",
      "Epoch 89, loss: 11.404585\n",
      "== W == -0.0014928079276162283\n",
      "log loss =  102638.384902101\n",
      "Epoch 90, loss: 11.404585\n",
      "== W == -0.0014927845795236494\n",
      "log loss =  102638.38472534751\n",
      "Epoch 91, loss: 11.404585\n",
      "== W == -0.001492763718632521\n",
      "log loss =  102638.38456647885\n",
      "Epoch 92, loss: 11.404585\n",
      "== W == -0.0014927450750879226\n",
      "log loss =  102638.38442368494\n",
      "Epoch 93, loss: 11.404585\n",
      "== W == -0.001492728409151601\n",
      "log loss =  102638.38429533894\n",
      "Epoch 94, loss: 11.404585\n",
      "== W == -0.0014927135077012783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102638.3841799787\n",
      "Epoch 95, loss: 11.404585\n",
      "== W == -0.001492700181159391\n",
      "log loss =  102638.38407629007\n",
      "Epoch 96, loss: 11.404585\n",
      "== W == -0.00149268826079505\n",
      "log loss =  102638.38398309192\n",
      "Epoch 97, loss: 11.404585\n",
      "== W == -0.0014926775963509016\n",
      "log loss =  102638.3838993227\n",
      "Epoch 98, loss: 11.404585\n",
      "== W == -0.0014926680539534838\n",
      "log loss =  102638.38382402828\n",
      "Epoch 99, loss: 11.404585\n",
      "== W == -0.0014926595142708918\n",
      "log loss =  102638.38375635108\n",
      "Epoch 100, loss: 11.404585\n",
      "== W == -0.0014926518708869353\n",
      "log loss =  102638.38369552039\n",
      "Epoch 101, loss: 11.404585\n",
      "== W == -0.0014926450288646892\n",
      "log loss =  102638.38364084344\n",
      "Epoch 102, loss: 11.404585\n",
      "== W == -0.0014926389034763288\n",
      "log loss =  102638.38359169757\n",
      "Epoch 103, loss: 11.404585\n",
      "== W == -0.0014926334190787547\n",
      "log loss =  102638.38354752319\n",
      "Epoch 104, loss: 11.404585\n",
      "== W == -0.0014926285081176232\n",
      "log loss =  102638.38350781729\n",
      "Epoch 105, loss: 11.404585\n",
      "== W == -0.00149262411024419\n",
      "log loss =  102638.38347212775\n",
      "Epoch 106, loss: 11.404585\n",
      "== W == -0.0014926201715317285\n",
      "log loss =  102638.38344004826\n",
      "Epoch 107, loss: 11.404585\n",
      "== W == -0.0014926166437796318\n",
      "log loss =  102638.38341121355\n",
      "Epoch 108, loss: 11.404585\n",
      "== W == -0.0014926134838951733\n",
      "log loss =  102638.38338529543\n",
      "Epoch 109, loss: 11.404585\n",
      "== W == -0.0014926106533437143\n",
      "log loss =  102638.38336199877\n",
      "Epoch 110, loss: 11.404585\n",
      "== W == -0.0014926081176595237\n",
      "log loss =  102638.3833410584\n",
      "Epoch 111, loss: 11.404585\n",
      "== W == -0.0014926058460104064\n",
      "log loss =  102638.38332223598\n",
      "Epoch 112, loss: 11.404585\n",
      "== W == -0.0014926038108098389\n",
      "log loss =  102638.38330531723\n",
      "Epoch 113, loss: 11.404585\n",
      "== W == -0.0014926019873713545\n",
      "log loss =  102638.3832901096\n",
      "Epoch 114, loss: 11.404585\n",
      "== W == -0.0014926003536004772\n",
      "log loss =  102638.38327643998\n",
      "Epoch 115, loss: 11.404585\n",
      "== W == -0.0014925988897198715\n",
      "log loss =  102638.38326415284\n",
      "Epoch 116, loss: 11.404585\n",
      "== W == -0.0014925975780241596\n",
      "log loss =  102638.3832531083\n",
      "Epoch 117, loss: 11.404585\n",
      "== W == -0.0014925964026610609\n",
      "log loss =  102638.38324318068\n",
      "Epoch 118, loss: 11.404585\n",
      "== W == -0.0014925953494359565\n",
      "log loss =  102638.38323425705\n",
      "Epoch 119, loss: 11.404585\n",
      "== W == -0.001492594405637337\n",
      "log loss =  102638.3832262358\n",
      "Epoch 120, loss: 11.404585\n",
      "== W == -0.0014925935598808539\n",
      "log loss =  102638.3832190257\n",
      "Epoch 121, loss: 11.404585\n",
      "== W == -0.0014925928019699354\n",
      "log loss =  102638.3832125447\n",
      "Epoch 122, loss: 11.404585\n",
      "== W == -0.0014925921227712407\n",
      "log loss =  102638.38320671907\n",
      "Epoch 123, loss: 11.404585\n",
      "== W == -0.0014925915141031816\n",
      "log loss =  102638.38320148252\n",
      "Epoch 124, loss: 11.404585\n",
      "== W == -0.0014925909686363606\n",
      "log loss =  102638.38319677545\n",
      "Epoch 125, loss: 11.404585\n",
      "== W == -0.001492590479804434\n",
      "log loss =  102638.38319254432\n",
      "Epoch 126, loss: 11.404585\n",
      "== W == -0.0014925900417243516\n",
      "log loss =  102638.38318874108\n",
      "Epoch 127, loss: 11.404585\n",
      "== W == -0.0014925896491250713\n",
      "log loss =  102638.38318532232\n",
      "Epoch 128, loss: 11.404585\n",
      "== W == -0.0014925892972836655\n",
      "log loss =  102638.38318224926\n",
      "Epoch 129, loss: 11.404585\n",
      "== W == -0.0014925889819681717\n",
      "log loss =  102638.38317948692\n",
      "Epoch 130, loss: 11.404585\n",
      "== W == -0.0014925886993864689\n",
      "log loss =  102638.38317700385\n",
      "Epoch 131, loss: 11.404585\n",
      "== W == -0.0014925884461403827\n",
      "log loss =  102638.38317477185\n",
      "Epoch 132, loss: 11.404585\n",
      "== W == -0.0014925882191847938\n",
      "log loss =  102638.3831727655\n",
      "Epoch 133, loss: 11.404585\n",
      "== W == -0.0014925880157908003\n",
      "log loss =  102638.38317096197\n",
      "Epoch 134, loss: 11.404585\n",
      "== W == -0.0014925878335128782\n",
      "log loss =  102638.38316934081\n",
      "Epoch 135, loss: 11.404585\n",
      "== W == -0.0014925876701594063\n",
      "log loss =  102638.38316788353\n",
      "Epoch 136, loss: 11.404585\n",
      "== W == -0.00149258752376628\n",
      "log loss =  102638.38316657358\n",
      "Epoch 137, loss: 11.404585\n",
      "== W == -0.0014925873925732483\n",
      "log loss =  102638.38316539605\n",
      "Epoch 138, loss: 11.404585\n",
      "== W == -0.0014925872750027628\n",
      "log loss =  102638.38316433757\n",
      "Epoch 139, loss: 11.404585\n",
      "== W == -0.001492587169640989\n",
      "log loss =  102638.3831633861\n",
      "Epoch 140, loss: 11.404585\n",
      "== W == -0.0014925870752208275\n",
      "log loss =  102638.3831625308\n",
      "Epoch 141, loss: 11.404585\n",
      "== W == -0.0014925869906066377\n",
      "log loss =  102638.38316176197\n",
      "Epoch 142, loss: 11.404585\n",
      "== W == -0.0014925869147806247\n",
      "log loss =  102638.38316107084\n",
      "Epoch 143, loss: 11.404585\n",
      "== W == -0.0014925868468306088\n",
      "log loss =  102638.3831604496\n",
      "Epoch 144, loss: 11.404585\n",
      "== W == -0.001492586785939057\n",
      "log loss =  102638.38315989112\n",
      "Epoch 145, loss: 11.404585\n",
      "== W == -0.0014925867313732395\n",
      "log loss =  102638.38315938912\n",
      "Epoch 146, loss: 11.404585\n",
      "== W == -0.00149258668247649\n",
      "log loss =  102638.38315893787\n",
      "Epoch 147, loss: 11.404585\n",
      "== W == -0.001492586638660258\n",
      "log loss =  102638.3831585322\n",
      "Epoch 148, loss: 11.404585\n",
      "== W == -0.0014925865993970443\n",
      "log loss =  102638.38315816756\n",
      "Epoch 149, loss: 11.404585\n",
      "== W == -0.0014925865642141257\n",
      "log loss =  102638.38315783975\n",
      "Epoch 150, loss: 11.404585\n",
      "== W == -0.0014925865326878038\n",
      "log loss =  102638.38315754509\n",
      "Epoch 151, loss: 11.404585\n",
      "== W == -0.0014925865044383517\n",
      "log loss =  102638.38315728022\n",
      "Epoch 152, loss: 11.404585\n",
      "== W == -0.001492586479125456\n",
      "log loss =  102638.3831570421\n",
      "Epoch 153, loss: 11.404585\n",
      "== W == -0.0014925864564441199\n",
      "log loss =  102638.38315682806\n",
      "Epoch 154, loss: 11.404585\n",
      "== W == -0.0014925864361210045\n",
      "log loss =  102638.38315663564\n",
      "Epoch 155, loss: 11.404585\n",
      "== W == -0.0014925864179111282\n",
      "log loss =  102638.38315646269\n",
      "Epoch 156, loss: 11.404585\n",
      "== W == -0.0014925864015949532\n",
      "log loss =  102638.3831563072\n",
      "Epoch 157, loss: 11.404585\n",
      "== W == -0.001492586386975723\n",
      "log loss =  102638.38315616743\n",
      "Epoch 158, loss: 11.404585\n",
      "== W == -0.0014925863738771154\n",
      "log loss =  102638.38315604179\n",
      "Epoch 159, loss: 11.404585\n",
      "== W == -0.0014925863621411115\n",
      "log loss =  102638.38315592884\n",
      "Epoch 160, loss: 11.404585\n",
      "== W == -0.0014925863516260973\n",
      "log loss =  102638.3831558273\n",
      "Epoch 161, loss: 11.404585\n",
      "== W == -0.0014925863422051591\n",
      "log loss =  102638.38315573604\n",
      "Epoch 162, loss: 11.404585\n",
      "== W == -0.0014925863337645665\n",
      "log loss =  102638.38315565397\n",
      "Epoch 163, loss: 11.404585\n",
      "== W == -0.0014925863262024175\n",
      "log loss =  102638.38315558022\n",
      "Epoch 164, loss: 11.404585\n",
      "== W == -0.001492586319427371\n",
      "log loss =  102638.3831555139\n",
      "Epoch 165, loss: 11.404585\n",
      "== W == -0.0014925863133575879\n",
      "log loss =  102638.3831554543\n",
      "Epoch 166, loss: 11.404585\n",
      "== W == -0.0014925863079197196\n",
      "log loss =  102638.38315540075\n",
      "Epoch 167, loss: 11.404585\n",
      "== W == -0.001492586303048052\n",
      "log loss =  102638.38315535255\n",
      "Epoch 168, loss: 11.404585\n",
      "== W == -0.0014925862986836776\n",
      "log loss =  102638.38315530928\n",
      "Epoch 169, loss: 11.404585\n",
      "== W == -0.0014925862947738331\n",
      "log loss =  102638.38315527032\n",
      "Epoch 170, loss: 11.404585\n",
      "== W == -0.0014925862912712334\n",
      "log loss =  102638.38315523535\n",
      "Epoch 171, loss: 11.404585\n",
      "== W == -0.0014925862881335053\n",
      "log loss =  102638.3831552039\n",
      "Epoch 172, loss: 11.404585\n",
      "== W == -0.0014925862853226786\n",
      "log loss =  102638.38315517562\n",
      "Epoch 173, loss: 11.404585\n",
      "== W == -0.0014925862828047178\n",
      "log loss =  102638.3831551502\n",
      "Epoch 174, loss: 11.404585\n",
      "== W == -0.001492586280549158\n",
      "log loss =  102638.38315512735\n",
      "Epoch 175, loss: 11.404585\n",
      "== W == -0.001492586278528673\n",
      "log loss =  102638.3831551068\n",
      "Epoch 176, loss: 11.404585\n",
      "== W == -0.0014925862767187971\n",
      "log loss =  102638.38315508833\n",
      "Epoch 177, loss: 11.404585\n",
      "== W == -0.0014925862750975937\n",
      "log loss =  102638.38315507174\n",
      "Epoch 178, loss: 11.404585\n",
      "== W == -0.0014925862736454279\n",
      "log loss =  102638.3831550568\n",
      "Epoch 179, loss: 11.404585\n",
      "== W == -0.0014925862723446771\n",
      "log loss =  102638.38315504338\n",
      "Epoch 180, loss: 11.404585\n",
      "== W == -0.0014925862711795842\n",
      "log loss =  102638.38315503133\n",
      "Epoch 181, loss: 11.404585\n",
      "== W == -0.0014925862701360081\n",
      "log loss =  102638.38315502048\n",
      "Epoch 182, loss: 11.404585\n",
      "== W == -0.001492586269201293\n",
      "log loss =  102638.38315501073\n",
      "Epoch 183, loss: 11.404585\n",
      "== W == -0.0014925862683640857\n",
      "log loss =  102638.38315500198\n",
      "Epoch 184, loss: 11.404585\n",
      "== W == -0.0014925862676142456\n",
      "log loss =  102638.38315499411\n",
      "Epoch 185, loss: 11.404585\n",
      "== W == -0.0014925862669426496\n",
      "log loss =  102638.38315498702\n",
      "Epoch 186, loss: 11.404585\n",
      "== W == -0.0014925862663411533\n",
      "log loss =  102638.38315498066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187, loss: 11.404585\n",
      "== W == -0.0014925862658024466\n",
      "log loss =  102638.38315497493\n",
      "Epoch 188, loss: 11.404585\n",
      "== W == -0.0014925862653199723\n",
      "log loss =  102638.38315496978\n",
      "Epoch 189, loss: 11.404585\n",
      "== W == -0.0014925862648878742\n",
      "log loss =  102638.38315496517\n",
      "Epoch 190, loss: 11.404585\n",
      "== W == -0.0014925862645009\n",
      "log loss =  102638.383154961\n",
      "Epoch 191, loss: 11.404585\n",
      "== W == -0.001492586264154344\n",
      "log loss =  102638.38315495726\n",
      "Epoch 192, loss: 11.404585\n",
      "== W == -0.0014925862638439807\n",
      "log loss =  102638.3831549539\n",
      "Epoch 193, loss: 11.404585\n",
      "== W == -0.0014925862635660463\n",
      "log loss =  102638.38315495089\n",
      "Epoch 194, loss: 11.404585\n",
      "== W == -0.0014925862633171476\n",
      "log loss =  102638.38315494817\n",
      "Epoch 195, loss: 11.404585\n",
      "== W == -0.00149258626309426\n",
      "log loss =  102638.38315494572\n",
      "Epoch 196, loss: 11.404585\n",
      "== W == -0.0014925862628946643\n",
      "log loss =  102638.38315494353\n",
      "Epoch 197, loss: 11.404585\n",
      "== W == -0.0014925862627159345\n",
      "log loss =  102638.38315494155\n",
      "Epoch 198, loss: 11.404585\n",
      "== W == -0.0014925862625558908\n",
      "log loss =  102638.38315493977\n",
      "Epoch 199, loss: 11.404585\n",
      "== W == -0.0014925862624125762\n",
      "log loss =  102638.3831549382\n",
      "Epoch 200, loss: 11.404585\n",
      "== W == -0.0014925862622842524\n",
      "log loss =  102638.38315493676\n",
      "Epoch 201, loss: 11.404585\n",
      "== W == -0.0014925862621693472\n",
      "log loss =  102638.38315493547\n",
      "Epoch 202, loss: 11.404585\n",
      "== W == -0.0014925862620664633\n",
      "log loss =  102638.38315493432\n",
      "Epoch 203, loss: 11.404585\n",
      "== W == -0.0014925862619743378\n",
      "log loss =  102638.38315493328\n",
      "Epoch 204, loss: 11.404585\n",
      "== W == -0.0014925862618918534\n",
      "log loss =  102638.38315493232\n",
      "Epoch 205, loss: 11.404585\n",
      "== W == -0.0014925862618180008\n",
      "log loss =  102638.38315493151\n",
      "Epoch 206, loss: 11.404585\n",
      "== W == -0.0014925862617518782\n",
      "log loss =  102638.38315493072\n",
      "Epoch 207, loss: 11.404585\n",
      "== W == -0.0014925862616926755\n",
      "log loss =  102638.38315493005\n",
      "Epoch 208, loss: 11.404585\n",
      "== W == -0.0014925862616396771\n",
      "log loss =  102638.38315492944\n",
      "Epoch 209, loss: 11.404585\n",
      "== W == -0.0014925862615922303\n",
      "log loss =  102638.38315492889\n",
      "Epoch 210, loss: 11.404585\n",
      "== W == -0.001492586261549752\n",
      "log loss =  102638.38315492841\n",
      "Epoch 211, loss: 11.404585\n",
      "== W == -0.0014925862615117191\n",
      "log loss =  102638.38315492794\n",
      "Epoch 212, loss: 11.404585\n",
      "== W == -0.001492586261477666\n",
      "log loss =  102638.38315492756\n",
      "Epoch 213, loss: 11.404585\n",
      "== W == -0.001492586261447186\n",
      "log loss =  102638.3831549272\n",
      "Epoch 214, loss: 11.404585\n",
      "== W == -0.0014925862614198968\n",
      "log loss =  102638.38315492687\n",
      "Epoch 215, loss: 11.404585\n",
      "== W == -0.0014925862613954625\n",
      "log loss =  102638.38315492659\n",
      "Epoch 216, loss: 11.404585\n",
      "== W == -0.0014925862613735952\n",
      "log loss =  102638.38315492631\n",
      "Epoch 217, loss: 11.404585\n",
      "== W == -0.0014925862613540263\n",
      "log loss =  102638.38315492608\n",
      "Epoch 218, loss: 11.404585\n",
      "== W == -0.0014925862613365038\n",
      "log loss =  102638.38315492589\n",
      "Epoch 219, loss: 11.404585\n",
      "== W == -0.0014925862613208237\n",
      "log loss =  102638.38315492567\n",
      "Epoch 220, loss: 11.404585\n",
      "== W == -0.0014925862613067832\n",
      "log loss =  102638.38315492551\n",
      "Epoch 221, loss: 11.404585\n",
      "== W == -0.001492586261294215\n",
      "log loss =  102638.38315492537\n",
      "Epoch 222, loss: 11.404585\n",
      "== W == -0.001492586261282968\n",
      "log loss =  102638.38315492522\n",
      "Epoch 223, loss: 11.404585\n",
      "== W == -0.0014925862612729028\n",
      "log loss =  102638.3831549251\n",
      "Epoch 224, loss: 11.404585\n",
      "== W == -0.0014925862612638974\n",
      "log loss =  102638.38315492499\n",
      "Epoch 225, loss: 11.404585\n",
      "== W == -0.0014925862612558244\n",
      "log loss =  102638.38315492489\n",
      "Epoch 226, loss: 11.404585\n",
      "== W == -0.0014925862612486063\n",
      "log loss =  102638.3831549248\n",
      "Epoch 227, loss: 11.404585\n",
      "== W == -0.0014925862612421557\n",
      "log loss =  102638.38315492473\n",
      "Epoch 228, loss: 11.404585\n",
      "== W == -0.0014925862612363686\n",
      "log loss =  102638.38315492465\n",
      "Epoch 229, loss: 11.404585\n",
      "== W == -0.001492586261231194\n",
      "log loss =  102638.38315492457\n",
      "Epoch 230, loss: 11.404585\n",
      "== W == -0.001492586261226561\n",
      "log loss =  102638.38315492451\n",
      "Epoch 231, loss: 11.404585\n",
      "== W == -0.0014925862612224171\n",
      "log loss =  102638.38315492448\n",
      "Epoch 232, loss: 11.404585\n",
      "== W == -0.0014925862612187094\n",
      "log loss =  102638.38315492442\n",
      "Epoch 233, loss: 11.404585\n",
      "== W == -0.001492586261215391\n",
      "log loss =  102638.38315492438\n",
      "Epoch 234, loss: 11.404585\n",
      "== W == -0.001492586261212419\n",
      "log loss =  102638.38315492432\n",
      "Epoch 235, loss: 11.404585\n",
      "== W == -0.0014925862612097545\n",
      "log loss =  102638.38315492429\n",
      "Epoch 236, loss: 11.404585\n",
      "== W == -0.0014925862612073884\n",
      "log loss =  102638.38315492426\n",
      "Epoch 237, loss: 11.404585\n",
      "== W == -0.0014925862612052581\n",
      "log loss =  102638.38315492425\n",
      "Epoch 238, loss: 11.404585\n",
      "== W == -0.0014925862612033551\n",
      "log loss =  102638.38315492423\n",
      "Epoch 239, loss: 11.404585\n",
      "== W == -0.0014925862612016508\n",
      "log loss =  102638.3831549242\n",
      "Epoch 240, loss: 11.404585\n",
      "== W == -0.001492586261200129\n",
      "log loss =  102638.38315492416\n",
      "Epoch 241, loss: 11.404585\n",
      "== W == -0.0014925862611987661\n",
      "log loss =  102638.38315492417\n",
      "Epoch 242, loss: 11.404585\n",
      "== W == -0.001492586261197544\n",
      "log loss =  102638.38315492413\n",
      "Epoch 243, loss: 11.404585\n",
      "== W == -0.0014925862611964516\n",
      "log loss =  102638.38315492413\n",
      "Epoch 244, loss: 11.404585\n",
      "== W == -0.00149258626119548\n",
      "log loss =  102638.38315492411\n",
      "Epoch 245, loss: 11.404585\n",
      "== W == -0.001492586261194604\n",
      "log loss =  102638.38315492411\n",
      "Epoch 246, loss: 11.404585\n",
      "== W == -0.0014925862611938228\n",
      "log loss =  102638.38315492409\n",
      "Epoch 247, loss: 11.404585\n",
      "== W == -0.001492586261193116\n",
      "log loss =  102638.38315492407\n",
      "Epoch 248, loss: 11.404585\n",
      "== W == -0.0014925862611924884\n",
      "log loss =  102638.38315492407\n",
      "Epoch 249, loss: 11.404585\n",
      "== W == -0.0014925862611919378\n",
      "log loss =  102638.38315492406\n",
      "Epoch 250, loss: 11.404585\n",
      "== W == -0.0014925862611914376\n",
      "log loss =  102638.38315492406\n",
      "Epoch 251, loss: 11.404585\n",
      "== W == -0.0014925862611909874\n",
      "log loss =  102638.38315492406\n",
      "Epoch 252, loss: 11.404585\n",
      "== W == -0.001492586261190582\n",
      "log loss =  102638.38315492406\n",
      "Epoch 253, loss: 11.404585\n",
      "== W == -0.0014925862611902298\n",
      "log loss =  102638.38315492403\n",
      "Epoch 254, loss: 11.404585\n",
      "== W == -0.0014925862611898997\n",
      "log loss =  102638.38315492403\n",
      "Epoch 255, loss: 11.404585\n",
      "== W == -0.0014925862611896179\n",
      "log loss =  102638.38315492403\n",
      "Epoch 256, loss: 11.404585\n",
      "== W == -0.0014925862611893583\n",
      "log loss =  102638.38315492404\n",
      "Epoch 257, loss: 11.404585\n",
      "== W == -0.0014925862611891295\n",
      "log loss =  102638.38315492403\n",
      "Epoch 258, loss: 11.404585\n",
      "== W == -0.0014925862611889261\n",
      "log loss =  102638.38315492403\n",
      "Epoch 259, loss: 11.404585\n",
      "== W == -0.0014925862611887427\n",
      "log loss =  102638.38315492403\n",
      "Epoch 260, loss: 11.404585\n",
      "== W == -0.0014925862611885816\n",
      "log loss =  102638.38315492401\n",
      "Epoch 261, loss: 11.404585\n",
      "== W == -0.0014925862611884346\n",
      "log loss =  102638.38315492401\n",
      "Epoch 262, loss: 11.404585\n",
      "== W == -0.0014925862611882995\n",
      "log loss =  102638.38315492401\n",
      "Epoch 263, loss: 11.404585\n",
      "== W == -0.0014925862611881867\n",
      "log loss =  102638.38315492401\n",
      "Epoch 264, loss: 11.404585\n",
      "== W == -0.0014925862611880761\n",
      "log loss =  102638.38315492403\n",
      "Epoch 265, loss: 11.404585\n",
      "== W == -0.001492586261187986\n",
      "log loss =  102638.383154924\n",
      "Epoch 266, loss: 11.404585\n",
      "== W == -0.0014925862611879013\n",
      "log loss =  102638.38315492401\n",
      "Epoch 267, loss: 11.404585\n",
      "== W == -0.001492586261187823\n",
      "log loss =  102638.38315492401\n",
      "Epoch 268, loss: 11.404585\n",
      "== W == -0.0014925862611877587\n",
      "log loss =  102638.383154924\n",
      "Epoch 269, loss: 11.404585\n",
      "== W == -0.0014925862611877062\n",
      "log loss =  102638.383154924\n",
      "Epoch 270, loss: 11.404585\n",
      "== W == -0.001492586261187648\n",
      "log loss =  102638.383154924\n",
      "Epoch 271, loss: 11.404585\n",
      "== W == -0.0014925862611876\n",
      "log loss =  102638.383154924\n",
      "Epoch 272, loss: 11.404585\n",
      "== W == -0.0014925862611875592\n",
      "log loss =  102638.38315492401\n",
      "Epoch 273, loss: 11.404585\n",
      "== W == -0.001492586261187528\n",
      "log loss =  102638.383154924\n",
      "Epoch 274, loss: 11.404585\n",
      "== W == -0.001492586261187486\n",
      "log loss =  102638.38315492401\n",
      "Epoch 275, loss: 11.404585\n",
      "== W == -0.0014925862611874523\n",
      "log loss =  102638.383154924\n",
      "Epoch 276, loss: 11.404585\n",
      "== W == -0.0014925862611874245\n",
      "log loss =  102638.383154924\n",
      "Epoch 277, loss: 11.404585\n",
      "== W == -0.0014925862611874028\n",
      "log loss =  102638.383154924\n",
      "Epoch 278, loss: 11.404585\n",
      "== W == -0.0014925862611873762\n",
      "log loss =  102638.383154924\n",
      "Epoch 279, loss: 11.404585\n",
      "== W == -0.0014925862611873627\n",
      "log loss =  102638.383154924\n",
      "Epoch 280, loss: 11.404585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== W == -0.0014925862611873423\n",
      "log loss =  102638.38315492398\n",
      "Epoch 281, loss: 11.404585\n",
      "== W == -0.0014925862611873295\n",
      "log loss =  102638.383154924\n",
      "Epoch 282, loss: 11.404585\n",
      "== W == -0.0014925862611873146\n",
      "log loss =  102638.383154924\n",
      "Epoch 283, loss: 11.404585\n",
      "== W == -0.0014925862611873076\n",
      "log loss =  102638.38315492398\n",
      "Epoch 284, loss: 11.404585\n",
      "== W == -0.001492586261187296\n",
      "log loss =  102638.383154924\n",
      "Epoch 285, loss: 11.404585\n",
      "== W == -0.001492586261187287\n",
      "log loss =  102638.383154924\n",
      "Epoch 286, loss: 11.404585\n",
      "== W == -0.0014925862611872755\n",
      "log loss =  102638.38315492398\n",
      "Epoch 287, loss: 11.404585\n",
      "== W == -0.0014925862611872701\n",
      "log loss =  102638.383154924\n",
      "Epoch 288, loss: 11.404585\n",
      "== W == -0.001492586261187263\n",
      "log loss =  102638.383154924\n",
      "Epoch 289, loss: 11.404585\n",
      "== W == -0.001492586261187254\n",
      "log loss =  102638.383154924\n",
      "Epoch 290, loss: 11.404585\n",
      "== W == -0.001492586261187242\n",
      "log loss =  102638.383154924\n",
      "Epoch 291, loss: 11.404585\n",
      "== W == -0.0014925862611872387\n",
      "log loss =  102638.383154924\n",
      "Epoch 292, loss: 11.404585\n",
      "== W == -0.0014925862611872309\n",
      "log loss =  102638.383154924\n",
      "Epoch 293, loss: 11.404585\n",
      "== W == -0.001492586261187232\n",
      "log loss =  102638.383154924\n",
      "Epoch 294, loss: 11.404585\n",
      "== W == -0.0014925862611872283\n",
      "log loss =  102638.383154924\n",
      "Epoch 295, loss: 11.404585\n",
      "== W == -0.0014925862611872202\n",
      "log loss =  102638.38315492398\n",
      "Epoch 296, loss: 11.404585\n",
      "== W == -0.0014925862611872187\n",
      "log loss =  102638.383154924\n",
      "Epoch 297, loss: 11.404585\n",
      "== W == -0.0014925862611872192\n",
      "log loss =  102638.383154924\n",
      "Epoch 298, loss: 11.404585\n",
      "== W == -0.001492586261187213\n",
      "log loss =  102638.383154924\n",
      "Epoch 299, loss: 11.404585\n",
      "== W == -0.0014925862611872085\n"
     ]
    }
   ],
   "source": [
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=300, learning_rate=1e-2, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.438084378746636, 11.431914322588472, 11.42689917769524, 11.422820529940504, 11.419501594620423, 11.41679922021833, 11.414597443258607, 11.412802289083546, 11.411337574710734, 11.410141518538248, 11.409164000315744, 11.408364345612863, 11.407709533637929, 11.40717274697282, 11.406732197599315, 11.406370176290078, 11.406072282648507, 11.405826801302368, 11.405624196380224, 11.405456701741652, 11.405317988743295, 11.405202896803804, 11.405107214843081, 11.405027503944249, 11.40496095342466, 11.404905263988862, 11.404858552839029, 11.404819276591756, 11.40478616863796, 11.404758188220413, 11.404734479020012, 11.40471433546024, 11.404697175277963, 11.4046825171835, 11.404669962655051, 11.404659181093063, 11.40464989770604, 11.404641883617877, 11.40463494778265, 11.404628930370809, 11.40462369735373, 11.404619136064875, 11.404615151557268, 11.404611663610828, 11.40460860427031, 11.404605915816935, 11.40460354909478, 11.404601462127571, 11.404599618973608, 11.404597988775974, 11.404596544973275, 11.404595264642374, 11.4045941279499, 11.404593117693473, 11.40459221891711, 11.404591418587986, 11.404590705324111, 11.404590069164335, 11.404589501373525, 11.404588994277185, 11.404588541120624, 11.404588135948732, 11.404587773503069, 11.404587449133567, 11.404587158722496, 11.4045868986189, 11.404586665581881, 11.404586456731408, 11.404586269505577, 11.404586101623357, 11.404585951052088, 11.40458581597901, 11.404585694786341, 11.404585586029357, 11.404585488417103, 11.404585400795423, 11.404585322131924, 11.404585251502745, 11.404585188080778, 11.404585131125287, 11.404585079972643, 11.404585034028125, 11.404584992758615, 11.404584955686103, 11.404584922381867, 11.404584892461342, 11.404584865579498, 11.404584841426699, 11.404584819725063, 11.404584800225159, 11.404584782703049, 11.404584766957685, 11.404584752808532, 11.404584740093473, 11.4045847286669, 11.404584718398048, 11.404584709169447, 11.404584700875581, 11.404584693421667, 11.404584686722542, 11.404584680701698, 11.4045846752904, 11.404584670426885, 11.404584666055657, 11.404584662126855, 11.404584658595665, 11.404584655421825, 11.404584652569147, 11.40458465000512, 11.404584647700522, 11.404584645629088, 11.40458464376722, 11.404584642093711, 11.404584640589492, 11.404584639237438, 11.404584638022145, 11.40458463692978, 11.4045846359479, 11.40458463506533, 11.404584634272029, 11.404584633558951, 11.404584632917992, 11.404584632341855, 11.404584631823981, 11.404584631358476, 11.404584630940047, 11.404584630563923, 11.40458463022584, 11.404584629921937, 11.404584629648765, 11.404584629403214, 11.404584629182489, 11.404584628984082, 11.404584628805736, 11.404584628645418, 11.404584628501313, 11.404584628371774, 11.40458462825533, 11.40458462815066, 11.404584628056572, 11.404584627971996, 11.404584627895966, 11.404584627827628, 11.404584627766194, 11.40458462771097, 11.404584627661327, 11.404584627616705, 11.404584627576595, 11.404584627540535, 11.404584627508122, 11.404584627478984, 11.404584627452792, 11.404584627429246, 11.40458462740808, 11.404584627389054, 11.40458462737195, 11.404584627356575, 11.404584627342757, 11.404584627330332, 11.404584627319164, 11.404584627309124, 11.404584627300096, 11.404584627291985, 11.40458462728469, 11.404584627278135, 11.40458462727224, 11.404584627266942, 11.404584627262182, 11.404584627257897, 11.404584627254051, 11.404584627250589, 11.404584627247479, 11.404584627244684, 11.404584627242171, 11.404584627239911, 11.40458462723788, 11.404584627236053, 11.40458462723441, 11.404584627232937, 11.404584627231609, 11.404584627230417, 11.404584627229346, 11.40458462722838, 11.404584627227512, 11.404584627226736, 11.404584627226036, 11.404584627225407, 11.404584627224843, 11.404584627224331, 11.404584627223874, 11.404584627223462, 11.404584627223093, 11.40458462722276, 11.404584627222462, 11.404584627222194, 11.404584627221952, 11.404584627221734, 11.40458462722154, 11.404584627221364, 11.404584627221206, 11.404584627221068, 11.40458462722094, 11.404584627220824, 11.404584627220721, 11.404584627220629, 11.404584627220544, 11.404584627220473, 11.404584627220402, 11.404584627220343, 11.404584627220288, 11.404584627220238, 11.404584627220196, 11.404584627220155, 11.404584627220121, 11.404584627220089, 11.404584627220059, 11.404584627220034, 11.404584627220009, 11.40458462721999, 11.404584627219974, 11.404584627219952, 11.404584627219938, 11.404584627219926, 11.404584627219913, 11.404584627219904, 11.404584627219892, 11.404584627219883, 11.404584627219874, 11.404584627219869, 11.404584627219862, 11.404584627219855, 11.40458462721985, 11.404584627219846, 11.404584627219842, 11.404584627219839, 11.404584627219831, 11.40458462721983, 11.404584627219828, 11.404584627219826, 11.404584627219826, 11.404584627219823, 11.404584627219817, 11.404584627219819, 11.404584627219817, 11.404584627219817, 11.404584627219815, 11.404584627219815, 11.404584627219812, 11.40458462721981, 11.40458462721981, 11.404584627219808, 11.40458462721981, 11.40458462721981, 11.40458462721981, 11.404584627219807, 11.404584627219807, 11.404584627219807, 11.404584627219808, 11.404584627219807, 11.404584627219807, 11.404584627219807, 11.404584627219805, 11.404584627219805, 11.404584627219805, 11.404584627219805, 11.404584627219807, 11.404584627219803, 11.404584627219805, 11.404584627219805, 11.404584627219803, 11.404584627219803, 11.404584627219803, 11.404584627219803, 11.404584627219805, 11.404584627219803, 11.404584627219805, 11.404584627219803, 11.404584627219803, 11.404584627219803, 11.404584627219803, 11.404584627219803, 11.404584627219801, 11.404584627219803, 11.404584627219803, 11.404584627219801, 11.404584627219803, 11.404584627219803, 11.404584627219801, 11.404584627219803, 11.404584627219803, 11.404584627219803, 11.404584627219803, 11.404584627219803, 11.404584627219803, 11.404584627219803, 11.404584627219803, 11.404584627219801, 11.404584627219803, 11.404584627219803, 11.404584627219803]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 12)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(loss_history)\n",
    "plt.plot(loss_history)\n",
    "plt.gca().set_ylim(10,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102670.7999651374\n",
      "Epoch 0, loss: 11.438334\n",
      "== W == 4915.78746514289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "  log =   np.sum(- np.log(probs[np.arange(probs.shape[0]),target_index.flatten()]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == -50685441.43709953\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == 506802194968.2201\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == -5067515149021195.0\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == 5.067008397506139e+19\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == -5.0665016966663887e+23\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == 5.065995046496721e+27\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == -5.065488446992072e+31\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == 5.064981898147372e+35\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == -5.064475399957558e+39\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == 5.063968952417561e+43\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == -5.06346255552232e+47\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == 5.0629562092667676e+51\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == -5.06244991364584e+55\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == 5.061943668654477e+59\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == -5.061437474287611e+63\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == 5.060931330540182e+67\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == -5.060425237407127e+71\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == 5.059919194883387e+75\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == -5.059413202963899e+79\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == 5.058907261643603e+83\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == -5.058401370917439e+87\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == 5.057895530780346e+91\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == -5.0573897412272683e+95\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == 5.056884002253145e+99\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == -5.05637831385292e+103\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == 5.0558726760215355e+107\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == -5.055367088753932e+111\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == 5.054861552045057e+115\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == -5.054356065889852e+119\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == 5.053850630283264e+123\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == -5.053345245220235e+127\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == 5.0528399106957135e+131\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == -5.0523346267046445e+135\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == 5.051829393241974e+139\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == -5.051324210302649e+143\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == 5.050819077881619e+147\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == -5.0503139959738304e+151\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == 5.049808964574233e+155\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == -5.049303983677776e+159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\сергей\\.virtualenvs\\layman-s-deep-learning--lrbitjvr\\lib\\site-packages\\numpy\\core\\fromnumeric.py:90: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == 5.048799053279408e+163\n",
      "log loss =  inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py:98: RuntimeWarning: overflow encountered in square\n",
      "  loss = reg_strength * np.sum(W**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, loss: inf\n",
      "== W == -5.04829417337408e+167\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == 5.0477893439567435e+171\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == -5.0472845650223474e+175\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == 5.0467798365658447e+179\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == -5.046275158582188e+183\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == 5.0457705310663305e+187\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == -5.045265954013223e+191\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == 5.044761427417822e+195\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == -5.0442569512750815e+199\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == 5.043752525579953e+203\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == -5.043248150327395e+207\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == 5.0427438255123615e+211\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == -5.042239551129811e+215\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == 5.041735327174698e+219\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == -5.04123115364198e+223\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == 5.040727030526616e+227\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == -5.040222957823564e+231\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == 5.0397189355277804e+235\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == -5.039214963634228e+239\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == 5.038711042137865e+243\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == -5.03820717103365e+247\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == 5.037703350316548e+251\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == -5.037199579981516e+255\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == 5.036695860023518e+259\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == -5.0361921904375154e+263\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == 5.035688571218472e+267\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == -5.035185002361351e+271\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == 5.034681483861114e+275\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == -5.034178015712728e+279\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == 5.033674597911156e+283\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == -5.033171230451367e+287\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == 5.032667913328321e+291\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == -5.032164646536989e+295\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == 5.031661430072335e+299\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == -5.031158263929328e+303\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == 5.030655148102934e+307\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == nan\n",
      "log loss =  nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py:201: RuntimeWarning: overflow encountered in multiply\n",
      "  self.W -= dW*learning_rate\n",
      "c:\\users\\сергей\\.virtualenvs\\layman-s-deep-learning--lrbitjvr\\lib\\site-packages\\numpy\\core\\fromnumeric.py:90: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "D:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py:100: RuntimeWarning: overflow encountered in multiply\n",
      "  grad = reg_strength*W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  102668.10907727949\n",
      "Epoch 0, loss: 11.438240\n",
      "== W == -310.8123140772547\n",
      "log loss =  1144048.4563301336\n",
      "Epoch 1, loss: 31143.856528\n",
      "== W == 388497.4621118123\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -388260939.21488565\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 387872778606.2036\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -387484905979572.06\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 3.8709742107369274e+17\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -3.8671032365261914e+20\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 3.8632361332896665e+23\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -3.859372897156376e+26\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 3.8555135242592185e+29\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -3.8516580107349606e+32\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 3.8478063527242255e+35\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -3.8439585463715e+38\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 3.84011458782513e+41\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -3.836274473237305e+44\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 3.832438198764067e+47\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -3.8286057605653026e+50\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 3.8247771548047377e+53\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -3.8209523776499325e+56\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 3.8171314252722823e+59\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -3.8133142938470106e+62\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 3.8095009795531635e+65\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -3.8056914785736103e+68\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 3.8018857870950374e+71\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -3.7980839013079415e+74\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 3.7942858174066336e+77\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -3.7904915315892277e+80\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 3.786701040057638e+83\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -3.7829143390175805e+86\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 3.779131424678563e+89\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -3.775352293253884e+92\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 3.7715769409606297e+95\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -3.7678053640196697e+98\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 3.7640375586556505e+101\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -3.760273521096995e+104\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 3.7565132475758965e+107\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -3.752756734328322e+110\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 3.7490039775939936e+113\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -3.7452549736164e+116\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 3.741509718642783e+119\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -3.73776820892414e+122\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 3.7340304407152166e+125\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -3.7302964102745015e+128\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 3.7265661138642266e+131\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -3.722839547750362e+134\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 3.719116708202612e+137\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -3.7153975914944094e+140\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 3.711682193902915e+143\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -3.7079705117090125e+146\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 3.7042625411973037e+149\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -3.700558278656106e+152\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 3.6968577203774495e+155\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -3.6931608626570723e+158\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 3.689467701794414e+161\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -3.6857782340926195e+164\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 3.682092455858527e+167\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -3.678410363402669e+170\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 3.6747319530392653e+173\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -3.671057221086227e+176\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 3.6673861638651407e+179\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -3.663718777701276e+182\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 3.660055058923574e+185\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -3.6563950038646505e+188\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 3.652738608860786e+191\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -3.6490858702519256e+194\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 3.6454367843816736e+197\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -3.641791347597291e+200\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 3.6381495562496936e+203\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -3.634511406693445e+206\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 3.630876895286752e+209\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -3.6272460183914645e+212\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 3.6236187723730735e+215\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -3.6199951536007e+218\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 3.6163751584471e+221\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -3.6127587832886517e+224\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 3.609146024505364e+227\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -3.6055368784808586e+230\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 3.601931341602377e+233\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -3.5983294102607752e+236\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 3.5947310808505145e+239\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -3.5911363497696643e+242\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 3.5875452134198943e+245\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -3.583957668206474e+248\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 3.5803737105382674e+251\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -3.5767933368277293e+254\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 3.573216543490902e+257\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -3.5696433269474103e+260\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 3.566073683620463e+263\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -3.5625076099368436e+266\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 3.558945102326906e+269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -3.555386157224579e+272\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 3.5518307710673546e+275\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -3.548278940296287e+278\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 3.544730661355991e+281\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -3.541185930694635e+284\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 3.537644744763939e+287\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -3.534107100019176e+290\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 3.530572992919157e+293\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -3.5270424199262386e+296\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 3.523515377506312e+299\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -3.519991862128805e+302\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 3.516471870266676e+305\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py:15: RuntimeWarning: overflow encountered in subtract\n",
      "  s_pred =predictions-np.max(predictions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  102668.52298924676\n",
      "Epoch 0, loss: 11.438346\n",
      "== W == -1.0840322273090934\n",
      "log loss =  159422.89030266882\n",
      "Epoch 1, loss: 322.965271\n",
      "== W == 8283.067329158092\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -835363.4420557179\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 82712047.89523286\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -8188508081.241434\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 810662311110.0338\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -80255568815232.94\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 7945301312719129.0\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -7.86584829959209e+17\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 7.78718981659617e+19\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -7.709317918430207e+21\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 7.632224739245906e+23\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -7.555902491853447e+25\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 7.480343466934912e+27\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -7.405540032265563e+29\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 7.331484631942907e+31\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -7.258169785623479e+33\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 7.185588087767243e+35\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -7.1137322068895725e+37\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 7.042594884820676e+39\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -6.97216893597247e+41\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 6.902447246612745e+43\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -6.833422774146617e+45\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 6.765088546405151e+47\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -6.697437660941099e+49\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 6.630463284331689e+51\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -6.564158651488371e+53\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 6.498517064973487e+55\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -6.433531894323752e+57\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 6.369196575380515e+59\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -6.30550460962671e+61\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 6.242449563530443e+63\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -6.180025067895138e+65\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 6.118224817216187e+67\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -6.057042569044024e+69\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 5.996472143353585e+71\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -5.936507421920049e+73\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 5.877142347700847e+75\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -5.81837092422384e+77\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 5.760187214981602e+79\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -5.7025853428317865e+81\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 5.645559489403468e+83\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -5.5891038945094326e+85\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 5.5332128555643386e+87\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -5.477880727008695e+89\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 5.423101919738609e+91\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -5.368870900541222e+93\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 5.31518219153581e+95\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -5.262030369620452e+97\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 5.209410065924247e+99\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -5.1573159652650054e+101\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 5.105742805612355e+103\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -5.054685377556232e+105\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 5.004138523780669e+107\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -4.954097138542862e+109\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 4.904556167157434e+111\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -4.855510605485859e+113\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 4.806955499431002e+115\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -4.7588859444366905e+117\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 4.711297084992325e+119\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -4.664184114142401e+121\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 4.6175422730009764e+123\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -4.571366850270967e+125\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 4.525653181768258e+127\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -4.480396649950575e+129\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 4.435592683451069e+131\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -4.3912367566165584e+133\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 4.347324389050393e+135\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -4.3038511451598896e+137\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 4.2608126337082914e+139\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -4.218204507371208e+141\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 4.176022462297496e+143\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -4.13426223767452e+145\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 4.092919615297776e+147\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -4.0519904191447976e+149\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 4.0114705149533496e+151\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -3.9713558098038155e+153\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 3.9316422517057776e+155\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -3.89232582918872e+157\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 3.853402570896833e+159\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -3.814868545187865e+161\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 3.776719859735986e+163\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -3.738952661138626e+165\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 3.701563134527241e+167\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -3.664547503181968e+169\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 3.6279020281501482e+171\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -3.5916230078686467e+173\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 3.55570677778996e+175\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -3.5201497100120606e+177\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 3.4849482129119406e+179\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -3.4500987307828205e+181\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 3.4155977434749916e+183\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -3.381441766040242e+185\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 3.34762734837984e+187\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -3.3141510748960414e+189\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 3.281009564147081e+191\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -3.24819946850561e+193\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 3.215717473820554e+195\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -3.183560299082349e+197\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 3.1517246960915246e+199\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -3.1202074491306094e+201\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 3.089005374639304e+203\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -3.0581153208929107e+205\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 3.0275341676839816e+207\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -2.9972588260071422e+209\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 2.9672862377470703e+211\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -2.937613375369599e+213\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 2.908237241615904e+215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -2.879154869199745e+217\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 2.850363320507748e+219\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -2.82185968730267e+221\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 2.793641090429643e+223\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -2.765704679525347e+225\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 2.7380476327300935e+227\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -2.710667156402793e+229\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 2.6835604848387643e+231\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -2.6567248799903767e+233\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 2.630157631190473e+235\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -2.603856054878569e+237\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 2.577817494329783e+239\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -2.552039319386485e+241\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 2.5265189261926205e+243\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -2.501253736930694e+245\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 2.4762411995613874e+247\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -2.4514787875657734e+249\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 2.4269639996901157e+251\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -2.4026943596932146e+253\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 2.378667416096282e+255\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -2.3548807419353196e+257\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 2.331331934515966e+259\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -2.3080186151708063e+261\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 2.2849384290190988e+263\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -2.262089044728907e+265\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 2.239468154281618e+267\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -2.2170734727388023e+269\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 2.1949027380114143e+271\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -2.1729537106313e+273\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 2.1512241735249867e+275\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -2.1297119317897375e+277\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 2.1084148124718396e+279\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -2.0873306643471213e+281\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 2.0664573577036503e+283\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -2.0457927841266136e+285\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 2.0253348562853478e+287\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -2.005081507722494e+289\n",
      "log loss =  inf\n",
      "Epoch 5, loss: inf\n",
      "== W == 1.9850306926452692e+291\n",
      "log loss =  inf\n",
      "Epoch 6, loss: inf\n",
      "== W == -1.965180385718816e+293\n",
      "log loss =  inf\n",
      "Epoch 7, loss: inf\n",
      "== W == 1.945528581861628e+295\n",
      "log loss =  inf\n",
      "Epoch 8, loss: inf\n",
      "== W == -1.9260732960430117e+297\n",
      "log loss =  inf\n",
      "Epoch 9, loss: inf\n",
      "== W == 1.9068125630825814e+299\n",
      "log loss =  inf\n",
      "Epoch 0, loss: inf\n",
      "== W == -1.887744437451756e+301\n",
      "log loss =  inf\n",
      "Epoch 1, loss: inf\n",
      "== W == 1.8688669930772383e+303\n",
      "log loss =  inf\n",
      "Epoch 2, loss: inf\n",
      "== W == -1.850178323146466e+305\n",
      "log loss =  inf\n",
      "Epoch 3, loss: inf\n",
      "== W == 1.8316765399150013e+307\n",
      "log loss =  inf\n",
      "Epoch 4, loss: inf\n",
      "== W == -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py:15: RuntimeWarning: invalid value encountered in subtract\n",
      "  s_pred =predictions-np.max(predictions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 0, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 1, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 2, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 3, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 4, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 5, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 6, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 7, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 8, loss: nan\n",
      "== W == nan\n",
      "log loss =  nan\n",
      "Epoch 9, loss: nan\n",
      "== W == nan\n",
      "log loss =  102668.85463337973\n",
      "Epoch 0, loss: 11.438269\n",
      "== W == 0.015540674042626997\n",
      "log loss =  102636.90398061136\n",
      "Epoch 1, loss: 11.404492\n",
      "== W == -0.019178979317764164\n",
      "log loss =  102639.98956245898\n",
      "Epoch 2, loss: 11.404746\n",
      "== W == 0.012820520248636072\n",
      "log loss =  102637.68682490056\n",
      "Epoch 3, loss: 11.404532\n",
      "== W == -0.014074301357844255\n",
      "log loss =  102639.08073265254\n",
      "Epoch 4, loss: 11.404651\n",
      "== W == 0.008554584713335171\n",
      "log loss =  102637.99636495634\n",
      "Epoch 5, loss: 11.404553\n",
      "== W == -0.009695722996500443\n",
      "log loss =  102638.70630193588\n",
      "Epoch 6, loss: 11.404614\n",
      "== W == 0.004938153026488035\n",
      "log loss =  102638.1814032685\n",
      "Epoch 7, loss: 11.404567\n",
      "== W == -0.00655459263283111\n",
      "log loss =  102638.53761094222\n",
      "Epoch 8, loss: 11.404599\n",
      "== W == 0.0024140073589356\n",
      "log loss =  102638.28080350203\n",
      "Epoch 9, loss: 11.404576\n",
      "== W == -0.004502930572640364\n",
      "log loss =  102638.45824000891\n",
      "Epoch 0, loss: 11.404591\n",
      "== W == 0.0008020075717093165\n",
      "log loss =  102638.33188748157\n",
      "Epoch 1, loss: 11.404580\n",
      "== W == -0.0032363546559212204\n",
      "log loss =  102638.41998445491\n",
      "Epoch 2, loss: 11.404588\n",
      "== W == -0.00017627468269548798\n",
      "log loss =  102638.3576259446\n",
      "Epoch 3, loss: 11.404582\n",
      "== W == -0.002483121835402553\n",
      "log loss =  102638.40130676396\n",
      "Epoch 4, loss: 11.404586\n",
      "== W == -0.0007505796435017544\n",
      "log loss =  102638.37047594166\n",
      "Epoch 5, loss: 11.404584\n",
      "== W == -0.00204685181922298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102638.39212512493\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0010799306970847582\n",
      "log loss =  102638.37686443982\n",
      "Epoch 7, loss: 11.404584\n",
      "== W == -0.0017990638669892253\n",
      "log loss =  102638.38759484571\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0012655476230026377\n",
      "log loss =  102638.3800347917\n",
      "Epoch 9, loss: 11.404584\n",
      "== W == -0.0016604283403662285\n",
      "log loss =  102638.38535484833\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.001368760057408545\n",
      "log loss =  102638.38160710236\n",
      "Epoch 1, loss: 11.404584\n",
      "== W == -0.0015837790212344946\n",
      "log loss =  102638.38424582212\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001425539838997514\n",
      "log loss =  102638.38238683227\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0015418060336400518\n",
      "log loss =  102638.38369623627\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014565044791253576\n",
      "log loss =  102638.38277360107\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.001519002892709914\n",
      "log loss =  102638.38342368805\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014732691709814336\n",
      "log loss =  102638.38296552277\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0015066961234742734\n",
      "log loss =  102638.38328844235\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014822906882471887\n",
      "log loss =  102638.38306080125\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0015000913785778508\n",
      "log loss =  102638.38322129086\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014871202632402263\n",
      "log loss =  102638.38310812521\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014965637826495162\n",
      "log loss =  102638.38318793049\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014896941916687297\n",
      "log loss =  102638.38313164271\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014946875196591865\n",
      "log loss =  102638.38317134819\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014910606509488478\n",
      "log loss =  102638.38314333587\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.001493693188211341\n",
      "log loss =  102638.38316310124\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014917836193831832\n",
      "log loss =  102638.38314915294\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014931679184135103\n",
      "log loss =  102638.38315899747\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014921649830617935\n",
      "log loss =  102638.38315204835\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014928912192499998\n",
      "log loss =  102638.38315695431\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014923656168671121\n",
      "log loss =  102638.38315349033\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.001492745826335253\n",
      "log loss =  102638.38315593649\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001492470919469213\n",
      "log loss =  102638.38315420883\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.001492669600067433\n",
      "log loss =  102638.38315542921\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925260702780196\n",
      "log loss =  102638.38315456704\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014926297166833872\n",
      "log loss =  102638.3831551762\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.001492554899666028\n",
      "log loss =  102638.38315474574\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014926088865564102\n",
      "log loss =  102638.38315504997\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925699439180353\n",
      "log loss =  102638.38315483493\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925980253009527\n",
      "log loss =  102638.38315498695\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925777823324542\n",
      "log loss =  102638.38315487947\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925923704297666\n",
      "log loss =  102638.38315495549\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925818605520996\n",
      "log loss =  102638.38315490172\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.00149258943021818\n",
      "log loss =  102638.38315493974\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925839796592394\n",
      "log loss =  102638.38315491287\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925879033569414\n",
      "log loss =  102638.38315493189\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925850794857323\n",
      "log loss =  102638.38315491841\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.001492587111344502\n",
      "log loss =  102638.38315492796\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.00149258564968692\n",
      "log loss =  102638.3831549212\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925867009350907\n",
      "log loss =  102638.38315492598\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925859450145192\n",
      "log loss =  102638.3831549226\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925864884675658\n",
      "log loss =  102638.38315492499\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925860978369536\n",
      "log loss =  102638.38315492329\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925863785690977\n",
      "log loss =  102638.38315492448\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925861768519343\n",
      "log loss =  102638.38315492365\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925863217696205\n",
      "log loss =  102638.38315492425\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862176744533\n",
      "log loss =  102638.38315492382\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862924350974\n",
      "log loss =  102638.38315492411\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862387503135\n",
      "log loss =  102638.38315492391\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862772953095\n",
      "log loss =  102638.38315492406\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.001492586249624253\n",
      "log loss =  102638.38315492395\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862694864106\n",
      "log loss =  102638.38315492401\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862552312423\n",
      "log loss =  102638.38315492397\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862654609923\n",
      "log loss =  102638.38315492401\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862581207962\n",
      "log loss =  102638.38315492398\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862633870216\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.001492586259609179\n",
      "log loss =  102638.38315492398\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862623190323\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862603754388\n",
      "log loss =  102638.38315492398\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862617693553\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862607697681\n",
      "log loss =  102638.383154924\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.001492586261486492\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862609726378\n",
      "log loss =  102638.38315492401\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862613410308\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862610769077\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.001492586261266255\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611305496\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862612277757\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611581298\n",
      "log loss =  102638.38315492397\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862612080712\n",
      "log loss =  102638.38315492398\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.001492586261172255\n",
      "log loss =  102638.383154924\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.001492586261197912\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611795096\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611927005\n",
      "log loss =  102638.38315492398\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611832512\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.001492586261190046\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611852017\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611886182\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611861638\n",
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.00149258626118794\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.001492586261186648\n",
      "log loss =  102638.38315492398\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611875982\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001492586261186906\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.001492586261187412\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611870646\n",
      "log loss =  102638.38315492398\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611872755\n",
      "log loss =  102638.38315492398\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611871211\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611872118\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.001492586261187166\n",
      "log loss =  102638.38315492398\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611871949\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611872153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102638.383154924\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611871435\n",
      "log loss =  102638.38315492398\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611871925\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611872276\n",
      "log loss =  102638.38315492398\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.001492586261187203\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.001492586261187214\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611871888\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611871905\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611872259\n",
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611872072\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.001492586261187221\n",
      "log loss =  102638.383154924\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611872137\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611871548\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611871823\n",
      "log loss =  102638.38315492398\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.001492586261187167\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.001492586261187185\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.001492586261187165\n",
      "log loss =  102638.38315492398\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.001492586261187207\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611872179\n",
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611871819\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.001492586261187224\n",
      "log loss =  102638.383154924\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611872085\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611871764\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611872035\n",
      "log loss =  102638.38315492398\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.001492586261187184\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611872207\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611871498\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.001492586261187199\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611871615\n",
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611871819\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611871808\n",
      "log loss =  102638.383154924\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611871801\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611871736\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611872048\n",
      "log loss =  102638.38315492398\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611872079\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611872244\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611871767\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611872066\n",
      "log loss =  102638.38315492398\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611871751\n",
      "log loss =  102638.38315492401\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611871823\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611871571\n",
      "log loss =  102638.38315492401\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.001492586261187181\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611871628\n",
      "log loss =  102638.38315492398\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611871949\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611871847\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.001492586261187174\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611871797\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611871955\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611871728\n",
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.001492586261187188\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.001492586261187215\n",
      "log loss =  102638.383154924\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.001492586261187188\n",
      "log loss =  102638.38315492398\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611872107\n",
      "log loss =  102638.38315492398\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611871953\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611871908\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611872027\n",
      "log loss =  102638.38315492398\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611871806\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611871654\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.001492586261187249\n",
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611871522\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611872443\n",
      "log loss =  102638.38315492398\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611871483\n",
      "log loss =  102638.38315492401\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001492586261187193\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611871548\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611872317\n",
      "log loss =  102638.38315492401\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611871593\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611871931\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611871515\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611871853\n",
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611871645\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611872064\n",
      "log loss =  102638.38315492401\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611871437\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611871981\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.001492586261187177\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611871912\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611871702\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.001492586261187167\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.001492586261187156\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611872205\n",
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611871684\n",
      "log loss =  102638.38315492398\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611871925\n",
      "log loss =  102638.383154924\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611871537\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611871916\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611871862\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611871897\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.001492586261187184\n",
      "log loss =  102638.38315492398\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611871899\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.001492586261187175\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611872062\n",
      "log loss =  102638.38315492398\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611871745\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611871717\n",
      "log loss =  102638.38315492401\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611872296\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001492586261187161\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.001492586261187171\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611871853\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.001492586261187208\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611871782\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611872356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611871665\n",
      "log loss =  102638.38315492398\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611872248\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611871602\n",
      "log loss =  102638.383154924\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611872356\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611871816\n",
      "log loss =  102638.38315492398\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611872031\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611871645\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611872252\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611872135\n",
      "log loss =  102638.38315492398\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611871814\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611871715\n",
      "log loss =  102638.38315492398\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611871957\n",
      "log loss =  102638.38315492398\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611871628\n",
      "log loss =  102638.383154924\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611872333\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611871769\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.00149258626118718\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.001492586261187205\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611872098\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.00149258626118723\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611871745\n",
      "log loss =  102638.38315492398\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611872424\n",
      "log loss =  102638.38315492398\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611871335\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611872142\n",
      "log loss =  102638.38315492398\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611871643\n",
      "log loss =  102638.38315492401\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611871823\n",
      "log loss =  102638.38315492398\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611871895\n",
      "log loss =  102638.38315492398\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611871892\n",
      "log loss =  102638.38315492398\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611871704\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611871743\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611872237\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.001492586261187164\n",
      "log loss =  102638.38315492398\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611871936\n",
      "log loss =  102638.38315492398\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.001492586261187171\n",
      "log loss =  102638.383154924\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611872038\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001492586261187186\n",
      "log loss =  102638.38315492398\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611871819\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611872218\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611872083\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611872068\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611871663\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611871697\n",
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611871784\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611871725\n",
      "log loss =  102638.38315492398\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611872562\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611871274\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611872185\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611871593\n",
      "log loss =  102638.38315492398\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.001492586261187197\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611871936\n",
      "log loss =  102638.38315492398\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.001492586261187214\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611871801\n",
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611872114\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611871485\n",
      "log loss =  102638.38315492398\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611872083\n",
      "log loss =  102638.38315492401\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611871637\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611872022\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.001492586261187153\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611871916\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611871628\n",
      "log loss =  102638.38315492398\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611872105\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611872012\n",
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611871732\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.001492586261187244\n",
      "log loss =  102638.383154924\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611871383\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611872031\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611871552\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.001492586261187241\n",
      "log loss =  102638.38315492401\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611871316\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611872274\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611871424\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611872547\n",
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611871177\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611872712\n",
      "log loss =  102638.383154924\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611871853\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611871992\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611871953\n",
      "log loss =  102638.38315492398\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.001492586261187184\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.001492586261187224\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611871827\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611872042\n",
      "log loss =  102638.38315492398\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611871775\n",
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.001492586261187202\n",
      "log loss =  102638.38315492401\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.001492586261187158\n",
      "log loss =  102638.383154924\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611871873\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611871745\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.001492586261187193\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611871966\n",
      "log loss =  102638.38315492398\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611872198\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611871537\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611871834\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.001492586261187161\n",
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611872257\n",
      "log loss =  102669.13626494212\n",
      "Epoch 0, loss: 11.438100\n",
      "== W == -0.013076976110252804\n",
      "log loss =  102665.62657377284\n",
      "Epoch 1, loss: 11.431931\n",
      "== W == -0.01039618721767805\n",
      "log loss =  102662.56962997033\n",
      "Epoch 2, loss: 11.426918\n",
      "== W == -0.008215310722975752\n",
      "log loss =  102659.8937160515\n",
      "Epoch 3, loss: 11.422841\n",
      "== W == -0.006456935123224199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102657.54167266138\n",
      "Epoch 4, loss: 11.419524\n",
      "== W == -0.005051803125814363\n",
      "log loss =  102655.46728013971\n",
      "Epoch 5, loss: 11.416823\n",
      "== W == -0.003939317791382083\n",
      "log loss =  102653.632654055\n",
      "Epoch 6, loss: 11.414621\n",
      "== W == -0.0030673314116571315\n",
      "log loss =  102652.0063529282\n",
      "Epoch 7, loss: 11.412826\n",
      "== W == -0.0023915410931257593\n",
      "log loss =  102650.56198893918\n",
      "Epoch 8, loss: 11.411361\n",
      "== W == -0.0018746960401104726\n",
      "log loss =  102649.27719607465\n",
      "Epoch 9, loss: 11.410164\n",
      "== W == -0.0014857430329906947\n",
      "log loss =  102648.13285409039\n",
      "Epoch 0, loss: 11.409186\n",
      "== W == -0.0011989852078417105\n",
      "log loss =  102647.11249703346\n",
      "Epoch 1, loss: 11.408385\n",
      "== W == -0.000993295982948745\n",
      "log loss =  102646.2018561342\n",
      "Epoch 2, loss: 11.407730\n",
      "== W == -0.0008514088005064282\n",
      "log loss =  102645.38850152814\n",
      "Epoch 3, loss: 11.407192\n",
      "== W == -0.0007592901859089854\n",
      "log loss =  102644.66155748774\n",
      "Epoch 4, loss: 11.406750\n",
      "== W == -0.0007055957306413788\n",
      "log loss =  102644.0114729995\n",
      "Epoch 5, loss: 11.406387\n",
      "== W == -0.000681204135995709\n",
      "log loss =  102643.42983455067\n",
      "Epoch 6, loss: 11.406087\n",
      "== W == -0.0006788221808408544\n",
      "log loss =  102642.90921154103\n",
      "Epoch 7, loss: 11.405841\n",
      "== W == -0.0006926525768757002\n",
      "log loss =  102642.443027256\n",
      "Epoch 8, loss: 11.405637\n",
      "== W == -0.0007181166072315259\n",
      "log loss =  102642.02545013749\n",
      "Epoch 9, loss: 11.405469\n",
      "== W == -0.000751623854839214\n",
      "log loss =  102641.65130138387\n",
      "Epoch 0, loss: 11.405329\n",
      "== W == -0.0007903819876906295\n",
      "log loss =  102641.31597584939\n",
      "Epoch 1, loss: 11.405213\n",
      "== W == -0.000832240335307312\n",
      "log loss =  102641.0153738996\n",
      "Epoch 2, loss: 11.405116\n",
      "== W == -0.0008755617765488742\n",
      "log loss =  102640.74584238748\n",
      "Epoch 3, loss: 11.405036\n",
      "== W == -0.0009191182121614022\n",
      "log loss =  102640.50412329266\n",
      "Epoch 4, loss: 11.404968\n",
      "== W == -0.0009620055887041845\n",
      "log loss =  102640.28730885373\n",
      "Epoch 5, loss: 11.404912\n",
      "== W == -0.0010035750612661471\n",
      "log loss =  102640.09280224094\n",
      "Epoch 6, loss: 11.404865\n",
      "== W == -0.001043377427532482\n",
      "log loss =  102639.91828298735\n",
      "Epoch 7, loss: 11.404825\n",
      "== W == -0.0010811184376066554\n",
      "log loss =  102639.76167652795\n",
      "Epoch 8, loss: 11.404791\n",
      "== W == -0.0011166229878672503\n",
      "log loss =  102639.6211273033\n",
      "Epoch 9, loss: 11.404763\n",
      "== W == -0.0011498065498069658\n",
      "log loss =  102639.49497496638\n",
      "Epoch 0, loss: 11.404739\n",
      "== W == -0.001180652473488489\n",
      "log loss =  102639.38173330281\n",
      "Epoch 1, loss: 11.404718\n",
      "== W == -0.001209194047047957\n",
      "log loss =  102639.28007152895\n",
      "Epoch 2, loss: 11.404701\n",
      "== W == -0.0012355003952150602\n",
      "log loss =  102639.1887976795\n",
      "Epoch 3, loss: 11.404686\n",
      "== W == -0.0012596654671063886\n",
      "log loss =  102639.10684383629\n",
      "Epoch 4, loss: 11.404673\n",
      "== W == -0.0012817995019102299\n",
      "log loss =  102639.03325298088\n",
      "Epoch 5, loss: 11.404662\n",
      "== W == -0.001302022475157027\n",
      "log loss =  102638.96716728361\n",
      "Epoch 6, loss: 11.404652\n",
      "== W == -0.0013204591220592602\n",
      "log loss =  102638.90781766368\n",
      "Epoch 7, loss: 11.404644\n",
      "== W == -0.0013372352113166998\n",
      "log loss =  102638.85451447623\n",
      "Epoch 8, loss: 11.404637\n",
      "== W == -0.001352474805705666\n",
      "log loss =  102638.8066391998\n",
      "Epoch 9, loss: 11.404631\n",
      "== W == -0.0013662982971333537\n",
      "log loss =  102638.76363701231\n",
      "Epoch 0, loss: 11.404625\n",
      "== W == -0.001378821045673921\n",
      "log loss =  102638.72501015774\n",
      "Epoch 1, loss: 11.404620\n",
      "== W == -0.001390152486107427\n",
      "log loss =  102638.69031201582\n",
      "Epoch 2, loss: 11.404616\n",
      "== W == -0.0014003955930633875\n",
      "log loss =  102638.659141799\n",
      "Epoch 3, loss: 11.404613\n",
      "== W == -0.0014096466181941327\n",
      "log loss =  102638.63113980742\n",
      "Epoch 4, loss: 11.404610\n",
      "== W == -0.0014179950308312018\n",
      "log loss =  102638.60598318267\n",
      "Epoch 5, loss: 11.404607\n",
      "== W == -0.0014255236081037864\n",
      "log loss =  102638.583382106\n",
      "Epoch 6, loss: 11.404604\n",
      "== W == -0.001432308632174292\n",
      "log loss =  102638.56307639371\n",
      "Epoch 7, loss: 11.404602\n",
      "== W == -0.0014384201616055462\n",
      "log loss =  102638.54483244753\n",
      "Epoch 8, loss: 11.404600\n",
      "== W == -0.001443922351356184\n",
      "log loss =  102638.52844052194\n",
      "Epoch 9, loss: 11.404599\n",
      "== W == -0.0014488738018624764\n",
      "log loss =  102638.51371227522\n",
      "Epoch 0, loss: 11.404597\n",
      "== W == -0.0014533279223982844\n",
      "log loss =  102638.50047857415\n",
      "Epoch 1, loss: 11.404596\n",
      "== W == -0.0014573332976483646\n",
      "log loss =  102638.48858752585\n",
      "Epoch 2, loss: 11.404595\n",
      "== W == -0.0014609340493769041\n",
      "log loss =  102638.47790271271\n",
      "Epoch 3, loss: 11.404593\n",
      "== W == -0.0014641701873803867\n",
      "log loss =  102638.46830160935\n",
      "Epoch 4, loss: 11.404593\n",
      "== W == -0.0014670779457093444\n",
      "log loss =  102638.45967416272\n",
      "Epoch 5, loss: 11.404592\n",
      "== W == -0.0014696901015301977\n",
      "log loss =  102638.45192151798\n",
      "Epoch 6, loss: 11.404591\n",
      "== W == -0.001472036275059534\n",
      "log loss =  102638.44495487548\n",
      "Epoch 7, loss: 11.404590\n",
      "== W == -0.0014741432098066963\n",
      "log loss =  102638.43869446496\n",
      "Epoch 8, loss: 11.404590\n",
      "== W == -0.00147603503295985\n",
      "log loss =  102638.43306862476\n",
      "Epoch 9, loss: 11.404589\n",
      "== W == -0.0014777334961900384\n",
      "log loss =  102638.42801297567\n",
      "Epoch 0, loss: 11.404589\n",
      "== W == -0.0014792581974614823\n",
      "log loss =  102638.42346967898\n",
      "Epoch 1, loss: 11.404588\n",
      "== W == -0.001480626784652904\n",
      "log loss =  102638.41938677047\n",
      "Epoch 2, loss: 11.404588\n",
      "== W == -0.0014818551419356638\n",
      "log loss =  102638.41571756282\n",
      "Epoch 3, loss: 11.404588\n",
      "== W == -0.001482957559938621\n",
      "log loss =  102638.41242010839\n",
      "Epoch 4, loss: 11.404587\n",
      "== W == -0.0014839468907694245\n",
      "log loss =  102638.40945671742\n",
      "Epoch 5, loss: 11.404587\n",
      "== W == -0.0014848346889704857\n",
      "log loss =  102638.40679352506\n",
      "Epoch 6, loss: 11.404587\n",
      "== W == -0.0014856313394716028\n",
      "log loss =  102638.40440010256\n",
      "Epoch 7, loss: 11.404587\n",
      "== W == -0.0014863461735689012\n",
      "log loss =  102638.40224910824\n",
      "Epoch 8, loss: 11.404586\n",
      "== W == -0.0014869875739155548\n",
      "log loss =  102638.40031597373\n",
      "Epoch 9, loss: 11.404586\n",
      "== W == -0.0014875630694577168\n",
      "log loss =  102638.39857862248\n",
      "Epoch 0, loss: 11.404586\n",
      "== W == -0.001488079421193175\n",
      "log loss =  102638.39701721654\n",
      "Epoch 1, loss: 11.404586\n",
      "== W == -0.001488542699571853\n",
      "log loss =  102638.39561392956\n",
      "Epoch 2, loss: 11.404586\n",
      "== W == -0.0014889583542985376\n",
      "log loss =  102638.39435274235\n",
      "Epoch 3, loss: 11.404586\n",
      "== W == -0.001489331277240667\n",
      "log loss =  102638.39321925971\n",
      "Epoch 4, loss: 11.404586\n",
      "== W == -0.0014896658590879417\n",
      "log loss =  102638.39220054555\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014899660403571383\n",
      "log loss =  102638.3912849748\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014902353572850253\n",
      "log loss =  102638.39046210061\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014904769831044513\n",
      "log loss =  102638.38972253469\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014906937651545172\n",
      "log loss =  102638.38905784009\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014908882582347158\n",
      "log loss =  102638.38846043462\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014910627545745795\n",
      "log loss =  102638.38792350433\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014912193107556265\n",
      "log loss =  102638.38744092548\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014913597718904372\n",
      "log loss =  102638.38700719472\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014914857933339973\n",
      "log loss =  102638.38661736608\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014915988601761224\n",
      "log loss =  102638.38626699465\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014917003047391565\n",
      "log loss =  102638.38595208565\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014917913222831309\n",
      "log loss =  102638.38566904882\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014918729851006313\n",
      "log loss =  102638.38541465765\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014919462551651118\n",
      "log loss =  102638.3851860122\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014920119954804625\n",
      "log loss =  102638.38498050625\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014920709802644016\n",
      "log loss =  102638.38479579752\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014921239040850396\n",
      "log loss =  102638.38462978085\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014921713900577906\n",
      "log loss =  102638.38448056424\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014922139971991826\n",
      "log loss =  102638.38434644722\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014922522270238233\n",
      "log loss =  102638.38422590152\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014922865294625196\n",
      "log loss =  102638.38411755358\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.001492317308171131\n",
      "log loss =  102638.38402016886\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.001492344925292929\n",
      "log loss =  102638.38393263787\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014923697057306218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102638.38385396331\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.001492391940978512\n",
      "log loss =  102638.38378324898\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014924118925601613\n",
      "log loss =  102638.38371968926\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014924297951119767\n",
      "log loss =  102638.38366256018\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001492445859149442\n",
      "log loss =  102638.38361121099\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014924602735484382\n",
      "log loss =  102638.38356505678\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014924732077711818\n",
      "log loss =  102638.38352357195\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014924848138630163\n",
      "log loss =  102638.38348628397\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014924952282436192\n",
      "log loss =  102638.38345276818\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925045733138894\n",
      "log loss =  102638.38342264295\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925129588974323\n",
      "log loss =  102638.38339556518\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.001492520483533731\n",
      "log loss =  102638.38337122656\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925272356382181\n",
      "log loss =  102638.38334934996\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925332945430528\n",
      "log loss =  102638.38332968626\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.00149253873143075\n",
      "log loss =  102638.3833120116\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925436101718282\n",
      "log loss =  102638.38329612477\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.001492547988076275\n",
      "log loss =  102638.38328184483\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925519165677813\n",
      "log loss =  102638.38326900931\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925554417885257\n",
      "log loss =  102638.38325747201\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.001492558605141903\n",
      "log loss =  102638.38324710165\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925614437793836\n",
      "log loss =  102638.38323778017\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925639910373268\n",
      "log loss =  102638.38322940146\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925662768289173\n",
      "log loss =  102638.38322187016\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925683279957782\n",
      "log loss =  102638.38321510056\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925701686234916\n",
      "log loss =  102638.38320901556\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925718203246574\n",
      "log loss =  102638.383203546\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925733024928954\n",
      "log loss =  102638.38319862958\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925746325307044\n",
      "log loss =  102638.38319421033\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925758260539831\n",
      "log loss =  102638.383190238\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925768970754628\n",
      "log loss =  102638.38318666737\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925778581693244\n",
      "log loss =  102638.38318345783\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925787206189426\n",
      "log loss =  102638.38318057286\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925794945493955\n",
      "log loss =  102638.3831779796\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925801890463886\n",
      "log loss =  102638.38317564857\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925808122629965\n",
      "log loss =  102638.38317355327\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.001492581371515373\n",
      "log loss =  102638.38317166982\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925818733687115\n",
      "log loss =  102638.38316997682\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925823237143098\n",
      "log loss =  102638.38316845501\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925827278387599\n",
      "log loss =  102638.38316708707\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925830904860258\n",
      "log loss =  102638.38316585743\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925834159131583\n",
      "log loss =  102638.38316475213\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925837079402607\n",
      "log loss =  102638.38316375857\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925839699953293\n",
      "log loss =  102638.38316286549\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925842051545276\n",
      "log loss =  102638.3831620627\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925844161782977\n",
      "log loss =  102638.38316134104\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925846055437592\n",
      "log loss =  102638.38316069238\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925847754737806\n",
      "log loss =  102638.38316010928\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925849279630848\n",
      "log loss =  102638.38315958514\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.001492585064801684\n",
      "log loss =  102638.38315911398\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925851875958653\n",
      "log loss =  102638.38315869044\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.001492585297787045\n",
      "log loss =  102638.38315830973\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925853966687216\n",
      "log loss =  102638.3831579675\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925854854016152\n",
      "log loss =  102638.38315765989\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925855650273455\n",
      "log loss =  102638.38315738336\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925856364806126\n",
      "log loss =  102638.38315713478\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925857006001667\n",
      "log loss =  102638.38315691132\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925857581387272\n",
      "log loss =  102638.38315671048\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925858097717066\n",
      "log loss =  102638.3831565299\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925858561052256\n",
      "log loss =  102638.3831563676\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925858976831859\n",
      "log loss =  102638.38315622171\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925859349936754\n",
      "log loss =  102638.38315609055\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925859684746866\n",
      "log loss =  102638.38315597265\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.001492585998519265\n",
      "log loss =  102638.38315586667\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925860254801045\n",
      "log loss =  102638.38315577141\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001492586049673703\n",
      "log loss =  102638.38315568576\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.001492586071384089\n",
      "log loss =  102638.38315560878\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925860908661223\n",
      "log loss =  102638.38315553959\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925861083485133\n",
      "log loss =  102638.38315547738\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925861240365043\n",
      "log loss =  102638.38315542146\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925861381142702\n",
      "log loss =  102638.38315537118\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925861507470752\n",
      "log loss =  102638.38315532598\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925861620832277\n",
      "log loss =  102638.38315528537\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925861722558157\n",
      "log loss =  102638.38315524884\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925861813842641\n",
      "log loss =  102638.38315521603\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001492586189575744\n",
      "log loss =  102638.38315518653\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925861969264224\n",
      "log loss =  102638.38315515999\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862035226032\n",
      "log loss =  102638.38315513615\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.001492586209441722\n",
      "log loss =  102638.38315511472\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862147532852\n",
      "log loss =  102638.38315509545\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862195196342\n",
      "log loss =  102638.38315507812\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862237967465\n",
      "log loss =  102638.38315506253\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862276348356\n",
      "log loss =  102638.38315504853\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.001492586231078959\n",
      "log loss =  102638.38315503596\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862341695572\n",
      "log loss =  102638.38315502464\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862369429166\n",
      "log loss =  102638.38315501448\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.001492586239431595\n",
      "log loss =  102638.38315500534\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862416648222\n",
      "log loss =  102638.38315499712\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862436688093\n",
      "log loss =  102638.38315498971\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.001492586245467096\n",
      "log loss =  102638.38315498308\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.00149258624708079\n",
      "log loss =  102638.38315497711\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.001492586248528837\n",
      "log loss =  102638.38315497176\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.001492586249828247\n",
      "log loss =  102638.38315496693\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862509942772\n",
      "log loss =  102638.38315496259\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862520406088\n",
      "log loss =  102638.38315495869\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862529795294\n",
      "log loss =  102638.38315495518\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862538220785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102638.38315495204\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862545781341\n",
      "log loss =  102638.3831549492\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862552565819\n",
      "log loss =  102638.38315494666\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862558653863\n",
      "log loss =  102638.38315494436\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.001492586256411691\n",
      "log loss =  102638.3831549423\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862569019229\n",
      "log loss =  102638.38315494046\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862573418355\n",
      "log loss =  102638.3831549388\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.001492586257736584\n",
      "log loss =  102638.38315493731\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862580908123\n",
      "log loss =  102638.38315493596\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862584086818\n",
      "log loss =  102638.38315493475\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862586939102\n",
      "log loss =  102638.38315493366\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862589498654\n",
      "log loss =  102638.38315493269\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862591795443\n",
      "log loss =  102638.38315493181\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862593856462\n",
      "log loss =  102638.38315493101\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862595705905\n",
      "log loss =  102638.38315493031\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.001492586259736541\n",
      "log loss =  102638.38315492967\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862598854666\n",
      "log loss =  102638.38315492909\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862600190956\n",
      "log loss =  102638.38315492858\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862601390086\n",
      "log loss =  102638.38315492813\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862602466196\n",
      "log loss =  102638.38315492771\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862603431767\n",
      "log loss =  102638.38315492733\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862604298244\n",
      "log loss =  102638.38315492698\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862605075745\n",
      "log loss =  102638.38315492669\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862605773455\n",
      "log loss =  102638.38315492643\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862606399525\n",
      "log loss =  102638.38315492617\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862606961298\n",
      "log loss =  102638.38315492596\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862607465378\n",
      "log loss =  102638.38315492574\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862607917827\n",
      "log loss =  102638.38315492558\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862608323763\n",
      "log loss =  102638.38315492542\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001492586260868808\n",
      "log loss =  102638.3831549253\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862609014913\n",
      "log loss =  102638.38315492513\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862609308227\n",
      "log loss =  102638.38315492503\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862609571376\n",
      "log loss =  102638.38315492492\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862609807578\n",
      "log loss =  102638.38315492483\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862610019555\n",
      "log loss =  102638.38315492474\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.001492586261020974\n",
      "log loss =  102638.38315492467\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862610380373\n",
      "log loss =  102638.3831549246\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.001492586261053345\n",
      "log loss =  102638.38315492454\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862610670913\n",
      "log loss =  102638.38315492448\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001492586261079417\n",
      "log loss =  102638.38315492444\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.001492586261090488\n",
      "log loss =  102638.38315492438\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.001492586261100416\n",
      "log loss =  102638.38315492436\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.001492586261109331\n",
      "log loss =  102638.3831549243\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611173202\n",
      "log loss =  102638.38315492429\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611244957\n",
      "log loss =  102638.38315492426\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.001492586261130927\n",
      "log loss =  102638.38315492422\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.001492586261136708\n",
      "log loss =  102638.3831549242\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611418926\n",
      "log loss =  102638.38315492419\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611465423\n",
      "log loss =  102638.38315492416\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001492586261150714\n",
      "log loss =  102638.38315492414\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611544602\n",
      "log loss =  102638.38315492414\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611578217\n",
      "log loss =  102638.38315492411\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611608375\n",
      "log loss =  102638.38315492411\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611635467\n",
      "log loss =  102638.3831549241\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611659723\n",
      "log loss =  102638.3831549241\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611681476\n",
      "log loss =  102638.38315492407\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.001492586261170107\n",
      "log loss =  102638.38315492407\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611718567\n",
      "log loss =  102638.38315492406\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611734324\n",
      "log loss =  102638.38315492406\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001492586261174854\n",
      "log loss =  102638.38315492406\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.001492586261176118\n",
      "log loss =  102638.38315492406\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611772553\n",
      "log loss =  102638.38315492403\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.001492586261178277\n",
      "log loss =  102638.38315492404\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611791971\n",
      "log loss =  102638.38315492403\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611800274\n",
      "log loss =  102638.38315492403\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611807558\n",
      "log loss =  102638.38315492403\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611814195\n",
      "log loss =  102638.38315492401\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611820098\n",
      "log loss =  102638.38315492403\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611825467\n",
      "log loss =  102638.38315492403\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001492586261183022\n",
      "log loss =  102638.38315492403\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.001492586261183451\n",
      "log loss =  102638.38315492401\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611838332\n",
      "log loss =  102638.38315492403\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611841825\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611844904\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.001492586261184763\n",
      "log loss =  102638.38315492401\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611850102\n",
      "log loss =  102638.38315492401\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611852331\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611854372\n",
      "log loss =  102638.38315492401\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611856206\n",
      "log loss =  102638.38315492398\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611857845\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611859268\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.001492586261186058\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611861716\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.001492586261186275\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611863722\n",
      "log loss =  102638.38315492398\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.0014925862611864596\n",
      "log loss =  102638.38315492398\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.0014925862611865355\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611866088\n",
      "log loss =  102638.38315492398\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.001492586261186664\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.0014925862611867178\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.0014925862611867653\n",
      "log loss =  102638.38315492398\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.001492586261186808\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611868425\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611868813\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611869115\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.001492586261186936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102638.383154924\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.00149258626118697\n",
      "log loss =  102638.383154924\n",
      "Epoch 0, loss: 11.404585\n",
      "== W == -0.0014925862611869895\n",
      "log loss =  102638.38315492398\n",
      "Epoch 1, loss: 11.404585\n",
      "== W == -0.0014925862611870106\n",
      "log loss =  102638.383154924\n",
      "Epoch 2, loss: 11.404585\n",
      "== W == -0.001492586261187035\n",
      "log loss =  102638.383154924\n",
      "Epoch 3, loss: 11.404585\n",
      "== W == -0.00149258626118705\n",
      "log loss =  102638.383154924\n",
      "Epoch 4, loss: 11.404585\n",
      "== W == -0.0014925862611870633\n",
      "log loss =  102638.383154924\n",
      "Epoch 5, loss: 11.404585\n",
      "== W == -0.0014925862611870763\n",
      "log loss =  102638.383154924\n",
      "Epoch 6, loss: 11.404585\n",
      "== W == -0.0014925862611870869\n",
      "log loss =  102638.383154924\n",
      "Epoch 7, loss: 11.404585\n",
      "== W == -0.0014925862611870975\n",
      "log loss =  102638.383154924\n",
      "Epoch 8, loss: 11.404585\n",
      "== W == -0.001492586261187106\n",
      "log loss =  102638.38315492401\n",
      "Epoch 9, loss: 11.404585\n",
      "== W == -0.001492586261187115\n",
      "log loss =  102667.135577723\n",
      "Epoch 0, loss: 11.438502\n",
      "== W == 0.24952798366912357\n",
      "log loss =  102666.80608056767\n",
      "Epoch 1, loss: 11.437848\n",
      "== W == 0.24527002928329156\n",
      "log loss =  102666.48097689755\n",
      "Epoch 2, loss: 11.437206\n",
      "== W == 0.24108506616588626\n",
      "log loss =  102666.160190625\n",
      "Epoch 3, loss: 11.436578\n",
      "== W == 0.236971821606925\n",
      "log loss =  102665.84364743327\n",
      "Epoch 4, loss: 11.435961\n",
      "== W == 0.23292904574801637\n",
      "log loss =  102665.53127472571\n",
      "Epoch 5, loss: 11.435357\n",
      "== W == 0.22895551115096197\n",
      "log loss =  102665.22300157648\n",
      "Epoch 6, loss: 11.434764\n",
      "== W == 0.22505001237518626\n",
      "log loss =  102664.918758683\n",
      "Epoch 7, loss: 11.434183\n",
      "== W == 0.2212113655637885\n",
      "log loss =  102664.61847831967\n",
      "Epoch 8, loss: 11.433614\n",
      "== W == 0.2174384080380232\n",
      "log loss =  102664.3220942934\n",
      "Epoch 9, loss: 11.433056\n",
      "== W == 0.2137299979000145\n",
      "log loss =  102664.02954190024\n",
      "Epoch 0, loss: 11.432508\n",
      "== W == 0.210085013643518\n",
      "log loss =  102663.74075788353\n",
      "Epoch 1, loss: 11.431972\n",
      "== W == 0.20650235377254927\n",
      "log loss =  102663.45568039337\n",
      "Epoch 2, loss: 11.431445\n",
      "== W == 0.20298093642769932\n",
      "log loss =  102663.17424894738\n",
      "Epoch 3, loss: 11.430930\n",
      "== W == 0.19951969901996588\n",
      "log loss =  102662.89640439265\n",
      "Epoch 4, loss: 11.430424\n",
      "== W == 0.19611759787193206\n",
      "log loss =  102662.6220888689\n",
      "Epoch 5, loss: 11.429928\n",
      "== W == 0.19277360786612685\n",
      "log loss =  102662.35124577284\n",
      "Epoch 6, loss: 11.429442\n",
      "== W == 0.1894867221004094\n",
      "log loss =  102662.08381972354\n",
      "Epoch 7, loss: 11.428965\n",
      "== W == 0.18625595155022026\n",
      "log loss =  102661.81975652918\n",
      "Epoch 8, loss: 11.428498\n",
      "== W == 0.18308032473754754\n",
      "log loss =  102661.55900315428\n",
      "Epoch 9, loss: 11.428040\n",
      "== W == 0.17995888740646054\n",
      "log loss =  102661.30150768855\n",
      "Epoch 0, loss: 11.427591\n",
      "== W == 0.17689070220506684\n",
      "log loss =  102661.04721931637\n",
      "Epoch 1, loss: 11.427151\n",
      "== W == 0.17387484837375045\n",
      "log loss =  102660.79608828727\n",
      "Epoch 2, loss: 11.426719\n",
      "== W == 0.17091042143955612\n",
      "log loss =  102660.54806588733\n",
      "Epoch 3, loss: 11.426296\n",
      "== W == 0.16799653291658412\n",
      "log loss =  102660.30310441158\n",
      "Epoch 4, loss: 11.425880\n",
      "== W == 0.16513231001226625\n",
      "log loss =  102660.06115713698\n",
      "Epoch 5, loss: 11.425474\n",
      "== W == 0.1623168953393952\n",
      "log loss =  102659.82217829661\n",
      "Epoch 6, loss: 11.425075\n",
      "== W == 0.15954944663378326\n",
      "log loss =  102659.58612305427\n",
      "Epoch 7, loss: 11.424683\n",
      "== W == 0.15682913647742946\n",
      "log loss =  102659.35294748009\n",
      "Epoch 8, loss: 11.424300\n",
      "== W == 0.1541551520270768\n",
      "log loss =  102659.12260852683\n",
      "Epoch 9, loss: 11.423924\n",
      "== W == 0.15152669474804434\n",
      "log loss =  102658.89506400688\n",
      "Epoch 0, loss: 11.423555\n",
      "== W == 0.14894298015322155\n",
      "log loss =  102658.67027257002\n",
      "Epoch 1, loss: 11.423193\n",
      "== W == 0.14640323754711604\n",
      "log loss =  102658.44819368172\n",
      "Epoch 2, loss: 11.422839\n",
      "== W == 0.1439067097748457\n",
      "log loss =  102658.22878760222\n",
      "Epoch 3, loss: 11.422491\n",
      "== W == 0.1414526529759725\n",
      "log loss =  102658.0120153663\n",
      "Epoch 4, loss: 11.422150\n",
      "== W == 0.1390403363430754\n",
      "log loss =  102657.79783876339\n",
      "Epoch 5, loss: 11.421816\n",
      "== W == 0.1366690418849615\n",
      "log loss =  102657.5862203186\n",
      "Epoch 6, loss: 11.421489\n",
      "== W == 0.1343380641944204\n",
      "log loss =  102657.37712327405\n",
      "Epoch 7, loss: 11.421167\n",
      "== W == 0.1320467102204244\n",
      "log loss =  102657.17051157099\n",
      "Epoch 8, loss: 11.420852\n",
      "== W == 0.12979429904468381\n",
      "log loss =  102656.96634983225\n",
      "Epoch 9, loss: 11.420543\n",
      "== W == 0.12758016166246508\n",
      "log loss =  102656.76460334528\n",
      "Epoch 0, loss: 11.420240\n",
      "== W == 0.12540364076758484\n",
      "log loss =  102656.56523804585\n",
      "Epoch 1, loss: 11.419943\n",
      "== W == 0.12326409054149214\n",
      "log loss =  102656.3682205019\n",
      "Epoch 2, loss: 11.419652\n",
      "== W == 0.12116087644635531\n",
      "log loss =  102656.17351789822\n",
      "Epoch 3, loss: 11.419367\n",
      "== W == 0.11909337502207093\n",
      "log loss =  102655.9810980213\n",
      "Epoch 4, loss: 11.419087\n",
      "== W == 0.11706097368711321\n",
      "log loss =  102655.79092924485\n",
      "Epoch 5, loss: 11.418812\n",
      "== W == 0.1150630705431476\n",
      "log loss =  102655.60298051548\n",
      "Epoch 6, loss: 11.418543\n",
      "== W == 0.11309907418332849\n",
      "log loss =  102655.41722133913\n",
      "Epoch 7, loss: 11.418279\n",
      "== W == 0.1111684035042087\n",
      "log loss =  102655.23362176755\n",
      "Epoch 8, loss: 11.418020\n",
      "== W == 0.10927048752118557\n",
      "log loss =  102655.05215238544\n",
      "Epoch 9, loss: 11.417766\n",
      "== W == 0.10740476518741254\n",
      "log loss =  102654.87278429781\n",
      "Epoch 0, loss: 11.417517\n",
      "== W == 0.10557068521610632\n",
      "log loss =  102654.69548911779\n",
      "Epoch 1, loss: 11.417273\n",
      "== W == 0.10376770590618006\n",
      "log loss =  102654.52023895465\n",
      "Epoch 2, loss: 11.417033\n",
      "== W == 0.10199529497113673\n",
      "log loss =  102654.34700640241\n",
      "Epoch 3, loss: 11.416799\n",
      "== W == 0.1002529293711565\n",
      "log loss =  102654.17576452845\n",
      "Epoch 4, loss: 11.416568\n",
      "== W == 0.09854009514831386\n",
      "log loss =  102654.00648686278\n",
      "Epoch 5, loss: 11.416343\n",
      "== W == 0.09685628726486259\n",
      "log loss =  102653.83914738735\n",
      "Epoch 6, loss: 11.416121\n",
      "== W == 0.09520100944452639\n",
      "log loss =  102653.67372052578\n",
      "Epoch 7, loss: 11.415904\n",
      "== W == 0.09357377401673647\n",
      "log loss =  102653.5101811334\n",
      "Epoch 8, loss: 11.415691\n",
      "== W == 0.09197410176375559\n",
      "log loss =  102653.34850448747\n",
      "Epoch 9, loss: 11.415483\n",
      "== W == 0.09040152177063379\n",
      "log loss =  102653.18866627781\n",
      "Epoch 0, loss: 11.415278\n",
      "== W == 0.08885557127793772\n",
      "log loss =  102653.03064259751\n",
      "Epoch 1, loss: 11.415077\n",
      "== W == 0.08733579553719938\n",
      "log loss =  102652.87440993411\n",
      "Epoch 2, loss: 11.414880\n",
      "== W == 0.0858417476690313\n",
      "log loss =  102652.71994516085\n",
      "Epoch 3, loss: 11.414687\n",
      "== W == 0.08437298852385428\n",
      "log loss =  102652.56722552821\n",
      "Epoch 4, loss: 11.414498\n",
      "== W == 0.0829290865451885\n",
      "log loss =  102652.41622865581\n",
      "Epoch 5, loss: 11.414312\n",
      "== W == 0.08150961763545506\n",
      "log loss =  102652.26693252422\n",
      "Epoch 6, loss: 11.414130\n",
      "== W == 0.0801141650242415\n",
      "log loss =  102652.11931546741\n",
      "Epoch 7, loss: 11.413951\n",
      "== W == 0.07874231913898123\n",
      "log loss =  102651.97335616504\n",
      "Epoch 8, loss: 11.413776\n",
      "== W == 0.07739367747800092\n",
      "log loss =  102651.82903363515\n",
      "Epoch 9, loss: 11.413604\n",
      "== W == 0.07606784448588948\n",
      "log loss =  102651.68632722701\n",
      "Epoch 0, loss: 11.413436\n",
      "== W == 0.07476443143114328\n",
      "log loss =  102651.54521661412\n",
      "Epoch 1, loss: 11.413270\n",
      "== W == 0.0734830562860444\n",
      "log loss =  102651.40568178751\n",
      "Epoch 2, loss: 11.413108\n",
      "== W == 0.0722233436087272\n",
      "log loss =  102651.267703049\n",
      "Epoch 3, loss: 11.412949\n",
      "== W == 0.07098492442739274\n",
      "log loss =  102651.13126100492\n",
      "Epoch 4, loss: 11.412793\n",
      "== W == 0.06976743612662836\n",
      "log loss =  102650.99633655975\n",
      "Epoch 5, loss: 11.412641\n",
      "== W == 0.06857052233579239\n",
      "log loss =  102650.86291091009\n",
      "Epoch 6, loss: 11.412491\n",
      "== W == 0.06739383281942447\n",
      "log loss =  102650.73096553868\n",
      "Epoch 7, loss: 11.412344\n",
      "== W == 0.06623702336964227\n",
      "log loss =  102650.6004822087\n",
      "Epoch 8, loss: 11.412199\n",
      "== W == 0.06509975570048707\n",
      "log loss =  102650.47144295799\n",
      "Epoch 9, loss: 11.412058\n",
      "== W == 0.0639816973441806\n",
      "log loss =  102650.34383009376\n",
      "Epoch 0, loss: 11.411919\n",
      "== W == 0.06288252154925665\n",
      "log loss =  102650.21762618708\n",
      "Epoch 1, loss: 11.411783\n",
      "== W == 0.06180190718053233\n",
      "log loss =  102650.09281406782\n",
      "Epoch 2, loss: 11.411650\n",
      "== W == 0.06073953862088313\n",
      "log loss =  102649.96937681938\n",
      "Epoch 3, loss: 11.411519\n",
      "== W == 0.05969510567478839\n",
      "log loss =  102649.84729777402\n",
      "Epoch 4, loss: 11.411390\n",
      "== W == 0.05866830347361294\n",
      "log loss =  102649.72656050776\n",
      "Epoch 5, loss: 11.411264\n",
      "== W == 0.057658832382592715\n",
      "log loss =  102649.60714883587\n",
      "Epoch 6, loss: 11.411141\n",
      "== W == 0.056666397909491455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102649.48904680824\n",
      "Epoch 7, loss: 11.411020\n",
      "== W == 0.05569071061489765\n",
      "log loss =  102649.37223870485\n",
      "Epoch 8, loss: 11.410901\n",
      "== W == 0.05473148602413039\n",
      "log loss =  102649.25670903153\n",
      "Epoch 9, loss: 11.410784\n",
      "== W == 0.053788444540723984\n",
      "log loss =  102649.14244251559\n",
      "Epoch 0, loss: 11.410670\n",
      "== W == 0.05286131136146189\n",
      "log loss =  102649.02942410178\n",
      "Epoch 1, loss: 11.410558\n",
      "== W == 0.05194981639293052\n",
      "log loss =  102648.91763894819\n",
      "Epoch 2, loss: 11.410448\n",
      "== W == 0.05105369416956483\n",
      "log loss =  102648.80707242226\n",
      "Epoch 3, loss: 11.410340\n",
      "== W == 0.05017268377315779\n",
      "log loss =  102648.6977100971\n",
      "Epoch 4, loss: 11.410234\n",
      "== W == 0.04930652875380599\n",
      "log loss =  102648.58953774752\n",
      "Epoch 5, loss: 11.410130\n",
      "== W == 0.04845497705226506\n",
      "log loss =  102648.48254134656\n",
      "Epoch 6, loss: 11.410028\n",
      "== W == 0.0476177809236887\n",
      "log loss =  102648.37670706167\n",
      "Epoch 7, loss: 11.409928\n",
      "== W == 0.04679469686272503\n",
      "log loss =  102648.27202125153\n",
      "Epoch 8, loss: 11.409830\n",
      "== W == 0.04598548552994575\n",
      "log loss =  102648.16847046239\n",
      "Epoch 9, loss: 11.409734\n",
      "== W == 0.04518991167958279\n",
      "log loss =  102648.06604142478\n",
      "Epoch 0, loss: 11.409640\n",
      "== W == 0.044407744088548765\n",
      "log loss =  102647.96472105039\n",
      "Epoch 1, loss: 11.409547\n",
      "== W == 0.04363875548671721\n",
      "log loss =  102647.86449642878\n",
      "Epoch 2, loss: 11.409456\n",
      "== W == 0.04288272248843909\n",
      "log loss =  102647.76535482422\n",
      "Epoch 3, loss: 11.409367\n",
      "== W == 0.042139425525273486\n",
      "log loss =  102647.66728367281\n",
      "Epoch 4, loss: 11.409280\n",
      "== W == 0.04140864877990931\n",
      "log loss =  102647.5702705794\n",
      "Epoch 5, loss: 11.409194\n",
      "== W == 0.040690180121256615\n",
      "log loss =  102647.47430331478\n",
      "Epoch 6, loss: 11.409110\n",
      "== W == 0.03998381104068585\n",
      "log loss =  102647.37936981276\n",
      "Epoch 7, loss: 11.409028\n",
      "== W == 0.039289336589393944\n",
      "log loss =  102647.28545816746\n",
      "Epoch 8, loss: 11.408947\n",
      "== W == 0.03860655531687673\n",
      "log loss =  102647.19255663062\n",
      "Epoch 9, loss: 11.408868\n",
      "== W == 0.03793526921048706\n",
      "log loss =  102647.10065360887\n",
      "Epoch 0, loss: 11.408790\n",
      "== W == 0.037275283636059295\n",
      "log loss =  102647.00973766127\n",
      "Epoch 1, loss: 11.408713\n",
      "== W == 0.036626407279580296\n",
      "log loss =  102646.91979749667\n",
      "Epoch 2, loss: 11.408638\n",
      "== W == 0.03598845208988786\n",
      "log loss =  102646.83082197129\n",
      "Epoch 3, loss: 11.408565\n",
      "== W == 0.03536123322237793\n",
      "log loss =  102646.74280008639\n",
      "Epoch 4, loss: 11.408493\n",
      "== W == 0.0347445689837022\n",
      "log loss =  102646.6557209856\n",
      "Epoch 5, loss: 11.408422\n",
      "== W == 0.03413828077743826\n",
      "log loss =  102646.569573953\n",
      "Epoch 6, loss: 11.408353\n",
      "== W == 0.03354219305071412\n",
      "log loss =  102646.48434841062\n",
      "Epoch 7, loss: 11.408285\n",
      "== W == 0.032956133241770255\n",
      "log loss =  102646.40003391623\n",
      "Epoch 8, loss: 11.408218\n",
      "== W == 0.03237993172844227\n",
      "log loss =  102646.31662016123\n",
      "Epoch 9, loss: 11.408152\n",
      "== W == 0.031813421777546626\n",
      "log loss =  102646.23409696852\n",
      "Epoch 0, loss: 11.408088\n",
      "== W == 0.031256439495154345\n",
      "log loss =  102646.15245429036\n",
      "Epoch 1, loss: 11.408025\n",
      "== W == 0.03070882377773558\n",
      "log loss =  102646.07168220643\n",
      "Epoch 2, loss: 11.407963\n",
      "== W == 0.030170416264159883\n",
      "log loss =  102645.99177092177\n",
      "Epoch 3, loss: 11.407902\n",
      "== W == 0.029641061288536663\n",
      "log loss =  102645.91271076485\n",
      "Epoch 4, loss: 11.407842\n",
      "== W == 0.029120605833880146\n",
      "log loss =  102645.8344921856\n",
      "Epoch 5, loss: 11.407784\n",
      "== W == 0.028608899486585182\n",
      "log loss =  102645.75710575366\n",
      "Epoch 6, loss: 11.407727\n",
      "== W == 0.02810579439169776\n",
      "log loss =  102645.68054215638\n",
      "Epoch 7, loss: 11.407670\n",
      "== W == 0.02761114520896734\n",
      "log loss =  102645.60479219719\n",
      "Epoch 8, loss: 11.407615\n",
      "== W == 0.027124809069666057\n",
      "log loss =  102645.52984679374\n",
      "Epoch 9, loss: 11.407561\n",
      "== W == 0.026646645534161267\n",
      "log loss =  102645.45569697613\n",
      "Epoch 0, loss: 11.407508\n",
      "== W == 0.0261765165502282\n",
      "log loss =  102645.38233388538\n",
      "Epoch 1, loss: 11.407455\n",
      "== W == 0.02571428641208915\n",
      "log loss =  102645.30974877154\n",
      "Epoch 2, loss: 11.407404\n",
      "== W == 0.02525982172016629\n",
      "log loss =  102645.2379329923\n",
      "Epoch 3, loss: 11.407354\n",
      "== W == 0.02481299134153521\n",
      "log loss =  102645.16687801127\n",
      "Epoch 4, loss: 11.407305\n",
      "== W == 0.024373666371066968\n",
      "log loss =  102645.09657539632\n",
      "Epoch 5, loss: 11.407256\n",
      "== W == 0.023941720093246258\n",
      "log loss =  102645.02701681832\n",
      "Epoch 6, loss: 11.407209\n",
      "== W == 0.023517027944653143\n",
      "log loss =  102644.95819404932\n",
      "Epoch 7, loss: 11.407162\n",
      "== W == 0.02309946747709745\n",
      "log loss =  102644.89009896132\n",
      "Epoch 8, loss: 11.407116\n",
      "== W == 0.02268891832139327\n",
      "log loss =  102644.82272352469\n",
      "Epoch 9, loss: 11.407072\n",
      "== W == 0.022285262151762734\n",
      "log loss =  102644.75605980676\n",
      "Epoch 0, loss: 11.407027\n",
      "== W == 0.021888382650857838\n",
      "log loss =  102644.69009997051\n",
      "Epoch 1, loss: 11.406984\n",
      "== W == 0.021498165475388884\n",
      "log loss =  102644.62483627308\n",
      "Epoch 2, loss: 11.406942\n",
      "== W == 0.021114498222349478\n",
      "log loss =  102644.56026106456\n",
      "Epoch 3, loss: 11.406900\n",
      "== W == 0.020737270395826717\n",
      "log loss =  102644.49636678648\n",
      "Epoch 4, loss: 11.406859\n",
      "== W == 0.02036637337438689\n",
      "log loss =  102644.43314597075\n",
      "Epoch 5, loss: 11.406819\n",
      "== W == 0.020001700379026037\n",
      "log loss =  102644.37059123823\n",
      "Epoch 6, loss: 11.406780\n",
      "== W == 0.019643146441675387\n",
      "log loss =  102644.30869529754\n",
      "Epoch 7, loss: 11.406741\n",
      "== W == 0.019290608374252248\n",
      "log loss =  102644.24745094379\n",
      "Epoch 8, loss: 11.406703\n",
      "== W == 0.0189439847382461\n",
      "log loss =  102644.18685105741\n",
      "Epoch 9, loss: 11.406666\n",
      "== W == 0.018603175814831088\n",
      "log loss =  102644.12688860304\n",
      "Epoch 0, loss: 11.406630\n",
      "== W == 0.01826808357549492\n",
      "log loss =  102644.0675566282\n",
      "Epoch 1, loss: 11.406594\n",
      "== W == 0.017938611653175722\n",
      "log loss =  102644.00884826231\n",
      "Epoch 2, loss: 11.406559\n",
      "== W == 0.017614665313897448\n",
      "log loss =  102643.95075671548\n",
      "Epoch 3, loss: 11.406524\n",
      "== W == 0.017296151428895203\n",
      "log loss =  102643.89327527743\n",
      "Epoch 4, loss: 11.406490\n",
      "== W == 0.016982978447221773\n",
      "log loss =  102643.83639731642\n",
      "Epoch 5, loss: 11.406457\n",
      "== W == 0.01667505636882714\n",
      "log loss =  102643.78011627821\n",
      "Epoch 6, loss: 11.406425\n",
      "== W == 0.016372296718102287\n",
      "log loss =  102643.72442568492\n",
      "Epoch 7, loss: 11.406392\n",
      "== W == 0.016074612517879373\n",
      "log loss =  102643.66931913413\n",
      "Epoch 8, loss: 11.406361\n",
      "== W == 0.015781918263880215\n",
      "log loss =  102643.61479029777\n",
      "Epoch 9, loss: 11.406330\n",
      "== W == 0.015494129899605245\n",
      "log loss =  102643.56083292118\n",
      "Epoch 0, loss: 11.406300\n",
      "== W == 0.015211164791654972\n",
      "log loss =  102643.50744082213\n",
      "Epoch 1, loss: 11.406270\n",
      "== W == 0.014932941705476741\n",
      "log loss =  102643.45460788984\n",
      "Epoch 2, loss: 11.406241\n",
      "== W == 0.014659380781528987\n",
      "log loss =  102643.40232808405\n",
      "Epoch 3, loss: 11.406212\n",
      "== W == 0.014390403511855915\n",
      "log loss =  102643.35059543407\n",
      "Epoch 4, loss: 11.406184\n",
      "== W == 0.014125932717065188\n",
      "log loss =  102643.29940403791\n",
      "Epoch 5, loss: 11.406157\n",
      "== W == 0.013865892523701854\n",
      "log loss =  102643.24874806136\n",
      "Epoch 6, loss: 11.406130\n",
      "== W == 0.013610208342011256\n",
      "log loss =  102643.19862173709\n",
      "Epoch 7, loss: 11.406103\n",
      "== W == 0.013358806844084435\n",
      "log loss =  102643.14901936377\n",
      "Epoch 8, loss: 11.406077\n",
      "== W == 0.013111615942379043\n",
      "log loss =  102643.09993530533\n",
      "Epoch 9, loss: 11.406051\n",
      "== W == 0.012868564768609422\n",
      "log loss =  102643.05136398993\n",
      "Epoch 0, loss: 11.406026\n",
      "== W == 0.012629583652999446\n",
      "log loss =  102643.00329990928\n",
      "Epoch 1, loss: 11.406001\n",
      "== W == 0.012394604103891446\n",
      "log loss =  102642.95573761778\n",
      "Epoch 2, loss: 11.405977\n",
      "== W == 0.01216355878770546\n",
      "log loss =  102642.90867173171\n",
      "Epoch 3, loss: 11.405953\n",
      "== W == 0.011936381509242295\n",
      "log loss =  102642.86209692844\n",
      "Epoch 4, loss: 11.405930\n",
      "== W == 0.01171300719232485\n",
      "log loss =  102642.81600794563\n",
      "Epoch 5, loss: 11.405907\n",
      "== W == 0.011493371860771307\n",
      "log loss =  102642.77039958056\n",
      "Epoch 6, loss: 11.405884\n",
      "== W == 0.011277412619694714\n",
      "log loss =  102642.7252666893\n",
      "Epoch 7, loss: 11.405862\n",
      "== W == 0.011065067637123427\n",
      "log loss =  102642.68060418588\n",
      "Epoch 8, loss: 11.405841\n",
      "== W == 0.01085627612593634\n",
      "log loss =  102642.63640704179\n",
      "Epoch 9, loss: 11.405819\n",
      "== W == 0.010650978326107941\n",
      "log loss =  102642.59267028506\n",
      "Epoch 0, loss: 11.405798\n",
      "== W == 0.010449115487257436\n",
      "log loss =  102642.54938899966\n",
      "Epoch 1, loss: 11.405778\n",
      "== W == 0.010250629851496957\n",
      "log loss =  102642.50655832472\n",
      "Epoch 2, loss: 11.405758\n",
      "== W == 0.01005546463657336\n",
      "log loss =  102642.46417345392\n",
      "Epoch 3, loss: 11.405738\n",
      "== W == 0.009863564019298799\n",
      "log loss =  102642.42222963476\n",
      "Epoch 4, loss: 11.405718\n",
      "== W == 0.00967487311926487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102642.38072216797\n",
      "Epoch 5, loss: 11.405699\n",
      "== W == 0.009489337982835487\n",
      "log loss =  102642.33964640673\n",
      "Epoch 6, loss: 11.405680\n",
      "== W == 0.009306905567413665\n",
      "log loss =  102642.29899775615\n",
      "Epoch 7, loss: 11.405662\n",
      "== W == 0.009127523725977386\n",
      "log loss =  102642.25877167251\n",
      "Epoch 8, loss: 11.405644\n",
      "== W == 0.008951141191880056\n",
      "log loss =  102642.21896366277\n",
      "Epoch 9, loss: 11.405626\n",
      "== W == 0.00877770756391072\n",
      "log loss =  102642.17956928385\n",
      "Epoch 0, loss: 11.405609\n",
      "== W == 0.008607173291609812\n",
      "log loss =  102642.14058414203\n",
      "Epoch 1, loss: 11.405592\n",
      "== W == 0.008439489660835782\n",
      "log loss =  102642.10200389236\n",
      "Epoch 2, loss: 11.405575\n",
      "== W == 0.008274608779578413\n",
      "log loss =  102642.06382423812\n",
      "Epoch 3, loss: 11.405558\n",
      "== W == 0.008112483564014505\n",
      "log loss =  102642.0260409302\n",
      "Epoch 4, loss: 11.405542\n",
      "== W == 0.00795306772480179\n",
      "log loss =  102641.98864976646\n",
      "Epoch 5, loss: 11.405526\n",
      "== W == 0.007796315753606772\n",
      "log loss =  102641.95164659122\n",
      "Epoch 6, loss: 11.405510\n",
      "== W == 0.007642182909862802\n",
      "log loss =  102641.91502729477\n",
      "Epoch 7, loss: 11.405495\n",
      "== W == 0.007490625207754047\n",
      "log loss =  102641.87878781268\n",
      "Epoch 8, loss: 11.405480\n",
      "== W == 0.007341599403421687\n",
      "log loss =  102641.84292412536\n",
      "Epoch 9, loss: 11.405465\n",
      "== W == 0.007195062982388428\n",
      "log loss =  102641.80743225745\n",
      "Epoch 0, loss: 11.405451\n",
      "== W == 0.007050974147197489\n",
      "log loss =  102641.77230827743\n",
      "Epoch 1, loss: 11.405436\n",
      "== W == 0.00690929180526265\n",
      "log loss =  102641.7375482969\n",
      "Epoch 2, loss: 11.405422\n",
      "== W == 0.00676997555692529\n",
      "log loss =  102641.70314847016\n",
      "Epoch 3, loss: 11.405408\n",
      "== W == 0.0066329856837151705\n",
      "log loss =  102641.66910499377\n",
      "Epoch 4, loss: 11.405395\n",
      "== W == 0.006498283136811398\n",
      "log loss =  102641.63541410597\n",
      "Epoch 5, loss: 11.405382\n",
      "== W == 0.006365829525700041\n",
      "log loss =  102641.60207208623\n",
      "Epoch 6, loss: 11.405368\n",
      "== W == 0.0062355871070250484\n",
      "log loss =  102641.56907525467\n",
      "Epoch 7, loss: 11.405356\n",
      "== W == 0.0061075187736292155\n",
      "log loss =  102641.53641997172\n",
      "Epoch 8, loss: 11.405343\n",
      "== W == 0.005981588043781717\n",
      "log loss =  102641.50410263751\n",
      "Epoch 9, loss: 11.405331\n",
      "== W == 0.0058577590505892955\n",
      "log loss =  102641.47211969158\n",
      "Epoch 0, loss: 11.405318\n",
      "== W == 0.005735996531587807\n",
      "log loss =  102641.44046761218\n",
      "Epoch 1, loss: 11.405306\n",
      "== W == 0.005616265818510796\n",
      "log loss =  102641.40914291603\n",
      "Epoch 2, loss: 11.405295\n",
      "== W == 0.005498532827232497\n",
      "log loss =  102641.3781421578\n",
      "Epoch 3, loss: 11.405283\n",
      "== W == 0.005382764047881983\n",
      "log loss =  102641.34746192957\n",
      "Epoch 4, loss: 11.405272\n",
      "== W == 0.005268926535125529\n",
      "log loss =  102641.31709886056\n",
      "Epoch 5, loss: 11.405261\n",
      "== W == 0.0051569878986143436\n",
      "log loss =  102641.28704961657\n",
      "Epoch 6, loss: 11.405250\n",
      "== W == 0.005046916293594959\n",
      "log loss =  102641.25731089966\n",
      "Epoch 7, loss: 11.405239\n",
      "== W == 0.004938680411679225\n",
      "log loss =  102641.22787944762\n",
      "Epoch 8, loss: 11.405228\n",
      "== W == 0.0048322494717713774\n",
      "log loss =  102641.19875203361\n",
      "Epoch 9, loss: 11.405218\n",
      "== W == 0.0047275932111494\n",
      "log loss =  102641.1699254658\n",
      "Epoch 0, loss: 11.405208\n",
      "== W == 0.004624681876698105\n",
      "log loss =  102641.1413965869\n",
      "Epoch 1, loss: 11.405198\n",
      "== W == 0.004523486216291202\n",
      "log loss =  102641.11316227374\n",
      "Epoch 2, loss: 11.405188\n",
      "== W == 0.004423977470319886\n",
      "log loss =  102641.08521943701\n",
      "Epoch 3, loss: 11.405178\n",
      "== W == 0.004326127363365539\n",
      "log loss =  102641.05756502069\n",
      "Epoch 4, loss: 11.405169\n",
      "== W == 0.004229908096013762\n",
      "log loss =  102641.03019600178\n",
      "Epoch 5, loss: 11.405160\n",
      "== W == 0.004135292336807722\n",
      "log loss =  102641.00310938993\n",
      "Epoch 6, loss: 11.405150\n",
      "== W == 0.004042253214338077\n",
      "log loss =  102640.97630222703\n",
      "Epoch 7, loss: 11.405141\n",
      "== W == 0.003950764309467353\n",
      "log loss =  102640.94977158685\n",
      "Epoch 8, loss: 11.405133\n",
      "== W == 0.003860799647686432\n",
      "log loss =  102640.92351457468\n",
      "Epoch 9, loss: 11.405124\n",
      "== W == 0.0037723336916007143\n",
      "log loss =  102640.89752832692\n",
      "Epoch 0, loss: 11.405115\n",
      "== W == 0.0036853413335441124\n",
      "log loss =  102640.87181001087\n",
      "Epoch 1, loss: 11.405107\n",
      "== W == 0.0035997978883182645\n",
      "log loss =  102640.8463568242\n",
      "Epoch 2, loss: 11.405099\n",
      "== W == 0.0035156790860551438\n",
      "log loss =  102640.82116599471\n",
      "Epoch 3, loss: 11.405091\n",
      "== W == 0.0034329610652007536\n",
      "log loss =  102640.79623477999\n",
      "Epoch 4, loss: 11.405083\n",
      "== W == 0.003351620365617968\n",
      "log loss =  102640.77156046705\n",
      "Epoch 5, loss: 11.405075\n",
      "== W == 0.003271633921806432\n",
      "log loss =  102640.747140372\n",
      "Epoch 6, loss: 11.405067\n",
      "== W == 0.003192979056237477\n",
      "log loss =  102640.72297183974\n",
      "Epoch 7, loss: 11.405059\n",
      "== W == 0.0031156334728021807\n",
      "log loss =  102640.69905224355\n",
      "Epoch 8, loss: 11.405052\n",
      "== W == 0.0030395752503705586\n",
      "log loss =  102640.6753789849\n",
      "Epoch 9, loss: 11.405045\n",
      "== W == 0.0029647828364599907\n",
      "log loss =  102640.65194949305\n",
      "Epoch 0, loss: 11.405038\n",
      "== W == 0.0028912350410111197\n",
      "log loss =  102640.62876122477\n",
      "Epoch 1, loss: 11.405030\n",
      "== W == 0.002818911030269243\n",
      "log loss =  102640.60581166402\n",
      "Epoch 2, loss: 11.405024\n",
      "== W == 0.0027477903207695107\n",
      "log loss =  102640.58309832161\n",
      "Epoch 3, loss: 11.405017\n",
      "== W == 0.002677852773424063\n",
      "log loss =  102640.56061873496\n",
      "Epoch 4, loss: 11.405010\n",
      "== W == 0.002609078587709485\n",
      "log loss =  102640.53837046778\n",
      "Epoch 5, loss: 11.405003\n",
      "== W == 0.0025414482959527284\n",
      "log loss =  102640.51635110976\n",
      "Epoch 6, loss: 11.404997\n",
      "== W == 0.0024749427577139445\n",
      "log loss =  102640.49455827632\n",
      "Epoch 7, loss: 11.404991\n",
      "== W == 0.0024095431542645136\n",
      "log loss =  102640.47298960823\n",
      "Epoch 8, loss: 11.404984\n",
      "== W == 0.0023452309831586285\n",
      "log loss =  102640.45164277148\n",
      "Epoch 9, loss: 11.404978\n",
      "== W == 0.0022819880528968956\n",
      "log loss =  102640.43051545684\n",
      "Epoch 0, loss: 11.404972\n",
      "== W == 0.0022197964776803514\n",
      "log loss =  102640.40960537973\n",
      "Epoch 1, loss: 11.404966\n",
      "== W == 0.0021586386722533945\n",
      "log loss =  102640.3889102798\n",
      "Epoch 2, loss: 11.404960\n",
      "== W == 0.002098497346833929\n",
      "log loss =  102640.36842792078\n",
      "Epoch 3, loss: 11.404955\n",
      "== W == 0.0020393555021296222\n",
      "log loss =  102640.34815609014\n",
      "Epoch 4, loss: 11.404949\n",
      "== W == 0.0019811964244383605\n",
      "log loss =  102640.32809259891\n",
      "Epoch 5, loss: 11.404943\n",
      "== W == 0.0019240036808318107\n",
      "log loss =  102640.30823528128\n",
      "Epoch 6, loss: 11.404938\n",
      "== W == 0.0018677611144204877\n",
      "log loss =  102640.28858199451\n",
      "Epoch 7, loss: 11.404932\n",
      "== W == 0.0018124528396990178\n",
      "log loss =  102640.26913061856\n",
      "Epoch 8, loss: 11.404927\n",
      "== W == 0.0017580632379701868\n",
      "log loss =  102640.24987905585\n",
      "Epoch 9, loss: 11.404922\n",
      "== W == 0.0017045769528464486\n",
      "log loss =  102640.23082523106\n",
      "Epoch 0, loss: 11.404917\n",
      "== W == 0.0016519788858276874\n",
      "log loss =  102640.21196709093\n",
      "Epoch 1, loss: 11.404912\n",
      "== W == 0.0016002541919536428\n",
      "log loss =  102640.19330260379\n",
      "Epoch 2, loss: 11.404907\n",
      "== W == 0.0015493882755300767\n",
      "log loss =  102640.17482975959\n",
      "Epoch 3, loss: 11.404902\n",
      "== W == 0.0014993667859272228\n",
      "log loss =  102640.15654656959\n",
      "Epoch 4, loss: 11.404897\n",
      "== W == 0.0014501756134492694\n",
      "log loss =  102640.13845106597\n",
      "Epoch 5, loss: 11.404893\n",
      "== W == 0.001401800885273823\n",
      "log loss =  102640.12054130182\n",
      "Epoch 6, loss: 11.404888\n",
      "== W == 0.00135422896146\n",
      "log loss =  102640.10281535077\n",
      "Epoch 7, loss: 11.404883\n",
      "== W == 0.0013074464310241169\n",
      "log loss =  102640.0852713068\n",
      "Epoch 8, loss: 11.404879\n",
      "== W == 0.0012614401080816877\n",
      "log loss =  102640.06790728406\n",
      "Epoch 9, loss: 11.404875\n",
      "== W == 0.0012161970280547413\n",
      "log loss =  102640.05072141656\n",
      "Epoch 0, loss: 11.404870\n",
      "== W == 0.0011717044439432138\n",
      "log loss =  102640.033711858\n",
      "Epoch 1, loss: 11.404866\n",
      "== W == 0.00112794982265952\n",
      "log loss =  102640.0168767817\n",
      "Epoch 2, loss: 11.404862\n",
      "== W == 0.0010849208414249493\n",
      "log loss =  102640.00021438005\n",
      "Epoch 3, loss: 11.404858\n",
      "== W == 0.0010426053842271113\n",
      "log loss =  102639.98372286462\n",
      "Epoch 4, loss: 11.404854\n",
      "== W == 0.0010009915383372486\n",
      "log loss =  102639.9674004658\n",
      "Epoch 5, loss: 11.404850\n",
      "== W == 0.0009600675908863817\n",
      "log loss =  102639.95124543262\n",
      "Epoch 6, loss: 11.404846\n",
      "== W == 0.0009198220254994106\n",
      "log loss =  102639.93525603258\n",
      "Epoch 7, loss: 11.404842\n",
      "== W == 0.0008802435189860557\n",
      "log loss =  102639.91943055135\n",
      "Epoch 8, loss: 11.404838\n",
      "== W == 0.0008413209380877147\n",
      "log loss =  102639.90376729277\n",
      "Epoch 9, loss: 11.404834\n",
      "== W == 0.0008030433362793904\n",
      "log loss =  102639.88826457837\n",
      "Epoch 0, loss: 11.404831\n",
      "== W == 0.0007653999506255582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102639.87292074745\n",
      "Epoch 1, loss: 11.404827\n",
      "== W == 0.000728380198689216\n",
      "log loss =  102639.85773415677\n",
      "Epoch 2, loss: 11.404824\n",
      "== W == 0.000691973675493179\n",
      "log loss =  102639.84270318029\n",
      "Epoch 3, loss: 11.404820\n",
      "== W == 0.0006561701505326681\n",
      "log loss =  102639.8278262091\n",
      "Epoch 4, loss: 11.404817\n",
      "== W == 0.0006209595648383947\n",
      "log loss =  102639.8131016512\n",
      "Epoch 5, loss: 11.404813\n",
      "== W == 0.0005863320280892733\n",
      "log loss =  102639.79852793131\n",
      "Epoch 6, loss: 11.404810\n",
      "== W == 0.0005522778157739001\n",
      "log loss =  102639.78410349067\n",
      "Epoch 7, loss: 11.404807\n",
      "== W == 0.0005187873663999279\n",
      "log loss =  102639.76982678688\n",
      "Epoch 8, loss: 11.404804\n",
      "== W == 0.0004858512787506526\n",
      "log loss =  102639.75569629375\n",
      "Epoch 9, loss: 11.404800\n",
      "== W == 0.0004534603091878608\n",
      "log loss =  102639.74171050102\n",
      "Epoch 0, loss: 11.404797\n",
      "== W == 0.0004216053690003123\n",
      "log loss =  102639.7278679144\n",
      "Epoch 1, loss: 11.404794\n",
      "== W == 0.00039027752179687444\n",
      "log loss =  102639.7141670551\n",
      "Epoch 2, loss: 11.404791\n",
      "== W == 0.00035946798094376895\n",
      "log loss =  102639.70060646001\n",
      "Epoch 3, loss: 11.404788\n",
      "== W == 0.00032916810704507627\n",
      "log loss =  102639.68718468118\n",
      "Epoch 4, loss: 11.404785\n",
      "== W == 0.0002993694054657333\n",
      "log loss =  102639.67390028594\n",
      "Epoch 5, loss: 11.404783\n",
      "== W == 0.0002700635238963825\n",
      "log loss =  102639.66075185657\n",
      "Epoch 6, loss: 11.404780\n",
      "== W == 0.00024124224995932106\n",
      "log loss =  102639.64773799019\n",
      "Epoch 7, loss: 11.404777\n",
      "== W == 0.00021289750885481904\n",
      "log loss =  102639.63485729863\n",
      "Epoch 8, loss: 11.404774\n",
      "== W == 0.00018502136104726332\n",
      "log loss =  102639.62210840818\n",
      "Epoch 9, loss: 11.404771\n",
      "== W == 0.00015760599999024762\n",
      "log loss =  102670.77082599516\n",
      "Epoch 0, loss: 11.438381\n",
      "== W == 0.39373044992339623\n",
      "log loss =  102670.73314550697\n",
      "Epoch 1, loss: 11.438315\n",
      "== W == 0.3930636314076666\n",
      "log loss =  102670.69551635315\n",
      "Epoch 2, loss: 11.438250\n",
      "== W == 0.39239794819111884\n",
      "log loss =  102670.6579384426\n",
      "Epoch 3, loss: 11.438185\n",
      "== W == 0.3917333983158726\n",
      "log loss =  102670.62041168442\n",
      "Epoch 4, loss: 11.438120\n",
      "== W == 0.39106997982749997\n",
      "log loss =  102670.58293598791\n",
      "Epoch 5, loss: 11.438056\n",
      "== W == 0.3904076907750198\n",
      "log loss =  102670.54551126256\n",
      "Epoch 6, loss: 11.437991\n",
      "== W == 0.389746529210892\n",
      "log loss =  102670.50813741816\n",
      "Epoch 7, loss: 11.437926\n",
      "== W == 0.3890864931910094\n",
      "log loss =  102670.47081436461\n",
      "Epoch 8, loss: 11.437862\n",
      "== W == 0.38842758077469325\n",
      "log loss =  102670.43354201209\n",
      "Epoch 9, loss: 11.437798\n",
      "== W == 0.38776979002468603\n",
      "log loss =  102670.396320271\n",
      "Epoch 0, loss: 11.437734\n",
      "== W == 0.38711311900714523\n",
      "log loss =  102670.35914905192\n",
      "Epoch 1, loss: 11.437670\n",
      "== W == 0.3864575657916376\n",
      "log loss =  102670.32202826564\n",
      "Epoch 2, loss: 11.437606\n",
      "== W == 0.38580312845113185\n",
      "log loss =  102670.28495782317\n",
      "Epoch 3, loss: 11.437542\n",
      "== W == 0.3851498050619934\n",
      "log loss =  102670.24793763575\n",
      "Epoch 4, loss: 11.437478\n",
      "== W == 0.3844975937039775\n",
      "log loss =  102670.21096761484\n",
      "Epoch 5, loss: 11.437415\n",
      "== W == 0.38384649246022357\n",
      "log loss =  102670.17404767204\n",
      "Epoch 6, loss: 11.437352\n",
      "== W == 0.38319649941724854\n",
      "log loss =  102670.13717771921\n",
      "Epoch 7, loss: 11.437288\n",
      "== W == 0.3825476126649406\n",
      "log loss =  102670.1003576684\n",
      "Epoch 8, loss: 11.437225\n",
      "== W == 0.3818998302965533\n",
      "log loss =  102670.06358743196\n",
      "Epoch 9, loss: 11.437162\n",
      "== W == 0.38125315040869934\n",
      "log loss =  102670.02686692227\n",
      "Epoch 0, loss: 11.437099\n",
      "== W == 0.3806075711013443\n",
      "log loss =  102669.99019605201\n",
      "Epoch 1, loss: 11.437037\n",
      "== W == 0.3799630904778004\n",
      "log loss =  102669.95357473416\n",
      "Epoch 2, loss: 11.436974\n",
      "== W == 0.3793197066447204\n",
      "log loss =  102669.91700288176\n",
      "Epoch 3, loss: 11.436912\n",
      "== W == 0.3786774177120919\n",
      "log loss =  102669.88048040809\n",
      "Epoch 4, loss: 11.436849\n",
      "== W == 0.37803622179323043\n",
      "log loss =  102669.84400722667\n",
      "Epoch 5, loss: 11.436787\n",
      "== W == 0.37739611700477405\n",
      "log loss =  102669.80758325121\n",
      "Epoch 6, loss: 11.436725\n",
      "== W == 0.3767571014666768\n",
      "log loss =  102669.77120839561\n",
      "Epoch 7, loss: 11.436663\n",
      "== W == 0.3761191733022029\n",
      "log loss =  102669.73488257399\n",
      "Epoch 8, loss: 11.436601\n",
      "== W == 0.37548233063792075\n",
      "log loss =  102669.69860570066\n",
      "Epoch 9, loss: 11.436539\n",
      "== W == 0.37484657160369633\n",
      "log loss =  102669.66237769014\n",
      "Epoch 0, loss: 11.436478\n",
      "== W == 0.37421189433268787\n",
      "log loss =  102669.62619845713\n",
      "Epoch 1, loss: 11.436416\n",
      "== W == 0.37357829696133904\n",
      "log loss =  102669.59006791654\n",
      "Epoch 2, loss: 11.436355\n",
      "== W == 0.3729457776293739\n",
      "log loss =  102669.55398598351\n",
      "Epoch 3, loss: 11.436293\n",
      "== W == 0.37231433447979\n",
      "log loss =  102669.51795257333\n",
      "Epoch 4, loss: 11.436232\n",
      "== W == 0.37168396565885287\n",
      "log loss =  102669.48196760147\n",
      "Epoch 5, loss: 11.436171\n",
      "== W == 0.3710546693160898\n",
      "log loss =  102669.44603098375\n",
      "Epoch 6, loss: 11.436110\n",
      "== W == 0.37042644360428423\n",
      "log loss =  102669.41014263597\n",
      "Epoch 7, loss: 11.436050\n",
      "== W == 0.36979928667946926\n",
      "log loss =  102669.37430247429\n",
      "Epoch 8, loss: 11.435989\n",
      "== W == 0.36917319670092197\n",
      "log loss =  102669.33851041496\n",
      "Epoch 9, loss: 11.435928\n",
      "== W == 0.36854817183115746\n",
      "log loss =  102669.30276637452\n",
      "Epoch 0, loss: 11.435868\n",
      "== W == 0.36792421023592314\n",
      "log loss =  102669.2670702696\n",
      "Epoch 1, loss: 11.435808\n",
      "== W == 0.3673013100841924\n",
      "log loss =  102669.23142201715\n",
      "Epoch 2, loss: 11.435747\n",
      "== W == 0.36667946954815916\n",
      "log loss =  102669.19582153422\n",
      "Epoch 3, loss: 11.435687\n",
      "== W == 0.36605868680323156\n",
      "log loss =  102669.16026873805\n",
      "Epoch 4, loss: 11.435627\n",
      "== W == 0.36543896002802634\n",
      "log loss =  102669.12476354613\n",
      "Epoch 5, loss: 11.435568\n",
      "== W == 0.3648202874043627\n",
      "log loss =  102669.0893058761\n",
      "Epoch 6, loss: 11.435508\n",
      "== W == 0.36420266711725685\n",
      "log loss =  102669.0538956458\n",
      "Epoch 7, loss: 11.435448\n",
      "== W == 0.363586097354916\n",
      "log loss =  102669.01853277326\n",
      "Epoch 8, loss: 11.435389\n",
      "== W == 0.3629705763087321\n",
      "log loss =  102668.98321717675\n",
      "Epoch 9, loss: 11.435329\n",
      "== W == 0.36235610217327685\n",
      "log loss =  102668.94794877464\n",
      "Epoch 0, loss: 11.435270\n",
      "== W == 0.36174267314629516\n",
      "log loss =  102668.91272748553\n",
      "Epoch 1, loss: 11.435211\n",
      "== W == 0.3611302874286996\n",
      "log loss =  102668.87755322823\n",
      "Epoch 2, loss: 11.435152\n",
      "== W == 0.36051894322456496\n",
      "log loss =  102668.84242592173\n",
      "Epoch 3, loss: 11.435093\n",
      "== W == 0.35990863874112167\n",
      "log loss =  102668.8073454852\n",
      "Epoch 4, loss: 11.435034\n",
      "== W == 0.35929937218875074\n",
      "log loss =  102668.77231183794\n",
      "Epoch 5, loss: 11.434976\n",
      "== W == 0.3586911417809779\n",
      "log loss =  102668.73732489957\n",
      "Epoch 6, loss: 11.434917\n",
      "== W == 0.35808394573446756\n",
      "log loss =  102668.70238458976\n",
      "Epoch 7, loss: 11.434859\n",
      "== W == 0.3574777822690173\n",
      "log loss =  102668.66749082845\n",
      "Epoch 8, loss: 11.434800\n",
      "== W == 0.35687264960755194\n",
      "log loss =  102668.63264353576\n",
      "Epoch 9, loss: 11.434742\n",
      "== W == 0.35626854597611834\n",
      "log loss =  102668.59784263192\n",
      "Epoch 0, loss: 11.434684\n",
      "== W == 0.35566546960387907\n",
      "log loss =  102668.5630880374\n",
      "Epoch 1, loss: 11.434626\n",
      "== W == 0.35506341872310704\n",
      "log loss =  102668.52837967289\n",
      "Epoch 2, loss: 11.434568\n",
      "== W == 0.35446239156918\n",
      "log loss =  102668.49371745922\n",
      "Epoch 3, loss: 11.434510\n",
      "== W == 0.3538623863805743\n",
      "log loss =  102668.45910131733\n",
      "Epoch 4, loss: 11.434453\n",
      "== W == 0.3532634013988599\n",
      "log loss =  102668.42453116852\n",
      "Epoch 5, loss: 11.434395\n",
      "== W == 0.35266543486869434\n",
      "log loss =  102668.39000693412\n",
      "Epoch 6, loss: 11.434338\n",
      "== W == 0.35206848503781707\n",
      "log loss =  102668.35552853567\n",
      "Epoch 7, loss: 11.434281\n",
      "== W == 0.35147255015704415\n",
      "log loss =  102668.32109589495\n",
      "Epoch 8, loss: 11.434223\n",
      "== W == 0.3508776284802623\n",
      "log loss =  102668.28670893383\n",
      "Epoch 9, loss: 11.434166\n",
      "== W == 0.3502837182644236\n",
      "log loss =  102668.25236757442\n",
      "Epoch 0, loss: 11.434109\n",
      "== W == 0.3496908177695396\n",
      "log loss =  102668.21807173901\n",
      "Epoch 1, loss: 11.434053\n",
      "== W == 0.34909892525867603\n",
      "log loss =  102668.18382135005\n",
      "Epoch 2, loss: 11.433996\n",
      "== W == 0.348508038997947\n",
      "log loss =  102668.14961633019\n",
      "Epoch 3, loss: 11.433939\n",
      "== W == 0.3479181572565094\n",
      "log loss =  102668.11545660219\n",
      "Epoch 4, loss: 11.433883\n",
      "== W == 0.3473292783065578\n",
      "log loss =  102668.08134208906\n",
      "Epoch 5, loss: 11.433826\n",
      "== W == 0.34674140042331847\n",
      "log loss =  102668.04727271399\n",
      "Epoch 6, loss: 11.433770\n",
      "== W == 0.3461545218850439\n",
      "log loss =  102668.01324840025\n",
      "Epoch 7, loss: 11.433714\n",
      "== W == 0.34556864097300766\n",
      "log loss =  102667.97926907141\n",
      "Epoch 8, loss: 11.433658\n",
      "== W == 0.34498375597149844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102667.94533465113\n",
      "Epoch 9, loss: 11.433602\n",
      "== W == 0.3443998651678146\n",
      "log loss =  102667.91144506328\n",
      "Epoch 0, loss: 11.433546\n",
      "== W == 0.3438169668522592\n",
      "log loss =  102667.87760023188\n",
      "Epoch 1, loss: 11.433490\n",
      "== W == 0.3432350593181338\n",
      "log loss =  102667.84380008116\n",
      "Epoch 2, loss: 11.433435\n",
      "== W == 0.34265414086173374\n",
      "log loss =  102667.81004453546\n",
      "Epoch 3, loss: 11.433379\n",
      "== W == 0.34207420978234226\n",
      "log loss =  102667.77633351937\n",
      "Epoch 4, loss: 11.433324\n",
      "== W == 0.34149526438222466\n",
      "log loss =  102667.74266695761\n",
      "Epoch 5, loss: 11.433268\n",
      "== W == 0.34091730296662404\n",
      "log loss =  102667.70904477504\n",
      "Epoch 6, loss: 11.433213\n",
      "== W == 0.34034032384375457\n",
      "log loss =  102667.67546689675\n",
      "Epoch 7, loss: 11.433158\n",
      "== W == 0.3397643253247972\n",
      "log loss =  102667.64193324797\n",
      "Epoch 8, loss: 11.433103\n",
      "== W == 0.3391893057238935\n",
      "log loss =  102667.60844375408\n",
      "Epoch 9, loss: 11.433048\n",
      "== W == 0.33861526335814035\n",
      "log loss =  102667.5749983407\n",
      "Epoch 0, loss: 11.432993\n",
      "== W == 0.3380421965475854\n",
      "log loss =  102667.54159693356\n",
      "Epoch 1, loss: 11.432939\n",
      "== W == 0.33747010361522056\n",
      "log loss =  102667.50823945853\n",
      "Epoch 2, loss: 11.432884\n",
      "== W == 0.33689898288697695\n",
      "log loss =  102667.47492584173\n",
      "Epoch 3, loss: 11.432830\n",
      "== W == 0.3363288326917204\n",
      "log loss =  102667.44165600938\n",
      "Epoch 4, loss: 11.432775\n",
      "== W == 0.3357596513612451\n",
      "log loss =  102667.4084298879\n",
      "Epoch 5, loss: 11.432721\n",
      "== W == 0.3351914372302686\n",
      "log loss =  102667.37524740389\n",
      "Epoch 6, loss: 11.432667\n",
      "== W == 0.33462418863642673\n",
      "log loss =  102667.34210848408\n",
      "Epoch 7, loss: 11.432613\n",
      "== W == 0.33405790392026813\n",
      "log loss =  102667.30901305535\n",
      "Epoch 8, loss: 11.432559\n",
      "== W == 0.33349258142524885\n",
      "log loss =  102667.2759610448\n",
      "Epoch 9, loss: 11.432505\n",
      "== W == 0.3329282194977271\n",
      "log loss =  102667.24295237966\n",
      "Epoch 0, loss: 11.432452\n",
      "== W == 0.3323648164869583\n",
      "log loss =  102667.20998698738\n",
      "Epoch 1, loss: 11.432398\n",
      "== W == 0.3318023707450895\n",
      "log loss =  102667.17706479545\n",
      "Epoch 2, loss: 11.432345\n",
      "== W == 0.33124088062715396\n",
      "log loss =  102667.14418573162\n",
      "Epoch 3, loss: 11.432291\n",
      "== W == 0.3306803444910664\n",
      "log loss =  102667.11134972381\n",
      "Epoch 4, loss: 11.432238\n",
      "== W == 0.33012076069761764\n",
      "log loss =  102667.07855670006\n",
      "Epoch 5, loss: 11.432185\n",
      "== W == 0.32956212761046905\n",
      "log loss =  102667.0458065886\n",
      "Epoch 6, loss: 11.432132\n",
      "== W == 0.32900444359614744\n",
      "log loss =  102667.01309931774\n",
      "Epoch 7, loss: 11.432079\n",
      "== W == 0.32844770702404025\n",
      "log loss =  102666.9804348161\n",
      "Epoch 8, loss: 11.432026\n",
      "== W == 0.32789191626639\n",
      "log loss =  102666.94781301235\n",
      "Epoch 9, loss: 11.431973\n",
      "== W == 0.32733706969828924\n",
      "log loss =  102666.91523383532\n",
      "Epoch 0, loss: 11.431921\n",
      "== W == 0.32678316569767524\n",
      "log loss =  102666.88269721407\n",
      "Epoch 1, loss: 11.431868\n",
      "== W == 0.326230202645325\n",
      "log loss =  102666.85020307773\n",
      "Epoch 2, loss: 11.431816\n",
      "== W == 0.3256781789248499\n",
      "log loss =  102666.81775135567\n",
      "Epoch 3, loss: 11.431763\n",
      "== W == 0.32512709292269093\n",
      "log loss =  102666.78534197736\n",
      "Epoch 4, loss: 11.431711\n",
      "== W == 0.3245769430281133\n",
      "log loss =  102666.75297487246\n",
      "Epoch 5, loss: 11.431659\n",
      "== W == 0.3240277276332011\n",
      "log loss =  102666.72064997078\n",
      "Epoch 6, loss: 11.431607\n",
      "== W == 0.3234794451328527\n",
      "log loss =  102666.68836720227\n",
      "Epoch 7, loss: 11.431555\n",
      "== W == 0.3229320939247753\n",
      "log loss =  102666.65612649707\n",
      "Epoch 8, loss: 11.431503\n",
      "== W == 0.32238567240947996\n",
      "log loss =  102666.62392778542\n",
      "Epoch 9, loss: 11.431451\n",
      "== W == 0.3218401789902763\n",
      "log loss =  102666.59177099777\n",
      "Epoch 0, loss: 11.431400\n",
      "== W == 0.32129561207326796\n",
      "log loss =  102666.55965606474\n",
      "Epoch 1, loss: 11.431348\n",
      "== W == 0.32075197006734685\n",
      "log loss =  102666.52758291701\n",
      "Epoch 2, loss: 11.431297\n",
      "== W == 0.3202092513841889\n",
      "log loss =  102666.49555148552\n",
      "Epoch 3, loss: 11.431246\n",
      "== W == 0.31966745443824784\n",
      "log loss =  102666.4635617013\n",
      "Epoch 4, loss: 11.431194\n",
      "== W == 0.3191265776467518\n",
      "log loss =  102666.43161349554\n",
      "Epoch 5, loss: 11.431143\n",
      "== W == 0.3185866194296966\n",
      "log loss =  102666.39970679961\n",
      "Epoch 6, loss: 11.431092\n",
      "== W == 0.31804757820984225\n",
      "log loss =  102666.36784154503\n",
      "Epoch 7, loss: 11.431041\n",
      "== W == 0.31750945241270656\n",
      "log loss =  102666.33601766342\n",
      "Epoch 8, loss: 11.430990\n",
      "== W == 0.31697224046656114\n",
      "log loss =  102666.3042350866\n",
      "Epoch 9, loss: 11.430940\n",
      "== W == 0.3164359408024262\n",
      "log loss =  102666.27249374655\n",
      "Epoch 0, loss: 11.430889\n",
      "== W == 0.315900551854065\n",
      "log loss =  102666.24079357537\n",
      "Epoch 1, loss: 11.430839\n",
      "== W == 0.3153660720579801\n",
      "log loss =  102666.20913450532\n",
      "Epoch 2, loss: 11.430788\n",
      "== W == 0.314832499853407\n",
      "log loss =  102666.17751646883\n",
      "Epoch 3, loss: 11.430738\n",
      "== W == 0.3142998336823101\n",
      "log loss =  102666.14593939844\n",
      "Epoch 4, loss: 11.430688\n",
      "== W == 0.3137680719893776\n",
      "log loss =  102666.11440322683\n",
      "Epoch 5, loss: 11.430638\n",
      "== W == 0.31323721322201653\n",
      "log loss =  102666.08290788694\n",
      "Epoch 6, loss: 11.430588\n",
      "== W == 0.3127072558303475\n",
      "log loss =  102666.05145331171\n",
      "Epoch 7, loss: 11.430538\n",
      "== W == 0.31217819826720045\n",
      "log loss =  102666.0200394343\n",
      "Epoch 8, loss: 11.430488\n",
      "== W == 0.31165003898810906\n",
      "log loss =  102665.98866618803\n",
      "Epoch 9, loss: 11.430438\n",
      "== W == 0.3111227764513067\n",
      "log loss =  102665.95733350635\n",
      "Epoch 0, loss: 11.430388\n",
      "== W == 0.31059640911772035\n",
      "log loss =  102665.92604132283\n",
      "Epoch 1, loss: 11.430339\n",
      "== W == 0.3100709354509673\n",
      "log loss =  102665.89478957123\n",
      "Epoch 2, loss: 11.430289\n",
      "== W == 0.30954635391734875\n",
      "log loss =  102665.86357818544\n",
      "Epoch 3, loss: 11.430240\n",
      "== W == 0.3090226629858458\n",
      "log loss =  102665.83240709949\n",
      "Epoch 4, loss: 11.430191\n",
      "== W == 0.30849986112811456\n",
      "log loss =  102665.80127624754\n",
      "Epoch 5, loss: 11.430142\n",
      "== W == 0.3079779468184813\n",
      "log loss =  102665.77018556393\n",
      "Epoch 6, loss: 11.430093\n",
      "== W == 0.3074569185339372\n",
      "log loss =  102665.73913498312\n",
      "Epoch 7, loss: 11.430044\n",
      "== W == 0.30693677475413417\n",
      "log loss =  102665.70812443971\n",
      "Epoch 8, loss: 11.429995\n",
      "== W == 0.3064175139613797\n",
      "log loss =  102665.67715386846\n",
      "Epoch 9, loss: 11.429946\n",
      "== W == 0.3058991346406319\n",
      "log loss =  102665.64622320425\n",
      "Epoch 0, loss: 11.429897\n",
      "== W == 0.30538163527949524\n",
      "log loss =  102665.61533238215\n",
      "Epoch 1, loss: 11.429849\n",
      "== W == 0.30486501436821534\n",
      "log loss =  102665.5844813373\n",
      "Epoch 2, loss: 11.429800\n",
      "== W == 0.30434927039967424\n",
      "log loss =  102665.55367000507\n",
      "Epoch 3, loss: 11.429752\n",
      "== W == 0.3038344018693858\n",
      "log loss =  102665.52289832087\n",
      "Epoch 4, loss: 11.429704\n",
      "== W == 0.303320407275491\n",
      "log loss =  102665.49216622033\n",
      "Epoch 5, loss: 11.429656\n",
      "== W == 0.3028072851187529\n",
      "log loss =  102665.46147363918\n",
      "Epoch 6, loss: 11.429607\n",
      "== W == 0.3022950339025521\n",
      "log loss =  102665.43082051333\n",
      "Epoch 7, loss: 11.429559\n",
      "== W == 0.30178365213288216\n",
      "log loss =  102665.40020677878\n",
      "Epoch 8, loss: 11.429512\n",
      "== W == 0.30127313831834474\n",
      "log loss =  102665.36963237169\n",
      "Epoch 9, loss: 11.429464\n",
      "== W == 0.30076349097014476\n",
      "log loss =  102665.33909722837\n",
      "Epoch 0, loss: 11.429416\n",
      "== W == 0.30025470860208586\n",
      "log loss =  102665.30860128527\n",
      "Epoch 1, loss: 11.429368\n",
      "== W == 0.2997467897305658\n",
      "log loss =  102665.27814447897\n",
      "Epoch 2, loss: 11.429321\n",
      "== W == 0.2992397328745716\n",
      "log loss =  102665.24772674617\n",
      "Epoch 3, loss: 11.429273\n",
      "== W == 0.29873353655567514\n",
      "log loss =  102665.21734802373\n",
      "Epoch 4, loss: 11.429226\n",
      "== W == 0.298228199298028\n",
      "log loss =  102665.18700824863\n",
      "Epoch 5, loss: 11.429179\n",
      "== W == 0.2977237196283574\n",
      "log loss =  102665.15670735802\n",
      "Epoch 6, loss: 11.429132\n",
      "== W == 0.2972200960759611\n",
      "log loss =  102665.12644528915\n",
      "Epoch 7, loss: 11.429085\n",
      "== W == 0.29671732717270316\n",
      "log loss =  102665.09622197942\n",
      "Epoch 8, loss: 11.429038\n",
      "== W == 0.2962154114530089\n",
      "log loss =  102665.06603736637\n",
      "Epoch 9, loss: 11.428991\n",
      "== W == 0.2957143474538608\n",
      "log loss =  102665.03589138766\n",
      "Epoch 0, loss: 11.428944\n",
      "== W == 0.29521413371479316\n",
      "log loss =  102665.0057839811\n",
      "Epoch 1, loss: 11.428897\n",
      "== W == 0.29471476877788827\n",
      "log loss =  102664.97571508463\n",
      "Epoch 2, loss: 11.428851\n",
      "== W == 0.2942162511877715\n",
      "log loss =  102664.94568463633\n",
      "Epoch 3, loss: 11.428804\n",
      "== W == 0.2937185794916065\n",
      "log loss =  102664.91569257439\n",
      "Epoch 4, loss: 11.428758\n",
      "== W == 0.2932217522390911\n",
      "log loss =  102664.88573883718\n",
      "Epoch 5, loss: 11.428711\n",
      "== W == 0.2927257679824523\n",
      "log loss =  102664.85582336315\n",
      "Epoch 6, loss: 11.428665\n",
      "== W == 0.29223062527644184\n",
      "log loss =  102664.82594609092\n",
      "Epoch 7, loss: 11.428619\n",
      "== W == 0.29173632267833227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102664.7961069592\n",
      "Epoch 8, loss: 11.428573\n",
      "== W == 0.29124285874791106\n",
      "log loss =  102664.76630590689\n",
      "Epoch 9, loss: 11.428527\n",
      "== W == 0.2907502320474774\n",
      "log loss =  102664.73654287298\n",
      "Epoch 0, loss: 11.428481\n",
      "== W == 0.29025844114183724\n",
      "log loss =  102664.7068177966\n",
      "Epoch 1, loss: 11.428435\n",
      "== W == 0.2897674845982984\n",
      "log loss =  102664.67713061703\n",
      "Epoch 2, loss: 11.428390\n",
      "== W == 0.28927736098666645\n",
      "log loss =  102664.64748127363\n",
      "Epoch 3, loss: 11.428344\n",
      "== W == 0.2887880688792404\n",
      "log loss =  102664.61786970597\n",
      "Epoch 4, loss: 11.428298\n",
      "== W == 0.2882996068508077\n",
      "log loss =  102664.58829585367\n",
      "Epoch 5, loss: 11.428253\n",
      "== W == 0.28781197347864007\n",
      "log loss =  102664.5587596565\n",
      "Epoch 6, loss: 11.428208\n",
      "== W == 0.2873251673424891\n",
      "log loss =  102664.52926105443\n",
      "Epoch 7, loss: 11.428162\n",
      "== W == 0.28683918702458155\n",
      "log loss =  102664.49979998745\n",
      "Epoch 8, loss: 11.428117\n",
      "== W == 0.2863540311096151\n",
      "log loss =  102664.47037639571\n",
      "Epoch 9, loss: 11.428072\n",
      "== W == 0.28586969818475394\n",
      "log loss =  102664.44099021958\n",
      "Epoch 0, loss: 11.428027\n",
      "== W == 0.28538618683962413\n",
      "log loss =  102664.41164139943\n",
      "Epoch 1, loss: 11.427982\n",
      "== W == 0.28490349566630924\n",
      "log loss =  102664.38232987582\n",
      "Epoch 2, loss: 11.427937\n",
      "== W == 0.2844216232593462\n",
      "log loss =  102664.35305558944\n",
      "Epoch 3, loss: 11.427893\n",
      "== W == 0.2839405682157205\n",
      "log loss =  102664.3238184811\n",
      "Epoch 4, loss: 11.427848\n",
      "== W == 0.28346032913486185\n",
      "log loss =  102664.2946184917\n",
      "Epoch 5, loss: 11.427804\n",
      "== W == 0.2829809046186403\n",
      "log loss =  102664.26545556233\n",
      "Epoch 6, loss: 11.427759\n",
      "== W == 0.282502293271361\n",
      "log loss =  102664.23632963415\n",
      "Epoch 7, loss: 11.427715\n",
      "== W == 0.2820244936997607\n",
      "log loss =  102664.20724064848\n",
      "Epoch 8, loss: 11.427670\n",
      "== W == 0.28154750451300264\n",
      "log loss =  102664.17818854675\n",
      "Epoch 9, loss: 11.427626\n",
      "== W == 0.28107132432267284\n",
      "log loss =  102664.14917327053\n",
      "Epoch 0, loss: 11.427582\n",
      "== W == 0.2805959517427752\n",
      "log loss =  102664.12019476146\n",
      "Epoch 1, loss: 11.427538\n",
      "== W == 0.28012138538972764\n",
      "log loss =  102664.0912529614\n",
      "Epoch 2, loss: 11.427494\n",
      "== W == 0.2796476238823571\n",
      "log loss =  102664.06234781226\n",
      "Epoch 3, loss: 11.427450\n",
      "== W == 0.27917466584189615\n",
      "log loss =  102664.03347925605\n",
      "Epoch 4, loss: 11.427407\n",
      "== W == 0.27870250989197776\n",
      "log loss =  102664.004647235\n",
      "Epoch 5, loss: 11.427363\n",
      "== W == 0.27823115465863174\n",
      "log loss =  102663.97585169137\n",
      "Epoch 6, loss: 11.427319\n",
      "== W == 0.2777605987702798\n",
      "log loss =  102663.94709256759\n",
      "Epoch 7, loss: 11.427276\n",
      "== W == 0.27729084085773187\n",
      "log loss =  102663.9183698062\n",
      "Epoch 8, loss: 11.427232\n",
      "== W == 0.27682187955418125\n",
      "log loss =  102663.88968334989\n",
      "Epoch 9, loss: 11.427189\n",
      "== W == 0.2763537134952006\n",
      "log loss =  102663.8610331414\n",
      "Epoch 0, loss: 11.427146\n",
      "== W == 0.2758863413187379\n",
      "log loss =  102663.83241912366\n",
      "Epoch 1, loss: 11.427103\n",
      "== W == 0.2754197616651116\n",
      "log loss =  102663.80384123969\n",
      "Epoch 2, loss: 11.427059\n",
      "== W == 0.2749539731770071\n",
      "log loss =  102663.77529943261\n",
      "Epoch 3, loss: 11.427016\n",
      "== W == 0.27448897449947185\n",
      "log loss =  102663.74679364575\n",
      "Epoch 4, loss: 11.426974\n",
      "== W == 0.27402476427991157\n",
      "log loss =  102663.71832382242\n",
      "Epoch 5, loss: 11.426931\n",
      "== W == 0.2735613411680857\n",
      "log loss =  102663.68988990616\n",
      "Epoch 6, loss: 11.426888\n",
      "== W == 0.27309870381610346\n",
      "log loss =  102663.66149184061\n",
      "Epoch 7, loss: 11.426845\n",
      "== W == 0.27263685087841955\n",
      "log loss =  102663.63312956945\n",
      "Epoch 8, loss: 11.426803\n",
      "== W == 0.2721757810118296\n",
      "log loss =  102663.6048030366\n",
      "Epoch 9, loss: 11.426760\n",
      "== W == 0.2717154928754666\n",
      "log loss =  102663.576512186\n",
      "Epoch 0, loss: 11.426718\n",
      "== W == 0.27125598513079646\n",
      "log loss =  102663.54825696179\n",
      "Epoch 1, loss: 11.426675\n",
      "== W == 0.2707972564416134\n",
      "log loss =  102663.52003730813\n",
      "Epoch 2, loss: 11.426633\n",
      "== W == 0.2703393054740364\n",
      "log loss =  102663.4918531694\n",
      "Epoch 3, loss: 11.426591\n",
      "== W == 0.26988213089650487\n",
      "log loss =  102663.46370448996\n",
      "Epoch 4, loss: 11.426549\n",
      "== W == 0.26942573137977416\n",
      "log loss =  102663.43559121445\n",
      "Epoch 5, loss: 11.426507\n",
      "== W == 0.26897010559691176\n",
      "log loss =  102663.40751328754\n",
      "Epoch 6, loss: 11.426465\n",
      "== W == 0.2685152522232932\n",
      "log loss =  102663.379470654\n",
      "Epoch 7, loss: 11.426423\n",
      "== W == 0.26806116993659757\n",
      "log loss =  102663.35146325876\n",
      "Epoch 8, loss: 11.426381\n",
      "== W == 0.2676078574168036\n",
      "log loss =  102663.3234910468\n",
      "Epoch 9, loss: 11.426340\n",
      "== W == 0.2671553133461858\n",
      "log loss =  102663.29555396331\n",
      "Epoch 0, loss: 11.426298\n",
      "== W == 0.26670353640931\n",
      "log loss =  102663.26765195352\n",
      "Epoch 1, loss: 11.426257\n",
      "== W == 0.26625252529302906\n",
      "log loss =  102663.23978496282\n",
      "Epoch 2, loss: 11.426215\n",
      "== W == 0.2658022786864795\n",
      "log loss =  102663.21195293663\n",
      "Epoch 3, loss: 11.426174\n",
      "== W == 0.2653527952810767\n",
      "log loss =  102663.1841558206\n",
      "Epoch 4, loss: 11.426133\n",
      "== W == 0.26490407377051123\n",
      "log loss =  102663.15639356045\n",
      "Epoch 5, loss: 11.426091\n",
      "== W == 0.26445611285074455\n",
      "log loss =  102663.12866610194\n",
      "Epoch 6, loss: 11.426050\n",
      "== W == 0.2640089112200052\n",
      "log loss =  102663.10097339108\n",
      "Epoch 7, loss: 11.426009\n",
      "== W == 0.26356246757878443\n",
      "log loss =  102663.07331537384\n",
      "Epoch 8, loss: 11.425968\n",
      "== W == 0.26311678062983257\n",
      "log loss =  102663.04569199643\n",
      "Epoch 9, loss: 11.425927\n",
      "== W == 0.26267184907815433\n",
      "log loss =  102663.0181032051\n",
      "Epoch 0, loss: 11.425887\n",
      "== W == 0.26222767163100563\n",
      "log loss =  102662.99054894625\n",
      "Epoch 1, loss: 11.425846\n",
      "== W == 0.261784246997889\n",
      "log loss =  102662.96302916633\n",
      "Epoch 2, loss: 11.425805\n",
      "== W == 0.2613415738905494\n",
      "log loss =  102662.93554381197\n",
      "Epoch 3, loss: 11.425765\n",
      "== W == 0.2608996510229708\n",
      "log loss =  102662.9080928299\n",
      "Epoch 4, loss: 11.425724\n",
      "== W == 0.260458477111372\n",
      "log loss =  102662.88067616693\n",
      "Epoch 5, loss: 11.425684\n",
      "== W == 0.26001805087420204\n",
      "log loss =  102662.85329376998\n",
      "Epoch 6, loss: 11.425644\n",
      "== W == 0.25957837103213716\n",
      "log loss =  102662.82594558611\n",
      "Epoch 7, loss: 11.425603\n",
      "== W == 0.259139436308076\n",
      "log loss =  102662.79863156247\n",
      "Epoch 8, loss: 11.425563\n",
      "== W == 0.25870124542713635\n",
      "log loss =  102662.77135164631\n",
      "Epoch 9, loss: 11.425523\n",
      "== W == 0.2582637971166505\n",
      "log loss =  102662.74410578501\n",
      "Epoch 0, loss: 11.425483\n",
      "== W == 0.25782709010616184\n",
      "log loss =  102662.71689392606\n",
      "Epoch 1, loss: 11.425443\n",
      "== W == 0.25739112312742035\n",
      "log loss =  102662.68971601702\n",
      "Epoch 2, loss: 11.425403\n",
      "== W == 0.25695589491437937\n",
      "log loss =  102662.66257200563\n",
      "Epoch 3, loss: 11.425364\n",
      "== W == 0.2565214042031909\n",
      "log loss =  102662.63546183964\n",
      "Epoch 4, loss: 11.425324\n",
      "== W == 0.2560876497322025\n",
      "log loss =  102662.608385467\n",
      "Epoch 5, loss: 11.425284\n",
      "== W == 0.25565463024195245\n",
      "log loss =  102662.5813428357\n",
      "Epoch 6, loss: 11.425245\n",
      "== W == 0.2552223444751667\n",
      "log loss =  102662.5543338939\n",
      "Epoch 7, loss: 11.425205\n",
      "== W == 0.2547907911767542\n",
      "log loss =  102662.5273585898\n",
      "Epoch 8, loss: 11.425166\n",
      "== W == 0.2543599690938037\n",
      "log loss =  102662.50041687176\n",
      "Epoch 9, loss: 11.425127\n",
      "== W == 0.25392987697557945\n",
      "log loss =  102662.47350868818\n",
      "Epoch 0, loss: 11.425088\n",
      "== W == 0.2535005135735174\n",
      "log loss =  102662.44663398765\n",
      "Epoch 1, loss: 11.425048\n",
      "== W == 0.2530718776412212\n",
      "log loss =  102662.41979271884\n",
      "Epoch 2, loss: 11.425009\n",
      "== W == 0.25264396793445854\n",
      "log loss =  102662.39298483044\n",
      "Epoch 3, loss: 11.424970\n",
      "== W == 0.25221678321115737\n",
      "log loss =  102662.36621027139\n",
      "Epoch 4, loss: 11.424931\n",
      "== W == 0.25179032223140163\n",
      "log loss =  102662.3394689906\n",
      "Epoch 5, loss: 11.424893\n",
      "== W == 0.251364583757428\n",
      "log loss =  102662.31276093716\n",
      "Epoch 6, loss: 11.424854\n",
      "== W == 0.25093956655362126\n",
      "log loss =  102662.28608606027\n",
      "Epoch 7, loss: 11.424815\n",
      "== W == 0.25051526938651136\n",
      "log loss =  102662.25944430918\n",
      "Epoch 8, loss: 11.424777\n",
      "== W == 0.2500916910247689\n",
      "log loss =  102662.2328356333\n",
      "Epoch 9, loss: 11.424738\n",
      "== W == 0.24966883023920175\n",
      "log loss =  102662.20625998211\n",
      "Epoch 0, loss: 11.424700\n",
      "== W == 0.24924668580275083\n",
      "log loss =  102662.17971730516\n",
      "Epoch 1, loss: 11.424661\n",
      "== W == 0.2488252564904867\n",
      "log loss =  102662.15320755221\n",
      "Epoch 2, loss: 11.424623\n",
      "== W == 0.24840454107960563\n",
      "log loss =  102662.12673067302\n",
      "Epoch 3, loss: 11.424585\n",
      "== W == 0.24798453834942574\n",
      "log loss =  102662.10028661747\n",
      "Epoch 4, loss: 11.424546\n",
      "== W == 0.24756524708138308\n",
      "log loss =  102662.0738753356\n",
      "Epoch 5, loss: 11.424508\n",
      "== W == 0.24714666605902835\n",
      "log loss =  102662.04749677745\n",
      "Epoch 6, loss: 11.424470\n",
      "== W == 0.2467287940680226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss =  102662.0211508933\n",
      "Epoch 7, loss: 11.424432\n",
      "== W == 0.2463116298961338\n",
      "log loss =  102661.99483763339\n",
      "Epoch 8, loss: 11.424395\n",
      "== W == 0.24589517233323288\n",
      "log loss =  102661.96855694815\n",
      "Epoch 9, loss: 11.424357\n",
      "== W == 0.2454794201712901\n",
      "log loss =  102661.9423087881\n",
      "Epoch 0, loss: 11.424319\n",
      "== W == 0.24506437220437136\n",
      "log loss =  102661.91609310379\n",
      "Epoch 1, loss: 11.424281\n",
      "== W == 0.24465002722863438\n",
      "log loss =  102661.88990984598\n",
      "Epoch 2, loss: 11.424244\n",
      "== W == 0.24423638404232495\n",
      "log loss =  102661.86375896544\n",
      "Epoch 3, loss: 11.424206\n",
      "== W == 0.24382344144577317\n",
      "log loss =  102661.83764041308\n",
      "Epoch 4, loss: 11.424169\n",
      "== W == 0.24341119824139004\n",
      "log loss =  102661.81155413992\n",
      "Epoch 5, loss: 11.424132\n",
      "== W == 0.24299965323366327\n",
      "log loss =  102661.78550009706\n",
      "Epoch 6, loss: 11.424094\n",
      "== W == 0.2425888052291541\n",
      "log loss =  102661.75947823568\n",
      "Epoch 7, loss: 11.424057\n",
      "== W == 0.24217865303649316\n",
      "log loss =  102661.73348850706\n",
      "Epoch 8, loss: 11.424020\n",
      "== W == 0.24176919546637712\n",
      "log loss =  102661.70753086265\n",
      "Epoch 9, loss: 11.423983\n",
      "== W == 0.24136043133156487\n",
      "log loss =  102661.6816052539\n",
      "Epoch 0, loss: 11.423946\n",
      "== W == 0.24095235944687365\n",
      "log loss =  102661.65571163244\n",
      "Epoch 1, loss: 11.423909\n",
      "== W == 0.24054497862917595\n",
      "log loss =  102661.62984994991\n",
      "Epoch 2, loss: 11.423872\n",
      "== W == 0.24013828769739515\n",
      "log loss =  102661.60402015815\n",
      "Epoch 3, loss: 11.423835\n",
      "== W == 0.23973228547250242\n",
      "log loss =  102661.57822220903\n",
      "Epoch 4, loss: 11.423799\n",
      "== W == 0.2393269707775128\n",
      "log loss =  102661.55245605449\n",
      "Epoch 5, loss: 11.423762\n",
      "== W == 0.23892234243748178\n",
      "log loss =  102661.52672164666\n",
      "Epoch 6, loss: 11.423726\n",
      "== W == 0.23851839927950114\n",
      "log loss =  102661.50101893766\n",
      "Epoch 7, loss: 11.423689\n",
      "== W == 0.2381151401326963\n",
      "log loss =  102661.47534787982\n",
      "Epoch 8, loss: 11.423653\n",
      "== W == 0.23771256382822156\n",
      "log loss =  102661.44970842545\n",
      "Epoch 9, loss: 11.423616\n",
      "== W == 0.23731066919925747\n",
      "   learn rate  regul  acc_val  acc_train\n",
      "0   1000.0000   10.0    0.049      0.049\n",
      "1    100.0000   10.0    0.049      0.049\n",
      "2     10.0000   10.0    0.049      0.049\n",
      "3      0.1000   10.0    0.129      0.129\n",
      "4      0.0100   10.0    0.129      0.129\n",
      "5      0.0010   10.0    0.127      0.127\n",
      "6      0.0001   10.0    0.118      0.118\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-196d39f1fe27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best validation accuracy achieved: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxU5dn/8c81kz2BkA0EAgQQZV8DYvFH1SqFVsWCVq1WqQu2arX6PLVorVp8bFGpT0vrBopYpaJ15bFYFNcuoEkQUTbZIewECIbsyf37IwMNIZADTDKZyff9euWVmXPu+5zroLnOmftc5x5zziEiIi2DL9QBiIhI01HSFxFpQZT0RURaECV9EZEWRElfRKQFUdIXEWlBPCV9MxttZqvMbI2ZTapn/Y/N7AszW2Jm/zSz3rXW3RXot8rMvh3M4EVE5PhYQ3X6ZuYHvgLOB/KBHOAK59zyWm1aO+f2B15fBNzknBsdSP4vAsOADsAC4DTnXFVjHIyIiByblyv9YcAa59w651w5MAcYW7vBwYQfkAgcPJOMBeY458qcc+uBNYHtiYhICER5aNMR2FzrfT5wRt1GZnYzcAcQA5xbq++iOn071tN3IjARIDExcUjPnj29xC7SJJyDL7cW0q51HG1bxYY6HJF65eXl7XbOZTTUzkvSt3qWHTEm5Jx7DHjMzH4A3ANccxx9pwPTAbKzs11ubq6HsESaRnllNafd8zY///bp3HzOqaEOR6ReZrbRSzsvwzv5QKda7zOBrcdoPwe4+AT7ijQ7vsClS3W15qmS8Ocl6ecAPcysq5nFAJcDc2s3MLMetd5+F1gdeD0XuNzMYs2sK9AD+PTkwxZpOj6ryfrK+RIJGhzecc5VmtktwHzAD8x0zi0zs8lArnNuLnCLmZ0HVAB7qRnaIdDuZWA5UAncrModCTd28EpfM9JKBPAypo9zbh4wr86ye2u9vu0YfR8EHjzRAEVCzcwwA01DfmIqKirIz8+ntLQ01KFEhLi4ODIzM4mOjj6h/p6SvkhL5zPT8M4Jys/Pp1WrVmRlZWFWX22HeOWco6CggPz8fLp27XpC29A0DCIe+EzDOyeqtLSUtLQ0JfwgMDPS0tJO6lOTkr6IB6Yr/ZOihB88J/tvqaQv4oFPY/oSIZT0RTyoGdNX0g9XSUlJoQ6hXvv27ePxxx9v0n0q6Yt4oBu5AlBVdfwV55WVlUddp6Qv0kyZbuRGjEceeYShQ4fSv39/7rvvvkPLL774YoYMGUKfPn2YPn36oeVJSUnce++9nHHGGSxcuJCsrCzuu+8+Bg8eTL9+/Vi5cuUR+5g1axaXXnopF154IaNGjaKoqIhvfetbh/q8+eabAEyaNIm1a9cycOBAfv7znx8zvmBRyaaIBz4zlPNP3q//bxnLt+5vuOFx6N2hNfdd2MdT23feeYfVq1fz6aef4pzjoosu4uOPP2bkyJHMnDmT1NRUSkpKGDp0KOPHjyctLY0DBw7Qt29fJk+efGg76enpLF68mMcff5ypU6fy9NNPH7GvhQsXsnTpUlJTU6msrOT111+ndevW7N69m+HDh3PRRRcxZcoUvvzyS5YsWdJgfMGipC/igUo2I8M777zDO++8w6BBgwAoKipi9erVjBw5kmnTpvH6668DsHnzZlavXk1aWhp+v5/x48cftp1x48YBMGTIEF577bV693X++eeTmpoK1BQB3H333Xz88cf4fD62bNnCjh07jiu+YFHSF/FAN3KDw+sVeWNxznHXXXdx4403Hrb8ww8/ZMGCBSxcuJCEhATOPvvsQ7XwcXFx+P3+w9rHxtZMse33+486Zp+YmHjo9ezZs9m1axd5eXlER0eTlZVVb6390eILJo3pi3igOv3I8O1vf5uZM2dSVFQEwJYtW9i5cyeFhYWkpKSQkJDAypUrWbRoUQNbOj6FhYW0bduW6OhoPvjgAzZurJkFuVWrVnz99dcNxhdMutIX8UB1+pFh1KhRrFixgjPPPBOouUn7wgsvMHr0aJ588kn69+/P6aefzvDhw4O63yuvvJILL7yQ7OxsBg4cyMEvikpLS2PEiBH07duXMWPG8Mgjj9QbX9u2bYMWS4PfkdvU9CUq0hwN/817fPO0DB66pH+oQwk7K1asoFevXqEOI6LU929qZnnOueyG+mp4R8QD3ciVSKGkL+KBxvQlUijpi3jg82lMXyKDkr6IByrZlEihpC/igebekUihpC/igebekUihpC/igebekaNprtM2H42SvogHKtmMHM45qqurQx1GyCjpi3igG7nhbcOGDfTq1YubbrqJwYMH8/zzz3PmmWcyePBgLr300kPTHsybN4+ePXty1llnceutt3LBBRcAcP/99zN16tRD2+vbty8bNmwIxaGcNE3DIOKB6vSD5O1JsP2L4G7zlH4wZkqDzVatWsWzzz7L5MmTGTduHAsWLCAxMZGHHnqIRx99lDvvvJMbb7yRjz/+mK5du3LFFVcEN85mQlf6Ih5o7p3w16VLF4YPH86iRYtYvnw5I0aMYODAgTz33HNs3LiRlStX0q1bN7p27QoQsUlfV/oiHqhkM0g8XJE3loNTHTvnOP/883nxxRcPW//ZZ58dtW9UVNRh9wHqmxY5XOhKX8QD3ciNHMOHD+df//oXa9asAaC4uJivvvqKnj17sm7dukNj9S+99NKhPllZWSxevBiAxYsXs379+iaPO1iU9EU80Jh+5MjIyGDWrFlcccUV9O/fn+HDh7Ny5Uri4+N5/PHHGT16NGeddRbt2rUjOTkZgPHjx7Nnzx4GDhzIE088wWmnnRbiozhxnoZ3zGw08AfADzztnJtSZ/0dwPVAJbALuNY5tzGwrgo4eOdmk3PuoiDFLtJkNKYf3rKysvjyyy8PvT/33HPJyck5ot0555zDypUrcc5x8803k51dM1NxfHw877zzTr3bPlj5Ey4avNI3Mz/wGDAG6A1cYWa96zT7DMh2zvUHXgEerrWuxDk3MPCjhC9hSSWbLcOMGTMYOHAgffr0obCwsFG/tjBUvFzpDwPWOOfWAZjZHGAssPxgA+fcB7XaLwKuCmaQIqHmM6MFP8/TYtx+++3cfvvtoQ6jUXkZ0+8IbK71Pj+w7GiuA96u9T7OzHLNbJGZXXwCMYqEnObekUjh5Urf6llW7//9ZnYVkA18s9bizs65rWbWDXjfzL5wzq2t028iMBGgc+fOngIXaUo+M6p0J1cigJcr/XygU633mcDWuo3M7Dzgl8BFzrmyg8udc1sDv9cBHwKD6vZ1zk13zmU757IzMjKO6wBEmoLPpyt9iQxekn4O0MPMuppZDHA5MLd2AzMbBDxFTcLfWWt5ipnFBl6nAyOodS9AJFzoRq5EigaHd5xzlWZ2CzCfmpLNmc65ZWY2Gch1zs0FHgGSgL+aGfynNLMX8JSZVVNzgpninFPSl7CjOn2JFJ7q9J1z84B5dZbdW+v1eUfp92+g38kEKNIcqE5fgiErK4vc3FzS09NDFoOeyBXxQHPvSKTQhGsiHmjuneB46NOHWLlnZVC32TO1J78Y9osG21188cVs3ryZ0tJSbrvtNiZOnMjf//537r77bqqqqkhPT+e9996jqKiIn/70p+Tm5mJm3HfffYwfP/6I7T3xxBOsX7+ehx+ueRZ11qxZ5OXl8cc//rHefTUXSvoiHmhMP/zNnDmT1NRUSkpKGDp0KGPHjuWGG244NH/+nj17AHjggQdITk7miy9qZo/Zu3dvvdu75JJLOPPMMw8l/Zdeeolf/vKX9e5r/PjxpKWlNcFRNkxJX8QDjekHh5cr8sYybdo0Xn/9dQA2b97M9OnTGTly5KH581NTUwFYsGABc+bMOdQvJSWl3u1lZGTQrVs3Fi1aRI8ePVi1ahUjRoyod1+rV69W0hcJJyrZDG8ffvghCxYsYOHChSQkJHD22WczYMAAVq1adURb5xyBKsQGXXbZZbz88sv07NmT733ve5hZvftqTvPv60auiAe6kRveCgsLSUlJISEhgZUrV7Jo0SLKysr46KOPDs2Nf3B4Z9SoUfzpT3861PdowzsA48aN44033uDFF1/ksssuO+q+mhMlfREPNPdOeBs9ejSVlZX079+fX/3qVwwfPpyMjAymT5/OuHHjGDBgwKGkfc8997B371769u3LgAED+OCDD4663ZSUFHr37s3GjRsZNmzYUffVnGh4R8QDnxnK+eErNjaWt99+u951Y8aMOex9UlISzz33nOdtv/XWW573dfBbuUJJV/oiHqhkUyKFrvRFPNCN3JbtjDPOoKys7LBlzz//PP36hd+EA0r6Ih6YvkSlRfvkk09CHULQaHhHxAPV6UukUNIX8UAlmxIplPRFPNCXqEikUNIX8UBz70ikUNIX8UBj+lKf3/zmNyfU7/rrr2f58tB8n5SSvogHKtmU+hwt6TvnqD5GudfTTz9N7969GyusY1LJpogHupEbHNt/8xvKVgR3Pv3YXj055e67G2wX7Pn0J02aRElJCQMHDqRPnz48+OCDjBkzhnPOOYeFCxfyxhtvMGXKFHJycigpKeGSSy7h17/+NQBnn302U6dOJTs7m6SkJG677Tbeeust4uPjefPNN2nXrl1Q/41q05W+iAeaeyf8zZw5k7y8PHJzc5k2bRo7duzghhtu4NVXX+Xzzz/nr3/9K3D4fPpLly7l3HPPrXd7U6ZMIT4+niVLljB79mwAVq1axdVXX81nn31Gly5dePDBB8nNzWXp0qV89NFHLF269IjtHDhwgOHDh/P5558zcuRIZsyY0Xj/COhKX8QTzb0THF6uyBtLsOfTr0+XLl0Om2Dt5ZdfZvr06VRWVrJt2zaWL19O//79D+sTExPDBRdcAMCQIUN49913T+wAPVLSF/FAc++Et8aaT7+uxMTEQ6/Xr1/P1KlTycnJISUlhQkTJtQ7r350dPSh/fn9fiorK09o315peEfEA93IDW+NNZ9+dHQ0FRUV9a7bv38/iYmJJCcns2PHjqPOvNnUlPRFPFCdfnhrrPn0J06cSP/+/bnyyiuPWDdgwAAGDRpEnz59uPbaaw99lWKoWXOrPc7Ozna5ubmhDkPkMA//fSUz/rGO1Q9+J9ShhJ0VK1bQq1evUIcRUer7NzWzPOdcdkN9daUv4oFKNiVS6EauiAe6kduyaT59kRbGAiWbJ1PZ0ZKF+79bc5pP/2SH5DW8I+KBL5CwdLF//OLi4igoKNDcRUHgnKOgoIC4uLgT3oanK30zGw38AfADTzvnptRZfwdwPVAJ7AKudc5tDKy7Brgn0PR/nHPev3FYpJnwBS5Sq53DR/hesYZCZmYm+fn57Nq1K9ShRIS4uDgyMzNPuH+DSd/M/MBjwPlAPpBjZnOdc7WniPsMyHbOFZvZT4CHgcvMLBW4D8gGHJAX6Hv0wleRZsgXyPq6mXv8oqOjDz31KqHnZXhnGLDGObfOOVcOzAHG1m7gnPvAOVcceLsIOHga+jbwrnNuTyDRvwuMDk7oIk3Hal3pi4QzL0m/I7C51vv8wLKjuQ44+OiZp75mNtHMcs0sVx8BpTnSmL5ECi9Jv74BzHr/1zezq6gZynnkePo656Y757Kdc9kZGRkeQhJpWj5d6UuE8JL084FOtd5nAlvrNjKz84BfAhc558qOp69Ic3fwSl9JX8Kdl6SfA/Qws65mFgNcDsyt3cDMBgFPUZPwd9ZaNR8YZWYpZpYCjAosEwkrZrqRK5Ghweod51ylmd1CTbL2AzOdc8vMbDKQ65ybS81wThLw18Afxybn3EXOuT1m9gA1Jw6Ayc65PY1yJCKN6ODwjmrNJdx5qtN3zs0D5tVZdm+t1+cdo+9MYOaJBijSHPh0pS8RQk/kinigG7kSKZT0RTww3ciVCKGkL+KB6vQlUijpi3ig4R2JFEr6Ih7oRq5ECiV9EQ8Ozb2jrC9hTklfxAON6UukUNIX8cAX+EvRmL6EOyV9EQ80945ECiV9EQ80945ECiV9EQ80945ECiV9EQ9UsimRQklfxAM9nCWRQklfxAPNvSORQklfxAPV6UukUNIX8UDDOxIplPRFPNCNXIkUSvoiHpiu9CVCKOmLePCfMX0lfQlvSvoiHmh4RyKFkr6IBz5NrSwRQklfxAPNvSORQklfxAPNvSORQklfxAOfT1f6EhmU9EU80MNZEimU9EU80Nw7EimU9EU80Nw7Eik8JX0zG21mq8xsjZlNqmf9SDNbbGaVZnZJnXVVZrYk8DM3WIGLNCUN70ikiGqogZn5gceA84F8IMfM5jrnltdqtgmYAPx3PZsocc4NDEKsIiGjh7MkUjSY9IFhwBrn3DoAM5sDjAUOJX3n3IbAuupGiFEk5DT3jkQKL8M7HYHNtd7nB5Z5FWdmuWa2yMwurq+BmU0MtMndtWvXcWxapGlo7h2JFF6SvtWz7Hj+z+/snMsGfgD83sy6H7Ex56Y757Kdc9kZGRnHsWmRpqHhHYkUXpJ+PtCp1vtMYKvXHTjntgZ+rwM+BAYdR3wizYJu5Eqk8JL0c4AeZtbVzGKAywFPVThmlmJmsYHX6cAIat0LEAkXmntHIkWDSd85VwncAswHVgAvO+eWmdlkM7sIwMyGmlk+cCnwlJktC3TvBeSa2efAB8CUOlU/ImFBc+9IpPBSvYNzbh4wr86ye2u9zqFm2Kduv38D/U4yRpGQ8+mJXIkQeiJXxINDSV9FyRLmlPRFPFCdvkQKJX0RDw5OraycL+FOSV/EA5VsSqRQ0hfxQA9nSaRQ0hfxQGP6EimU9EU80Nw7EimU9EU80PCORApPD2eJtHQ+g+z971P+2l+Z/1FCqMNpNBYVjS86Bl9sLL6YWPwxsfhj44iKjSMqJg5/bBzRcQlExdT8jolLIDo2nui4BKJjEoiNTyQ6Ji7UhyHHoKQv4sHCp+7hgffnNdywhagK/JTWs67aoNIPlX6jMgqq/D6qooyqKB/VUb6a39F+XJQfF+3HRUXV/I6OgqgoiInGoqMhOhqLicFiDj8R+WJi8MfE4Y89eFKKJzo2Hn9MHNFx8UTFxRMdE3/ohBQTl0hsfBLRsQn4fBrcUNIXacAHM35N56f+j7wuSRTd+geuHHFqqENqFK66msryMirKiikvK6a8tJjKspLAT9mh11XlZVSWlVJVVoqrKKeqrIzq8jJcRTnV5eW48nKoqMRVVEB5BVRUQkUFVlGJVVbhq6jCKirxl1XgKyrFX1mNr9Lhr6zGX+WIqnREVTn8VRB1nE9AO6Ai8FNcZ121QWErP0WpcVRkJEO7dGI6dCQxM4vULqfRrlsfktM6RvyJQUlf5Bj+8fzDtH10Dpt7JDO513/x363a0apN21CH1WJUVpRTXlZMRVkJ5aUHqCiteV1RVkxFaTGVZaVUBE5G1eXlNSelslKq656MysqpLinB7dxN9O59tF67k+TFW4muWgrUnCy2A+tjYH+bGErSE6nKSMXfvh1xHTJJ7tSdtKyetM/qS0x8eA/vKemLHMXCv/6RNr99li1ZSQyZ+Sblv8vVjdwmFhUdQ1R0DCS1Cfq2q6oqKdi6lh3rlrNv02qK8zdSsW0bvp0FxO7+mqSN62h9YO2h9sXAamB/Kx9FKXGUp7eGdulEd+hAYmYXUrqcRruufUhp16VZf1pQ0hepx6dzZ5B4/+Ps6BDPGS/MJbZ1OqA6/Uji90fRttPptO10+lHbFBftY/v6LynYsJKv8zdQuiWf6u07iNq1j1abCkheup2Yyi8Ptd8JbI6CwpRoSlITqcxog++UdsR1zKRVp26kZ/XklG59iU9o3QRHWD8lfZE6Fs9/gZi7H2V321iG/OUNktPaU15ZM7isnN+yJCS1oVu/s+jW76x611dXV7Nn+3p2rF/Gvo1rOJC/gfJtW7EdBcTu3k+bJZtILtoAfALU3PjeAOxPNL4OfFqobpdGdPv2JGZ2oU2XHrTN6kVah+74/Y2TnpX0RWr54sNX4ecPUpgSTb8XXiGlbWeg1tw7Gt+RWnw+H+kdupPeoXvN9wLWo6ykiO0bllOwYSWFm9dSmr+Zqu078O/aS8LWvSQv20FcxX++W6oA2O6HwuQoitMSqMhog++UtsR2yKRVp66kZp3OKVl9SUpOO6GYlfRFAlYs/BtlP/sVJUlRnP7CHDI6/qdKRw9nyYmKjU+iS69hdOk1rN711dXVFO7OZ/u6Zezd+BVF+Ruo2LYVduwmelchqV9uIXnhJnwuF6ipTNoMFMUb+1NjKUtrRXU77ycAJX0RYHXeexTd9HPKY310fe7PnNKl92HrNfeONBafz0dK2841nyqHj6m3TXlZMTs3rWLX+uUUbl5LyZbNVG3bjn/nHuJ37qf1ql2e96ekLy3eui/+ScGNt+KijI4zZ5DZY9ARbcwMM829I6ERE5tAZo9B9f6/ecjBK5MGKOlLi7b5qzy2X/dj/M6RPuNxsvqcedS2PjMN70jYU9KXFmvrui/YeM0E4sqrSXrqUU4ddPYx2/sMHMr6Et6U9KVF2pn/FWuuvpKkA5XEPPZbep4xusE+pit9iQDN97ExkUayZ/tGll31fVoXVuD73b30/X8Xe+rnM93IlfCnpC8tyr7dW/jsyotJ3VVG5ZQ7GXj+FZ77+sz0cJaEPSV9aTG+3reTnCvH0nZbKSWTbyH7uz86rv4+Mz2cJWFPSV9ahANf7+HfV11I+00H2H/PDQwff/Nxb8NMD2dJ+FPSl4hXVlLEP354AZlr9lPw8ysZ8YM7Tmg7NSWbyvoS3jwlfTMbbWarzGyNmU2qZ/1IM1tsZpVmdkmdddeY2erAzzXBClzEi/KyYt67ZgxdVu5lx63j+Oa195zwtnx6OEsiQINJ38z8wGPAGKA3cIWZ9a7TbBMwAfhLnb6pwH3AGcAw4D4zSzn5sEUaVlFeyrvXXkDXpbvJnziGc2968KS2p4ezJBJ4udIfBqxxzq1zzpUDc4CxtRs45zY455YCdb/c7NvAu865Pc65vcC7QMMF0SInqaqqkvk3jqVb3jY2XXMO59/x6Elv0zS8IxHAS9LvSM2kbgflB5Z54amvmU00s1wzy921y/vEQSL1qa6u5u2bv0f3hZtY//3hfPuux4Oy3batYnlvxU62F9b3deAi4cFL0q9vFh+vlzue+jrnpjvnsp1z2RkZGR43LXKk6upq5t1+Kd0/XMO6Cwcy+v5ngrbtqZcO4OvSCq57LocDZZVB265IU/KS9POBTrXeZwJbPW7/ZPqKHLe37/oh3ecvZ+2oXox5aHZQv6u0d4fW/OkHg1mxbT+3zfmMKg3wSxjy8heRA/Qws65mFgNcDsz1uP35wCgzSwncwB0VWCYSdG/ffx3d3lzM2m925zu/f6VRvpz6nJ5tue/CPixYsZMH/7Yi6NsXaWwN/lU45yqBW6hJ1iuAl51zy8xsspldBGBmQ80sH7gUeMrMlgX67gEeoObEkQNMDiwTCar5v72ZrDn/Zu3wTox5/I1GSfgHXfONLCZ8I4uZ/1rP8ws3NNp+RBqDNbe64+zsbJebmxvqMCSMvPvof5E5fR7rBp/CqFlvEx0T1+j7rKp2TPxzLh+s2skzE4ZyzultG32fIsdiZnnOueyG2umJXAlr7z/5KzKnz2N9v3TOf/ZvTZLwAfw+Y9oVg+h5Smtumb2YFdv2N8l+RU6Wkr6ErY+f/Q3t/vAKG3umcM5zfyMmNqFJ958YG8UzE7JJioviulk57NyvUk5p/pT0JSz96y+Pkvrw8+R3a8X/e/4t4hNahySO9snxPHPNUPaVVHDdc7kUl6uUU5o3JX0JO5+8/iSt/2cG2zon8o3Zb5HYKjWk8fTtmMy0ywexbGshP5uzRKWc0qwp6UtYyf3bs8T96g/sbB/H0Nlv0qpN87iBel7vdtzz3d68s3wHU95WKac0X/qOXAkbS96bg3/Sw+xNi2HgC6/RJt3rbCBN40cjsthQcIAZ/1hPVnoiV57RJdQhiRxBSV/CwrJ/vknVHb/mQHI0vV94mbT2XUMd0hHMjHsv6M2mPcXc++YyMlMS+OZpmlZEmhcN70izt/LT+ZTcchclCVGc+ufZtO10eqhDOqoov48//WAwPdomcfPsxaza/nWoQxI5jJK+NGtrPv+Iwp/cTnmM0fnZmXTo1i/UITUoKTaKmROGkhDj59pZOez8WqWc0nwo6UuztXH5J+y6/iacGac88xSdew4NdUiedWhTU8q550A5NzyXS0l5VahDEgGU9KWZ2rJmCfk/uo6oSkfqk3+gW7+zQh3SceuXmcwfLh/I0i2F3PHyEqpVyinNgJK+NDvbNy5n3dU/JK60msTHHua07PNCHdIJG9XnFH75nV68/eV2Hpq/MtThiKh6R5qXXVvWsPKHV9C6qJKoaQ/Q+xsXhDqkk3bdWV1Zv/sAT320jqy0RK4Y1jnUIUkLpqQvzcbenZv44qpLSd1TTtXUu+l/9iWhDikozIxfX9SHzXtLuOeNL+mUksBZPdJDHZa0UBrekWZh/57t5F35PdJ3llL+4O0MGf3DUIcUVFF+H4/9YBCnZiTxk9l5rN6hUk4JDSV9CbmiwgIWXXkh7bYUU3TfTxg2dmKoQ2oUreKieWZCNrFRfn40K4fdRWWhDklaICV9Canion3886rv0nFDEXsnTeAb37811CE1qsyUBJ65JpvdRWXc8OdcSitUyilNS0lfQqa8pJiPrvkunVYXsuuOyxh59S9CHVKTGNCpDb+/bCBLNu/jv/76uUo5pUkp6UtIVJSXsuBH3yFr2R623XQRZ99wf6hDalKj+7Zn0uie/G3pNn737qpQhyMtiKp3pMlVVpQz//oL6L5kB5uvPZ9Rtz4U6pBCYuLIbmwoOMBjH6ylS1oi38/uFOqQpAVQ0pcmVVVVyd9vupjun25hww/OYsyd00IdUsiYGZPH9iV/bwl3v/YFmSnxfKO7SjmlcWl4R5pMdXU1b986nu7/WM/68UMZc++MUIcUctF+H49dOZiu6Yn8+Pk81uwsCnVIEuGU9KVJVFdX8/Z/X073975i3Xf6MfqBWaEOqdloHRfNzAlDiYnyce2sHApUyimNSElfmsTffzWBbvO+YO23TmPM1Dn4fPpfr7ZOqQnMuDqbHftLmfh8nil1SIwAAAxNSURBVEo5pdHoL08a3duTb6DrqzmsPSuLMdNeVcI/ikGdU/jfywaSt3Evd76yFOdUyinBp78+aVTvPHIbWX/5J2uHdWT0E2/i96t24Fi+0689d44+nbmfb+V/3/0q1OFIBNJfoDSaBdN+Qadn3mH9wLaMmjGXqOiYUIcUFn7yze5s3F3MtPfX0CUtkfFDMkMdkkQQT1f6ZjbazFaZ2Rozm1TP+lgzeymw/hMzywoszzKzEjNbEvh5MrjhS3P14Yz7af/4XDb0SeW8Z98mJjYh1CGFDTPjf77Xl290T2PSa0tZtK4g1CFJBGkw6ZuZH3gMGAP0Bq4ws951ml0H7HXOnQr8L1D7aZu1zrmBgZ8fByluacY+/vNDZDz6Ept7JPPN5/5GTLwS/vGK9vt44sohdE5N4Mbn81i3S6WcEhxervSHAWucc+ucc+XAHGBsnTZjgecCr18BvmVmFrwwJVz8++VppEyZxZasJM564W8kJLUJdUhhKzkhmmcnDCPKZ1w7K4c9B8pDHZJEAC9j+h2BzbXe5wNnHK2Nc67SzAqBtMC6rmb2GbAfuMc5949j7Wz/to38/YHInFo30lWXlpL5Rg47OiYwfPb/kZSc1nAnOabOaQlMvzqbK2Ys4sbnc3nh+jOIjfKHOiwJY16Sfn1X7HVryY7WZhvQ2TlXYGZDgDfMrI9zbv9hnc0mAhMB+sTG0WX2Mc8L0oxt6ZTAkNmv0zr1lFCHEjGGdEnhd5cO4KcvfsakV7/g0e8PQB+k5UR5Sfr5QO2ZoDKBrUdpk29mUUAysMfVFBqXATjn8sxsLXAakFu7s3NuOjAdYPDgQa7Lxx+cwKFIc3B6QmvV4TeCCwd0YGPBAaa+8xVd0hL42XmnhTokCVNekn4O0MPMugJbgMuBH9RpMxe4BlgIXAK875xzZpZBTfKvMrNuQA9g3bF25vP5NQ4sUo+bzzmV9buL+f2C1WSlJXLxoI6hDknCUINJPzBGfwswH/ADM51zy8xsMpDrnJsLPAM8b2ZrgD3UnBgARgKTzawSqAJ+7Jzb0xgHIhLpzIzfjuvHln3F3PnKUjq0iWdY19RQhyVhxprbo97Z2dkuNze34YYiLdS+4nLGPf5v9haX8/pNI8hKTwx1SNIMmFmecy67oXYafBUJM20SYpg5YSgA187KYV+xSjnFOyV9kTCUlZ7I9Kuzyd9bwo3P51FeWR3qkCRMKOmLhKmhWak8fEl/Plm/h0mvaVZO8UYTromEsYsHdWRDwQF+v2A1XdMS+em3eoQ6JGnmlPRFwtxt3+rBxoJifvfuV3ROS2DsQJVyytFpeEckzJkZU8b3Y1hWKj9/ZSl5G1UVLUenpC8SAWKj/Dz1wyF0SI7jhj/nsbHgQKhDkmZKSV8kQqQk1pRyVjvHj2blUFhcEeqQpBlS0heJIN0yknjqqiFs3lPMj19QKaccSUlfJMKc0S2NKeP6s3BdAb98/QuVcsphVL0jEoHGD8lkY8EBpr2/hqz0RG4+59RQhyTNhJK+SIS6/fzT2FBQzCPzV9ElLYEL+ncIdUjSDGh4RyRCmRkPX9Kf7C4p3PHy5yzetDfUIUkzoKQvEsHiomtKOU9pHccNz+WyeU9xqEOSEFPSF4lwaUmxzJwwlIqq6ppSzhKVcrZkSvoiLcCpbZN48odD2LD7ADfNzqOiSqWcLZWSvkgL8Y3u6fx2XD/+taaAX73xpUo5WyhV74i0IJdmd2JDwQEe+2AtWemJ/Pib3UMdkjQxJX2RFua/zj+djQXFTHl7JV1SExjTr32oQ5ImpOEdkRbG5zOmXjqAQZ3b8LOXlrBk875QhyRNSElfpAWKi/Yz4+ps2raO5frncsnfq1LOlkJJX6SFSk+K5dkJQymrrOLaWTnsL1UpZ0ugpC/Sgp3athVPXjWEdbsOcPPsxWwrLKGqWlU9kcyaW9lWdna2y83NDXUYIi3KSzmb+MWrXwAQ5TNOSY6jQ3I8HdrE0aFNPB3axNMx8LtDmzhaxUWHOGKpy8zynHPZDbVT9Y6IcNnQzpzaNokV275m674SthWWsmVfCbkb97J96TYq61z9t4qLOuwkcPCk0D5womjXOo5ovwYSmiMlfREBYEiXVIZ0ST1ieVW1Y9fXZWzZV8LWWj9b9pWydV8JizftZV+db+nyGbRr/Z9PCR3axNWcJJL/86mhdXwUZtZUhycBSvoickz+wHDPKclxDOmSUm+bA2WVbCssYWvgRFD7pLA0fx/zvyylvM7UD4kx/sNOCgdPCAdPCqckxxETpU8LwaakLyInLTE2ilPbtuLUtq3qXV9d7dh9oKzOSeHgp4ZSlm0tZHdR+WF9zCAjKbbW/YTD7y+0T44jNTFGnxaOk6ekb2ajgT8AfuBp59yUOutjgT8DQ4AC4DLn3IbAuruA64Aq4Fbn3PygRS8iYcHnM9q2iqNtqzgGdmpTb5vSiiq2FZbWOSHUnBRWbNvPghU7KKvznb+xUb4j7i3UvuncPjmOuGh/Uxxi2Ggw6ZuZH3gMOB/IB3LMbK5zbnmtZtcBe51zp5rZ5cBDwGVm1hu4HOgDdAAWmNlpzrmqYB+IiIS3uGg/XdMT6ZqeWO965xx7DpSzdV/pYSeFgzedP1y1i51flx3RLz0phg5t4mnbKo6YKMNnht9n+M3w1f7t47Blfl+d9VbT5rD1ddr5ffxn+7XXH1rGkfv31Y2JI+I7PBaOWOb3ef+04+VKfxiwxjm3DsDM5gBjgdpJfyxwf+D1K8CfrOYz11hgjnOuDFhvZmsC21voOUIREWq+CSwtKZa0pFj6ZSbX26assoodhXVuOhfW3F/I31tMZbWjutpR5RxVh72G6iOWuf8sa16V7SfFS9LvCGyu9T4fOONobZxzlWZWCKQFli+q07dj3R2Y2URgYuBtmZl96Sn68JQO7A51EI1IxxfeIvn4IvnYAE730shL0q/vc0Pd897R2njpi3NuOjAdwMxyvTxgEK50fOFNxxe+IvnYoOb4vLTzUg+VD3Sq9T4T2Hq0NmYWBSQDezz2FRGRJuIl6ecAPcysq5nFUHNjdm6dNnOBawKvLwHedzXzO8wFLjezWDPrCvQAPg1O6CIicrwaHN4JjNHfAsynpmRzpnNumZlNBnKdc3OBZ4DnAzdq91BzYiDQ7mVqbvpWAjd7qNyZfuKHExZ0fOFNxxe+IvnYwOPxNbsJ10REpPHoGWcRkRZESV9EpAVpVknfzEab2SozW2Nmk0IdTzCZ2Uwz2xmpzyCYWScz+8DMVpjZMjO7LdQxBYuZxZnZp2b2eeDYfh3qmBqDmfnN7DMzeyvUsQSbmW0wsy/MbInX0sZwYmZtzOwVM1sZ+Bs886htm8uYfmC6h6+oNd0DcEWd6R7ClpmNBIqAPzvn+oY6nmAzs/ZAe+fcYjNrBeQBF0fCf7/A0+WJzrkiM4sG/gnc5pxb1EDXsGJmdwDZQGvn3AWhjieYzGwDkO2ci8iHs8zsOeAfzrmnA1WWCc65er/xvjld6R+a7sE5Vw4cnO4hIjjnPqamsikiOee2OecWB15/Daygnqevw5GrURR4Gx34aR5XS0FiZpnAd4GnQx2LHB8zaw2MpKaKEudc+dESPjSvpF/fdA8RkTRaGjPLAgYBn4Q2kuAJDH0sAXYC7zrnIubYAn4P3AlUN9QwTDngHTPLC0z7Ekm6AbuAZwPDc0+bWf2z1tG8kr6nKRukeTOzJOBV4GfOuf2hjidYnHNVzrmB1DxVPszMImaIzswuAHY65/JCHUsjGuGcGwyMAW4ODLdGiihgMPCEc24QcAA46j3R5pT0NWVDmAuMd78KzHbOvRbqeBpD4GPzh8DoEIcSTCOAiwLj3nOAc83shdCGFFzOua2B3zuB16kZTo4U+UB+rU+fr1BzEqhXc0r6XqZ7kGYqcLPzGWCFc+7RUMcTTGaWYWZtAq/jgfOAlaGNKnicc3c55zKdc1nU/N2975y7KsRhBY2ZJQaKCwgMe4wCIqaKzjm3HdhsZgdn2fwWh099f5hm83WJR5vuIcRhBY2ZvQicDaSbWT5wn3PumdBGFVQjgB8CXwTGvgHuds7NC2FMwdIeeC5QYeYDXnbORVxZYwRrB7we+FrFKOAvzrm/hzakoPspMDtwwbwO+NHRGjabkk0REWl8zWl4R0REGpmSvohIC6KkLyLSgijpi4i0IEr6IiItiJK+iEgLoqQvItKC/H+6/UFkQCRG9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e3, 1e2, 1e1,1e-1, 1e-2,1e-3, 1e-4,]\n",
    "reg_strengths = [1e1]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "data:list = list()\n",
    "train_val:list = list()\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "   \n",
    "for l in learning_rates:\n",
    "    for r in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        for i in range(0,num_epochs,10): \n",
    "            classifier.fit(train_X, train_y, epochs=10, learning_rate=l, batch_size=300, reg=r)\n",
    "            pred_val = classifier.predict(val_X)\n",
    "            pred_train = classifier.predict(train_X)\n",
    "            accuracy_val = multiclass_accuracy(pred_val, val_y)\n",
    "            accuracy_train = multiclass_accuracy(pred_train, val_y)\n",
    "            train_val.append({'acc_val':accuracy_val,'acc_train':accuracy_val})\n",
    "        data.append({'learn rate':l,'regul':r,'acc_val':accuracy_val,'acc_train':accuracy_val})\n",
    "        \n",
    "\n",
    "\n",
    "frame = pd.DataFrame(data)\n",
    "tr_val = pd.DataFrame(train_val)\n",
    "print(frame)\n",
    "frame.plot()\n",
    "axes = plt.gca()\n",
    "axes.set_ylim(0,0.3)\n",
    "\n",
    "# print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     acc_val  acc_train\n",
      "0      0.087      0.087\n",
      "1      0.087      0.087\n",
      "2      0.087      0.087\n",
      "3      0.087      0.087\n",
      "4      0.087      0.087\n",
      "..       ...        ...\n",
      "205    0.116      0.116\n",
      "206    0.116      0.116\n",
      "207    0.117      0.117\n",
      "208    0.116      0.116\n",
      "209    0.118      0.118\n",
      "\n",
      "[210 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_val=pd.DataFrame(train_val)\n",
    "tr_val.plot()\n",
    "print(tr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x288ace10>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      learn rate         regul  accuracy\n",
      "0   1.000000e-04  1.000000e-07     0.089\n",
      "1   1.000000e-04  1.000000e-08     0.112\n",
      "2   1.000000e-04  1.000000e-09     0.117\n",
      "3   1.000000e-04  1.000000e-10     0.081\n",
      "4   1.000000e-04  1.000000e-11     0.131\n",
      "5   1.000000e-05  1.000000e-07     0.089\n",
      "6   1.000000e-05  1.000000e-08     0.123\n",
      "7   1.000000e-05  1.000000e-09     0.078\n",
      "8   1.000000e-05  1.000000e-10     0.116\n",
      "9   1.000000e-05  1.000000e-11     0.102\n",
      "10  1.000000e-06  1.000000e-07     0.083\n",
      "11  1.000000e-06  1.000000e-08     0.088\n",
      "12  1.000000e-06  1.000000e-09     0.108\n",
      "13  1.000000e-06  1.000000e-10     0.107\n",
      "14  1.000000e-06  1.000000e-11     0.091\n",
      "15  1.000000e-07  1.000000e-07     0.112\n",
      "16  1.000000e-07  1.000000e-08     0.092\n",
      "17  1.000000e-07  1.000000e-09     0.113\n",
      "18  1.000000e-07  1.000000e-10     0.131\n",
      "19  1.000000e-07  1.000000e-11     0.094\n",
      "20  1.000000e-08  1.000000e-07     0.096\n",
      "21  1.000000e-08  1.000000e-08     0.110\n",
      "22  1.000000e-08  1.000000e-09     0.101\n",
      "23  1.000000e-08  1.000000e-10     0.133\n",
      "24  1.000000e-08  1.000000e-11     0.123\n",
      "25  1.000000e-09  1.000000e-07     0.084\n",
      "26  1.000000e-09  1.000000e-08     0.090\n",
      "27  1.000000e-09  1.000000e-09     0.131\n",
      "28  1.000000e-09  1.000000e-10     0.096\n",
      "29  1.000000e-09  1.000000e-11     0.091\n",
      "30  1.000000e-10  1.000000e-07     0.093\n",
      "31  1.000000e-10  1.000000e-08     0.094\n",
      "32  1.000000e-10  1.000000e-09     0.087\n",
      "33  1.000000e-10  1.000000e-10     0.075\n",
      "34  1.000000e-10  1.000000e-11     0.110\n",
      "35  1.000000e-11  1.000000e-07     0.087\n",
      "36  1.000000e-11  1.000000e-08     0.085\n",
      "37  1.000000e-11  1.000000e-09     0.111\n",
      "38  1.000000e-11  1.000000e-10     0.145\n",
      "39  1.000000e-11  1.000000e-11     0.093\n",
      "40  1.000000e-12  1.000000e-07     0.075\n",
      "41  1.000000e-12  1.000000e-08     0.089\n",
      "42  1.000000e-12  1.000000e-09     0.098\n",
      "43  1.000000e-12  1.000000e-10     0.090\n",
      "44  1.000000e-12  1.000000e-11     0.132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'accuracy')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(frame)\n",
    "ax=plt.subplot(projection = '3d')\n",
    "ax.scatter(frame['learn rate'],frame['regul'],frame['accuracy'],c = frame['accuracy'],cmap = 'viridis')\n",
    "ax.set_xlabel('learn rate')\n",
    "ax.set_ylabel('regul')\n",
    "ax.set_zlabel('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     learn rate    regul  accuracy\n",
      "0  1.000000e-01  0.00001     0.088\n",
      "1  1.000000e-02  0.00001     0.090\n",
      "2  1.000000e-03  0.00001     0.083\n",
      "3  1.000000e-04  0.00001     0.084\n",
      "4  1.000000e-05  0.00001     0.084\n",
      "5  1.000000e-06  0.00001     0.094\n",
      "6  1.000000e-07  0.00001     0.100\n",
      "7  1.000000e-08  0.00001     0.076\n",
      "8  1.000000e-09  0.00001     0.136\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'set_ylim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e4d38c4e96c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'set_ylim'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(frame)\n",
    "frame.plot()\n",
    "plt.set_ylim(0,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     learn rate    regul  accuracy\n",
      "0  1.000000e-01  0.00001     0.088\n",
      "1  1.000000e-02  0.00001     0.090\n",
      "2  1.000000e-03  0.00001     0.083\n",
      "3  1.000000e-04  0.00001     0.084\n",
      "4  1.000000e-05  0.00001     0.084\n",
      "5  1.000000e-06  0.00001     0.094\n",
      "6  1.000000e-07  0.00001     0.100\n",
      "7  1.000000e-08  0.00001     0.076\n",
      "8  1.000000e-09  0.00001     0.136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0.3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(frame)\n",
    "frame.plot()\n",
    "axes = plt.gca()\n",
    "axes.set_ylim(0,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.096\n"
     ]
    }
   ],
   "source": [
    "test_class = linear_classifer.LinearSoftmaxClassifier()\n",
    "test_pred = test_class.pred_rand(val_X)\n",
    "accuracy = multiclass_accuracy(test_pred, val_y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-51d73b5f6183>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulticlass_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Linear softmax classifier test set accuracy: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.17.0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
