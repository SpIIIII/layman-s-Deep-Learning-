{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "\n",
    "def fmt_items(lines,max_lines=0):\n",
    "    max_width=max([len(line)for line in lines])\n",
    "    empty =' '*max_width\n",
    "    lines = [line.ljust(max_width)for line in lines]\n",
    "    lines += [empty]*(max_lines - len(lines))\n",
    "    return lines\n",
    "    \n",
    "def pp (*list):\n",
    "    lines = [ str(item).split('\\n') for item in list]\n",
    "    max_lines=max([len(item)for  item in lines])\n",
    "    lines = [fmt_items(item,max_lines=max_lines)for item in lines]\n",
    "    lines_t= np.array(lines).T\n",
    "    print('\\n'.join([' '.join(line) for  line in lines_t]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==GRAD_CHECK== at (0,)  num = 6.000000000039306 anal = 6.0\n",
      "Gradient check passed!\n",
      "==GRAD_CHECK== at (0,)  num = 1.0000000000065512 anal = 1.0\n",
      "==GRAD_CHECK== at (1,)  num = 1.0000000000065512 anal = 1.0\n",
      "Gradient check passed!\n",
      "==GRAD_CHECK== at (0, 0)  num = 1.0000000000065512 anal = 1.0\n",
      "==GRAD_CHECK== at (0, 1)  num = 1.0000000000065512 anal = 1.0\n",
      "==GRAD_CHECK== at (1, 0)  num = 1.0000000000065512 anal = 1.0\n",
      "==GRAD_CHECK== at (1, 1)  num = 1.0000000000065512 anal = 1.0\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(array_sum(np.array([5.0, 2.0]))[0]-array_sum(np.array([1.0, 2.0]))[0])/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max = [2.06106005e-09 4.53978686e-05 9.99954600e-01]\n",
      "[2.06106005e-09 4.53978686e-05 9.99954600e-01]\n",
      "soft max = [1. 0. 0.]\n",
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import linear_classifer \n",
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "print(probs)\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "print(probs)\n",
    "# assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max = [4.50940412e-05 6.69254912e-03 9.93262357e-01]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7e7aa37f4ed7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[1;34m(probs, target_index)\u001b[0m\n\u001b[0;32m     36\u001b[0m     '''\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mpp\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'log = '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "    \n",
    "linear_classifer.cross_entropy_loss(probs, np.array(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = linear_classifer.softmax(np.array([1, 0, 0]))\n",
    "linear_classifer.cross_entropy_loss(probs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [1. 0. 0.] 0\n",
      "soft max = [0.57611688 0.21194156 0.21194156]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6b65fea27065>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloss1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mloss2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss,grad = '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36msoftmax_with_cross_entropy\u001b[1;34m(predictions, target_index)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0msoftmax_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmax_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[1;34m(probs, target_index)\u001b[0m\n\u001b[0;32m     37\u001b[0m     '''\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# log = - np.log(probs[target_index])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mpp\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'log = '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "loss, grad= linear_classifer.softmax_with_cross_entropy(np.array([1., 0., 0.]), 0)\n",
    "loss1, grad1= linear_classifer.softmax_with_cross_entropy(np.array([3, 0, 0]), 0)\n",
    "loss2, grad2 = linear_classifer.softmax_with_cross_entropy(np.array([-1, 0, 0]), 0)\n",
    "print((loss1-loss2)/4)\n",
    "print('loss,grad = ',loss, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [1. 0. 0.] 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2837c8be80ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO Implement combined function orsoftmax and cross entropy and produces gradient #-2.0491237621641485\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcheck_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\gradient_check.py\u001b[0m in \u001b[0;36mcheck_gradient\u001b[1;34m(f, x, delta, tol)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0morig_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mfx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalytic_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Functions shouldn't modify input variables\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-2837c8be80ff>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO Implement combined function orsoftmax and cross entropy and produces gradient #-2.0491237621641485\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcheck_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36msoftmax_with_cross_entropy\u001b[1;34m(predictions, target_index)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0msoftmax_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmax_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[1;34m(probs, target_index)\u001b[0m\n\u001b[0;32m     37\u001b[0m     '''\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;31m# pp ('log = ',log)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "# TODO Implement combined function orsoftmax and cross entropy and produces gradient #-2.0491237621641485\n",
    "\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([ 1,0,0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[ 1.  2. -1.  1.]]\n",
      "targets =  [[2]]\n",
      "enter of the function =  [[ 1.  2. -1.  1.]] [[2]]\n",
      "soft max = [[0.20603191 0.56005279 0.02788339 0.20603191]]\n",
      "log =  3.5797242232074917\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797242232074917 [[ 0.20603191  0.56005279 -0.97211661  0.20603191]] \n",
      "                                                                                                    \n",
      "enter of the function =  [[ 1.00002  2.      -1.       1.     ]] [[2]]\n",
      "soft max = [[0.20603518 0.56005049 0.02788327 0.20603106]]\n",
      "log =  3.5797283438783922\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797283438783922 [[ 0.20603518  0.56005049 -0.97211673  0.20603106]] \n",
      "                                                                                                    \n",
      "enter of the function =  [[ 0.99998  2.      -1.       1.     ]] [[2]]\n",
      "soft max = [[0.20602864 0.5600551  0.0278835  0.20603276]]\n",
      "log =  3.5797201026020242\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797201026020242 [[ 0.20602864  0.5600551  -0.9721165   0.20603276]] \n",
      "                                                                                                    \n",
      "==GRAD_CHECK== at (0, 0)  num = 0.20603190920009948 anal = 0.20603190919001857\n",
      "enter of the function =  [[ 1.       2.00002 -1.       1.     ]] [[2]]\n",
      "soft max = [[0.2060296  0.56005772 0.02788307 0.2060296 ]]\n",
      "log =  3.579735424312667\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.579735424312667 [[ 0.2060296   0.56005772 -0.97211693  0.2060296 ]] \n",
      "                                                                                                   \n",
      "enter of the function =  [[ 1.       1.99998 -1.       1.     ]] [[2]]\n",
      "soft max = [[0.20603422 0.56004787 0.0278837  0.20603422]]\n",
      "log =  3.5797130222008735\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797130222008735 [[ 0.20603422  0.56004787 -0.9721163   0.20603422]] \n",
      "                                                                                                    \n",
      "==GRAD_CHECK== at (0, 1)  num = 0.5600527948401712 anal = 0.5600527948339517\n",
      "enter of the function =  [[ 1.       2.      -0.99998  1.     ]] [[2]]\n",
      "soft max = [[0.20603179 0.56005248 0.02788393 0.20603179]]\n",
      "log =  3.5797047808806486\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797047808806486 [[ 0.20603179  0.56005248 -0.97211607  0.20603179]] \n",
      "                                                                                                    \n",
      "enter of the function =  [[ 1.       2.      -1.00002  1.     ]] [[2]]\n",
      "soft max = [[0.20603202 0.56005311 0.02788284 0.20603202]]\n",
      "log =  3.5797436655451773\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797436655451773 [[ 0.20603202  0.56005311 -0.97211716  0.20603202]] \n",
      "                                                                                                    \n",
      "==GRAD_CHECK== at (0, 2)  num = -0.9721166132181657 anal = -0.9721166132139888\n",
      "enter of the function =  [[ 1.       2.      -1.       1.00002]] [[2]]\n",
      "soft max = [[0.20603106 0.56005049 0.02788327 0.20603518]]\n",
      "log =  3.5797283438783922\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797283438783922 [[ 0.20603106  0.56005049 -0.97211673  0.20603518]] \n",
      "                                                                                                    \n",
      "enter of the function =  [[ 1.       2.      -1.       0.99998]] [[2]]\n",
      "soft max = [[0.20603276 0.5600551  0.0278835  0.20602864]]\n",
      "log =  3.5797201026020242\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797201026020242 [[ 0.20603276  0.5600551  -0.9721165   0.20602864]] \n",
      "                                                                                                    \n",
      "==GRAD_CHECK== at (0, 3)  num = 0.20603190920009948 anal = 0.20603190919001857\n",
      "Gradient check passed!\n",
      "predictions = [[ 2. -1. -1.  1.]\n",
      " [ 0.  1.  1.  1.]\n",
      " [ 1.  2. -1.  2.]]\n",
      "targets =  [[3]\n",
      " [3]\n",
      " [2]]\n",
      "enter of the function =  [[ 2. -1. -1.  1.]  [[3] \n",
      "                          [ 0.  1.  1.  1.]   [3] \n",
      "                          [ 1.  2. -1.  2.]]  [2]]\n",
      "soft max = [[0.19515646 0.00971627 0.00971627 0.07179405] \n",
      "            [0.02641156 0.07179405 0.07179405 0.07179405] \n",
      "            [0.07179405 0.19515646 0.00971627 0.19515646]]\n",
      "log =  9.901861007689417\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.3006203358964723 [[ 0.19515646  0.00971627  0.00971627 -0.26153928]  \n",
      "                                                 [ 0.02641156  0.07179405  0.07179405 -0.26153928]  \n",
      "                                                 [ 0.07179405  0.19515646 -0.32361707  0.19515646]] \n",
      "enter of the function =  [[ 2.00002 -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.1951596  0.00971623 0.00971623 0.07179377] \n",
      "            [0.02641145 0.07179377 0.07179377 0.07179377] \n",
      "            [0.07179377 0.1951557  0.00971623 0.1951557 ]]\n",
      "log =  9.901872717171424\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300624239057141 [[ 0.1951596   0.00971623  0.00971623 -0.26153956]  \n",
      "                                                [ 0.02641145  0.07179377  0.07179377 -0.26153956]  \n",
      "                                                [ 0.07179377  0.1951557  -0.3236171   0.1951557 ]] \n",
      "enter of the function =  [[ 1.99998 -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515332 0.00971631 0.00971631 0.07179433] \n",
      "            [0.02641166 0.07179433 0.07179433 0.07179433] \n",
      "            [0.07179433 0.19515722 0.00971631 0.19515722]]\n",
      "log =  9.901849298395895\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300616432798632 [[ 0.19515332  0.00971631  0.00971631 -0.261539  ]  \n",
      "                                                [ 0.02641166  0.07179433  0.07179433 -0.261539  ]  \n",
      "                                                [ 0.07179433  0.19515722 -0.32361703  0.19515722]] \n",
      "==GRAD_CHECK== at (0, 0)  num = 0.19515646273449147 anal = 0.19515646271894127\n",
      "enter of the function =  [[ 2.      -0.99998 -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515642 0.00971646 0.00971627 0.07179404] \n",
      "            [0.02641155 0.07179404 0.07179404 0.07179404] \n",
      "            [0.07179404 0.19515642 0.00971627 0.19515642]]\n",
      "log =  9.901861590671281\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.3006205302237603 [[ 0.19515642  0.00971646  0.00971627 -0.2615393 ]  \n",
      "                                                 [ 0.02641155  0.07179404  0.07179404 -0.2615393 ]  \n",
      "                                                 [ 0.07179404  0.19515642 -0.32361707  0.19515642]] \n",
      "enter of the function =  [[ 2.      -1.00002 -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.1951565  0.00971608 0.00971627 0.07179406] \n",
      "            [0.02641156 0.07179406 0.07179406 0.07179406] \n",
      "            [0.07179406 0.1951565  0.00971627 0.1951565 ]]\n",
      "log =  9.901860424719102\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300620141573034 [[ 0.1951565   0.00971608  0.00971627 -0.26153927]  \n",
      "                                                [ 0.02641156  0.07179406  0.07179406 -0.26153927]  \n",
      "                                                [ 0.07179406  0.1951565  -0.32361706  0.1951565 ]] \n",
      "==GRAD_CHECK== at (0, 1)  num = 0.009716268156712005 anal = 0.00971626815181842\n",
      "enter of the function =  [[ 2.      -1.      -0.99998  1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515642 0.00971627 0.00971646 0.07179404] \n",
      "            [0.02641155 0.07179404 0.07179404 0.07179404] \n",
      "            [0.07179404 0.19515642 0.00971627 0.19515642]]\n",
      "log =  9.901861590671281\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.3006205302237603 [[ 0.19515642  0.00971627  0.00971646 -0.2615393 ]  \n",
      "                                                 [ 0.02641155  0.07179404  0.07179404 -0.2615393 ]  \n",
      "                                                 [ 0.07179404  0.19515642 -0.32361707  0.19515642]] \n",
      "enter of the function =  [[ 2.      -1.      -1.00002  1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.1951565  0.00971627 0.00971608 0.07179406] \n",
      "            [0.02641156 0.07179406 0.07179406 0.07179406] \n",
      "            [0.07179406 0.1951565  0.00971627 0.1951565 ]]\n",
      "log =  9.901860424719102\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300620141573034 [[ 0.1951565   0.00971627  0.00971608 -0.26153927]  \n",
      "                                                [ 0.02641156  0.07179406  0.07179406 -0.26153927]  \n",
      "                                                [ 0.07179406  0.1951565  -0.32361706  0.1951565 ]] \n",
      "==GRAD_CHECK== at (0, 2)  num = 0.009716268156712005 anal = 0.00971626815181842\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.00002]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515618 0.00971625 0.00971625 0.07179538] \n",
      "            [0.02641152 0.07179395 0.07179395 0.07179395] \n",
      "            [0.07179395 0.19515618 0.00971625 0.19515618]]\n",
      "log =  9.901845315372428\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300615105124143 [[ 0.19515618  0.00971625  0.00971625 -0.26153795]  \n",
      "                                                [ 0.02641152  0.07179395  0.07179395 -0.26153939]  \n",
      "                                                [ 0.07179395  0.19515618 -0.32361708  0.19515618]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       0.99998]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515674 0.00971628 0.00971628 0.07179272] \n",
      "            [0.02641159 0.07179415 0.07179415 0.07179415] \n",
      "            [0.07179415 0.19515674 0.00971628 0.19515674]]\n",
      "log =  9.901876700086374\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.3006255666954583 [[ 0.19515674  0.00971628  0.00971628 -0.26154062]  \n",
      "                                                 [ 0.02641159  0.07179415  0.07179415 -0.26153918]  \n",
      "                                                 [ 0.07179415  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (0, 3)  num = -0.2615392828864138 anal = -0.2615392828872938\n",
      "enter of the function =  [[ 2.e+00 -1.e+00 -1.e+00  1.e+00]  [[3] \n",
      "                          [ 2.e-05  1.e+00  1.e+00  1.e+00]   [3] \n",
      "                          [ 1.e+00  2.e+00 -1.e+00  2.e+00]]  [2]]\n",
      "soft max = [[0.19515636 0.00971626 0.00971626 0.07179401] \n",
      "            [0.02641207 0.07179401 0.07179401 0.07179401] \n",
      "            [0.07179401 0.19515636 0.00971626 0.19515636]]\n",
      "log =  9.901862592398155\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300620864132718 [[ 0.19515636  0.00971626  0.00971626 -0.26153932]  \n",
      "                                                [ 0.02641207  0.07179401  0.07179401 -0.26153932]  \n",
      "                                                [ 0.07179401  0.19515636 -0.32361707  0.19515636]] \n",
      "enter of the function =  [[ 2.e+00 -1.e+00 -1.e+00  1.e+00]  [[3] \n",
      "                          [-2.e-05  1.e+00  1.e+00  1.e+00]   [3] \n",
      "                          [ 1.e+00  2.e+00 -1.e+00  2.e+00]]  [2]]\n",
      "soft max = [[0.19515657 0.00971627 0.00971627 0.07179409] \n",
      "            [0.02641104 0.07179409 0.07179409 0.07179409] \n",
      "            [0.07179409 0.19515657 0.00971627 0.19515657]]\n",
      "log =  9.901859423011537\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.3006198076705124 [[ 0.19515657  0.00971627  0.00971627 -0.26153924]  \n",
      "                                                 [ 0.02641104  0.07179409  0.07179409 -0.26153924]  \n",
      "                                                 [ 0.07179409  0.19515657 -0.32361706  0.19515657]] \n",
      "==GRAD_CHECK== at (1, 0)  num = 0.02641155514293558 anal = 0.026411555157523366\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.00002  1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515618 0.00971625 0.00971625 0.07179395] \n",
      "            [0.02641152 0.07179538 0.07179395 0.07179395] \n",
      "            [0.07179395 0.19515618 0.00971625 0.19515618]]\n",
      "log =  9.90186531537243\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.30062177179081 [[ 0.19515618  0.00971625  0.00971625 -0.26153939]  \n",
      "                                               [ 0.02641152  0.07179538  0.07179395 -0.26153939]  \n",
      "                                               [ 0.07179395  0.19515618 -0.32361708  0.19515618]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       0.99998  1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515674 0.00971628 0.00971628 0.07179415] \n",
      "            [0.02641159 0.07179272 0.07179415 0.07179415] \n",
      "            [0.07179415 0.19515674 0.00971628 0.19515674]]\n",
      "log =  9.901856700086373\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300618900028791 [[ 0.19515674  0.00971628  0.00971628 -0.26153918]  \n",
      "                                                [ 0.02641159  0.07179272  0.07179415 -0.26153918]  \n",
      "                                                [ 0.07179415  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (1, 1)  num = 0.07179405047130771 anal = 0.07179405044603954\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.00002  1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515618 0.00971625 0.00971625 0.07179395] \n",
      "            [0.02641152 0.07179395 0.07179538 0.07179395] \n",
      "            [0.07179395 0.19515618 0.00971625 0.19515618]]\n",
      "log =  9.90186531537243\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.30062177179081 [[ 0.19515618  0.00971625  0.00971625 -0.26153939]  \n",
      "                                               [ 0.02641152  0.07179395  0.07179538 -0.26153939]  \n",
      "                                               [ 0.07179395  0.19515618 -0.32361708  0.19515618]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       0.99998  1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515674 0.00971628 0.00971628 0.07179415] \n",
      "            [0.02641159 0.07179415 0.07179272 0.07179415] \n",
      "            [0.07179415 0.19515674 0.00971628 0.19515674]]\n",
      "log =  9.901856700086373\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300618900028791 [[ 0.19515674  0.00971628  0.00971628 -0.26153918]  \n",
      "                                                [ 0.02641159  0.07179415  0.07179272 -0.26153918]  \n",
      "                                                [ 0.07179415  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (1, 2)  num = 0.07179405047130771 anal = 0.07179405044603954\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.00002]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515618 0.00971625 0.00971625 0.07179395] \n",
      "            [0.02641152 0.07179395 0.07179395 0.07179538] \n",
      "            [0.07179395 0.19515618 0.00971625 0.19515618]]\n",
      "log =  9.901845315372428\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300615105124143 [[ 0.19515618  0.00971625  0.00971625 -0.26153939]  \n",
      "                                                [ 0.02641152  0.07179395  0.07179395 -0.26153795]  \n",
      "                                                [ 0.07179395  0.19515618 -0.32361708  0.19515618]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       0.99998]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515674 0.00971628 0.00971628 0.07179415] \n",
      "            [0.02641159 0.07179415 0.07179415 0.07179272] \n",
      "            [0.07179415 0.19515674 0.00971628 0.19515674]]\n",
      "log =  9.901876700086374\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.3006255666954583 [[ 0.19515674  0.00971628  0.00971628 -0.26153918]  \n",
      "                                                 [ 0.02641159  0.07179415  0.07179415 -0.26154062]  \n",
      "                                                 [ 0.07179415  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (1, 3)  num = -0.2615392828864138 anal = -0.2615392828872938\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.00002  2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515618 0.00971625 0.00971625 0.07179395] \n",
      "            [0.02641152 0.07179395 0.07179395 0.07179395] \n",
      "            [0.07179538 0.19515618 0.00971625 0.19515618]]\n",
      "log =  9.90186531537243\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.30062177179081 [[ 0.19515618  0.00971625  0.00971625 -0.26153939]  \n",
      "                                               [ 0.02641152  0.07179395  0.07179395 -0.26153939]  \n",
      "                                               [ 0.07179538  0.19515618 -0.32361708  0.19515618]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 0.99998  2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515674 0.00971628 0.00971628 0.07179415] \n",
      "            [0.02641159 0.07179415 0.07179415 0.07179415] \n",
      "            [0.07179272 0.19515674 0.00971628 0.19515674]]\n",
      "log =  9.901856700086373\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300618900028791 [[ 0.19515674  0.00971628  0.00971628 -0.26153918]  \n",
      "                                                [ 0.02641159  0.07179415  0.07179415 -0.26153918]  \n",
      "                                                [ 0.07179272  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (2, 0)  num = 0.07179405047130771 anal = 0.07179405044603954\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.00002 -1.       2.     ]]  [2]]\n",
      "soft max = [[0.1951557  0.00971623 0.00971623 0.07179377] \n",
      "            [0.02641145 0.07179377 0.07179377 0.07179377] \n",
      "            [0.07179377 0.1951596  0.00971623 0.1951557 ]]\n",
      "log =  9.901872717171424\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300624239057141 [[ 0.1951557   0.00971623  0.00971623 -0.26153956]  \n",
      "                                                [ 0.02641145  0.07179377  0.07179377 -0.26153956]  \n",
      "                                                [ 0.07179377  0.1951596  -0.3236171   0.1951557 ]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       1.99998 -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515722 0.00971631 0.00971631 0.07179433] \n",
      "            [0.02641166 0.07179433 0.07179433 0.07179433] \n",
      "            [0.07179433 0.19515332 0.00971631 0.19515722]]\n",
      "log =  9.901849298395895\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300616432798632 [[ 0.19515722  0.00971631  0.00971631 -0.261539  ]  \n",
      "                                                [ 0.02641166  0.07179433  0.07179433 -0.261539  ]  \n",
      "                                                [ 0.07179433  0.19515332 -0.32361703  0.19515722]] \n",
      "==GRAD_CHECK== at (2, 1)  num = 0.19515646273449147 anal = 0.19515646271894127\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -0.99998  2.     ]]  [2]]\n",
      "soft max = [[0.19515642 0.00971627 0.00971627 0.07179404] \n",
      "            [0.02641155 0.07179404 0.07179404 0.07179404] \n",
      "            [0.07179404 0.19515642 0.00971646 0.19515642]]\n",
      "log =  9.90184159067128\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300613863557093 [[ 0.19515642  0.00971627  0.00971627 -0.2615393 ]  \n",
      "                                                [ 0.02641155  0.07179404  0.07179404 -0.2615393 ]  \n",
      "                                                [ 0.07179404  0.19515642 -0.32361687  0.19515642]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.00002  2.     ]]  [2]]\n",
      "soft max = [[0.1951565  0.00971627 0.00971627 0.07179406] \n",
      "            [0.02641156 0.07179406 0.07179406 0.07179406] \n",
      "            [0.07179406 0.1951565  0.00971608 0.1951565 ]]\n",
      "log =  9.901880424719103\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300626808239701 [[ 0.1951565   0.00971627  0.00971627 -0.26153927]  \n",
      "                                                [ 0.02641156  0.07179406  0.07179406 -0.26153927]  \n",
      "                                                [ 0.07179406  0.1951565  -0.32361726  0.1951565 ]] \n",
      "==GRAD_CHECK== at (2, 2)  num = -0.3236170651899073 anal = -0.3236170651815149\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.00002]]  [2]]\n",
      "soft max = [[0.1951557  0.00971623 0.00971623 0.07179377] \n",
      "            [0.02641145 0.07179377 0.07179377 0.07179377] \n",
      "            [0.07179377 0.1951557  0.00971623 0.1951596 ]]\n",
      "log =  9.901872717171424\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300624239057141 [[ 0.1951557   0.00971623  0.00971623 -0.26153956]  \n",
      "                                                [ 0.02641145  0.07179377  0.07179377 -0.26153956]  \n",
      "                                                [ 0.07179377  0.1951557  -0.3236171   0.1951596 ]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       1.99998]]  [2]]\n",
      "soft max = [[0.19515722 0.00971631 0.00971631 0.07179433] \n",
      "            [0.02641166 0.07179433 0.07179433 0.07179433] \n",
      "            [0.07179433 0.19515722 0.00971631 0.19515332]]\n",
      "log =  9.901849298395895\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300616432798632 [[ 0.19515722  0.00971631  0.00971631 -0.261539  ]  \n",
      "                                                [ 0.02641166  0.07179433  0.07179433 -0.261539  ]  \n",
      "                                                [ 0.07179433  0.19515722 -0.32361703  0.19515332]] \n",
      "==GRAD_CHECK== at (2, 3)  num = 0.19515646273449147 anal = 0.19515646271894127\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "pp('predictions =',predictions)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "pp('targets = ',target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "print('predictions =',predictions)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "print('targets = ',target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[ 1. -1.]  [1 1]\n",
      "                          [ 0.  3.]]      \n",
      "soft max = [[0.11245721 0.01521943] \n",
      "            [0.0413707  0.83095266]]\n",
      "log =  4.370364905207625\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851824526038124 [[ 0.11245721 -0.48478057]  \n",
      "                                                 [ 0.0413707   0.33095266]] \n",
      "loss , grand (prediction), grad by W =  2.1851824526038124 [[ 0.11245721 -0.48478057]  [[-0.11245721  0.48478057]  \n",
      "                                                            [ 0.0413707   0.33095266]]  [-0.07108652  0.81573323]  \n",
      "                                                                                        [ 0.15382791 -0.15382791]] \n",
      "[[-0.11245721  0.48478057] \n",
      " [-0.07108652  0.81573323] \n",
      " [ 0.15382791 -0.15382791]]\n",
      "enter of the function =  [[ 1. -1.]  [1 1]\n",
      "                          [ 0.  3.]]      \n",
      "soft max = [[0.11245721 0.01521943] \n",
      "            [0.0413707  0.83095266]]\n",
      "log =  4.370364905207625\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851824526038124 [[ 0.11245721 -0.48478057]  \n",
      "                                                 [ 0.0413707   0.33095266]] \n",
      "loss , grand (prediction), grad by W =  2.1851824526038124 [[ 0.11245721 -0.48478057]  [[-0.11245721  0.48478057]  \n",
      "                                                            [ 0.0413707   0.33095266]]  [-0.07108652  0.81573323]  \n",
      "                                                                                        [ 0.15382791 -0.15382791]] \n",
      "enter of the function =  [[ 0.99998 -1.     ]  [1 1]\n",
      "                          [ 0.       3.     ]]      \n",
      "soft max = [[0.11245522 0.01521946] \n",
      "            [0.04137079 0.83095453]]\n",
      "log =  4.370360406959003\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851802034795016 [[ 0.11245522 -0.48478054]  \n",
      "                                                 [ 0.04137079  0.33095453]] \n",
      "loss , grand (prediction), grad by W =  2.1851802034795016 [[ 0.11245522 -0.48478054]  [[-0.11245522  0.48478054]  \n",
      "                                                            [ 0.04137079  0.33095453]]  [-0.07108443  0.81573507]  \n",
      "                                                                                        [ 0.15382601 -0.15382601]] \n",
      "enter of the function =  [[ 1.00002 -1.     ]  [1 1]\n",
      "                          [ 0.       3.     ]]      \n",
      "soft max = [[0.11245921 0.01521939] \n",
      "            [0.0413706  0.83095079]]\n",
      "log =  4.370369403536097\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851847017680486 [[ 0.11245921 -0.48478061]  \n",
      "                                                 [ 0.0413706   0.33095079]] \n",
      "loss , grand (prediction), grad by W =  2.1851847017680486 [[ 0.11245921 -0.48478061]  [[-0.11245921  0.48478061]  \n",
      "                                                            [ 0.0413706   0.33095079]]  [-0.07108861  0.8157314 ]  \n",
      "                                                                                        [ 0.15382981 -0.15382981]] \n",
      "==GRAD_CHECK== at (0, 0)  num = -0.11245721367458826 anal = -0.11245721367093255\n",
      "enter of the function =  [[ 1.      -1.00002]  [1 1]\n",
      "                          [ 0.       3.     ]]      \n",
      "soft max = [[0.11245725 0.01521913] \n",
      "            [0.04137071 0.83095291]]\n",
      "log =  4.370384296436465\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851921482182326 [[ 0.11245725 -0.48478087]  \n",
      "                                                 [ 0.04137071  0.33095291]] \n",
      "loss , grand (prediction), grad by W =  2.1851921482182326 [[ 0.11245725 -0.48478087]  [[-0.11245725  0.48478087]  \n",
      "                                                            [ 0.04137071  0.33095291]]  [-0.07108654  0.81573378]  \n",
      "                                                                                        [ 0.15382796 -0.15382796]] \n",
      "enter of the function =  [[ 1.      -0.99998]  [1 1]\n",
      "                          [ 0.       3.     ]]      \n",
      "soft max = [[0.11245718 0.01521973] \n",
      "            [0.04137068 0.83095241]]\n",
      "log =  4.370345513990775\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851727569953874 [[ 0.11245718 -0.48478027]  \n",
      "                                                 [ 0.04137068  0.33095241]] \n",
      "loss , grand (prediction), grad by W =  2.1851727569953874 [[ 0.11245718 -0.48478027]  [[-0.11245718  0.48478027]  \n",
      "                                                            [ 0.04137068  0.33095241]]  [-0.0710865   0.81573268]  \n",
      "                                                                                        [ 0.15382786 -0.15382786]] \n",
      "==GRAD_CHECK== at (0, 1)  num = 0.4847805711305497 anal = 0.48478057113584405\n",
      "enter of the function =  [[ 9.9998e-01 -1.0000e+00]  [1 1]\n",
      "                          [ 2.0000e-05  3.0000e+00]]      \n",
      "soft max = [[0.11245512 0.01521945] \n",
      "            [0.04137158 0.83095384]]\n",
      "log =  4.370362061806466\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.185181030903233 [[ 0.11245512 -0.48478055]  \n",
      "                                                [ 0.04137158  0.33095384]] \n",
      "loss , grand (prediction), grad by W =  2.185181030903233 [[ 0.11245512 -0.48478055]  [[-0.11245512  0.48478055]  \n",
      "                                                           [ 0.04137158  0.33095384]]  [-0.07108354  0.81573439]  \n",
      "                                                                                       [ 0.15382671 -0.15382671]] \n",
      "enter of the function =  [[ 1.00002e+00 -1.00000e+00]  [1 1]\n",
      "                          [-2.00000e-05  3.00000e+00]]      \n",
      "soft max = [[0.1124593  0.01521941] \n",
      "            [0.04136981 0.83095148]]\n",
      "log =  4.370367748727805\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851838743639025 [[ 0.1124593  -0.48478059]  \n",
      "                                                 [ 0.04136981  0.33095148]] \n",
      "loss , grand (prediction), grad by W =  2.1851838743639025 [[ 0.1124593  -0.48478059]  [[-0.1124593   0.48478059]  \n",
      "                                                            [ 0.04136981  0.33095148]]  [-0.07108949  0.81573207]  \n",
      "                                                                                        [ 0.15382911 -0.15382911]] \n",
      "==GRAD_CHECK== at (1, 0)  num = -0.07108651673970456 anal = -0.0710865167499724\n",
      "enter of the function =  [[ 1.      -1.00002]  [1 1]\n",
      "                          [ 0.       3.00002]]      \n",
      "soft max = [[0.11245538 0.01521888] \n",
      "            [0.04137002 0.83095572]]\n",
      "log =  4.370397534609193\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851987673045965 [[ 0.11245538 -0.48478112]  \n",
      "                                                 [ 0.04137002  0.33095572]] \n",
      "loss , grand (prediction), grad by W =  2.1851987673045965 [[ 0.11245538 -0.48478112]  [[-0.11245538  0.48478112]  \n",
      "                                                            [ 0.04137002  0.33095572]]  [-0.07108536  0.81573685]  \n",
      "                                                                                        [ 0.1538254  -0.1538254 ]] \n",
      "enter of the function =  [[ 1.      -0.99998]  [1 1]\n",
      "                          [ 0.       2.99998]]      \n",
      "soft max = [[0.11245905 0.01521998] \n",
      "            [0.04137137 0.8309496 ]]\n",
      "log =  4.370332275950659\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851661379753295 [[ 0.11245905 -0.48478002]  \n",
      "                                                 [ 0.04137137  0.3309496 ]] \n",
      "loss , grand (prediction), grad by W =  2.1851661379753295 [[ 0.11245905 -0.48478002]  [[-0.11245905  0.48478002]  \n",
      "                                                            [ 0.04137137  0.3309496 ]]  [-0.07108768  0.81572962]  \n",
      "                                                                                        [ 0.15383042 -0.15383042]] \n",
      "==GRAD_CHECK== at (1, 1)  num = 0.8157332316738318 anal = 0.8157332316797954\n",
      "enter of the function =  [[ 1.00002e+00 -1.00000e+00]  [1 1]\n",
      "                          [ 2.00000e-05  3.00000e+00]]      \n",
      "soft max = [[0.11245912 0.01521938] \n",
      "            [0.0413714  0.8309501 ]]\n",
      "log =  4.370371058376115\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851855291880575 [[ 0.11245912 -0.48478062]  \n",
      "                                                 [ 0.0413714   0.3309501 ]] \n",
      "loss , grand (prediction), grad by W =  2.1851855291880575 [[ 0.11245912 -0.48478062]  [[-0.11245912  0.48478062]  \n",
      "                                                            [ 0.0413714   0.3309501 ]]  [-0.07108772  0.81573072]  \n",
      "                                                                                        [ 0.15383051 -0.15383051]] \n",
      "enter of the function =  [[ 9.9998e-01 -1.0000e+00]  [1 1]\n",
      "                          [-2.0000e-05  3.0000e+00]]      \n",
      "soft max = [[0.11245531 0.01521948] \n",
      "            [0.04137    0.83095522]]\n",
      "log =  4.370358752143268\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.185179376071634 [[ 0.11245531 -0.48478052]  \n",
      "                                                [ 0.04137     0.33095522]] \n",
      "loss , grand (prediction), grad by W =  2.185179376071634 [[ 0.11245531 -0.48478052]  [[-0.11245531  0.48478052]  \n",
      "                                                           [ 0.04137     0.33095522]]  [-0.07108531  0.81573574]  \n",
      "                                                                                       [ 0.15382531 -0.15382531]] \n",
      "==GRAD_CHECK== at (2, 0)  num = 0.15382791058726752 anal = 0.15382791059189269\n",
      "enter of the function =  [[ 1.      -0.99998]  [1 1]\n",
      "                          [ 0.       3.00002]]      \n",
      "soft max = [[0.11245531 0.01521948] \n",
      "            [0.04137    0.83095522]]\n",
      "log =  4.370358752143268\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.185179376071634 [[ 0.11245531 -0.48478052]  \n",
      "                                                [ 0.04137     0.33095522]] \n",
      "loss , grand (prediction), grad by W =  2.185179376071634 [[ 0.11245531 -0.48478052]  [[-0.11245531  0.48478052]  \n",
      "                                                           [ 0.04137     0.33095522]]  [-0.07108531  0.81573574]  \n",
      "                                                                                       [ 0.15382531 -0.15382531]] \n",
      "enter of the function =  [[ 1.      -1.00002]  [1 1]\n",
      "                          [ 0.       2.99998]]      \n",
      "soft max = [[0.11245912 0.01521938] \n",
      "            [0.0413714  0.8309501 ]]\n",
      "log =  4.370371058376115\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851855291880575 [[ 0.11245912 -0.48478062]  \n",
      "                                                 [ 0.0413714   0.3309501 ]] \n",
      "loss , grand (prediction), grad by W =  2.1851855291880575 [[ 0.11245912 -0.48478062]  [[-0.11245912  0.48478062]  \n",
      "                                                            [ 0.0413714   0.3309501 ]]  [-0.07108772  0.81573072]  \n",
      "                                                                                        [ 0.15383051 -0.15383051]] \n",
      "==GRAD_CHECK== at (2, 1)  num = -0.15382791058726752 anal = -0.1538279105918927\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "pp(dW)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.06 [[ 0.01  0.02] \n",
      "                       [-0.01  0.01] \n",
      "                       [ 0.01  0.02]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.06 [[ 0.01  0.02] \n",
      "                       [-0.01  0.01] \n",
      "                       [ 0.01  0.02]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.060000200002 [[ 0.0100002  0.02     ] \n",
      "                                 [-0.01       0.01     ] \n",
      "                                 [ 0.01       0.02     ]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.059999800002 [[ 0.0099998  0.02     ] \n",
      "                                 [-0.01       0.01     ] \n",
      "                                 [ 0.01       0.02     ]]\n",
      "==GRAD_CHECK== at (0, 0)  num = 0.009999999999940612 anal = 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.06000040000200001 [[ 0.01       0.0200002] \n",
      "                                      [-0.01       0.01     ] \n",
      "                                      [ 0.01       0.02     ]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.059999600002 [[ 0.01       0.0199998] \n",
      "                                 [-0.01       0.01     ] \n",
      "                                 [ 0.01       0.02     ]]\n",
      "==GRAD_CHECK== at (0, 1)  num = 0.02000000000022817 anal = 0.02\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.059999800002 [[ 0.01       0.02     ] \n",
      "                                 [-0.0099998  0.01     ] \n",
      "                                 [ 0.01       0.02     ]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.060000200002 [[ 0.01       0.02     ] \n",
      "                                 [-0.0100002  0.01     ] \n",
      "                                 [ 0.01       0.02     ]]\n",
      "==GRAD_CHECK== at (1, 0)  num = -0.009999999999940612 anal = -0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.060000200002 [[ 0.01       0.02     ] \n",
      "                                 [-0.01       0.0100002] \n",
      "                                 [ 0.01       0.02     ]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.059999800002 [[ 0.01       0.02     ] \n",
      "                                 [-0.01       0.0099998] \n",
      "                                 [ 0.01       0.02     ]]\n",
      "==GRAD_CHECK== at (1, 1)  num = 0.009999999999940612 anal = 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.060000200002 [[ 0.01       0.02     ] \n",
      "                                 [-0.01       0.01     ] \n",
      "                                 [ 0.0100002  0.02     ]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.059999800002 [[ 0.01       0.02     ] \n",
      "                                 [-0.01       0.01     ] \n",
      "                                 [ 0.0099998  0.02     ]]\n",
      "==GRAD_CHECK== at (2, 0)  num = 0.009999999999940612 anal = 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.06000040000200001 [[ 0.01       0.02     ] \n",
      "                                      [-0.01       0.01     ] \n",
      "                                      [ 0.01       0.0200002]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.059999600002 [[ 0.01       0.02     ] \n",
      "                                 [-0.01       0.01     ] \n",
      "                                 [ 0.01       0.0199998]]\n",
      "==GRAD_CHECK== at (2, 1)  num = 0.02000000000022817 anal = 0.02\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss , grand (prediction) =  11.407754245650851 [[ 1.10183016e-05 -9.99798898e-05  1.10120799e-05 ...  1.11431312e-05 \n",
      "                                                   1.11880378e-05  1.11244247e-05]                                    \n",
      "                                                 [ 1.11241086e-05  1.10885994e-05  1.11341011e-05 ...  1.10924594e-05 \n",
      "                                                   1.10397386e-05  1.10504022e-05]                                    \n",
      "                                                 [ 1.09393556e-05  1.11736972e-05  1.12084050e-05 ...  1.09764153e-05 \n",
      "                                                   1.12364555e-05  1.11472021e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.09742226e-05  1.12698107e-05  1.11560314e-05 ...  1.10969608e-05 \n",
      "                                                   1.11516618e-05  1.10667981e-05]                                    \n",
      "                                                 [ 1.10232266e-05  1.11692841e-05  1.11340920e-05 ... -9.99259556e-05 \n",
      "                                                   1.11430426e-05  1.10165632e-05]                                    \n",
      "                                                 [ 1.11972682e-05  1.09906547e-05  1.10449777e-05 ...  1.11817383e-05 \n",
      "                                                   1.11813641e-05  1.10382759e-05]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "Epoch 0, loss: 11.438237\n",
      "== W == 0.08929612656268157\n",
      "loss , grand (prediction) =  11.407713582112898 [[ 1.13412798e-05 -1.00210207e-04  1.10746104e-05 ...  1.12246439e-05 \n",
      "                                                   1.13606574e-05  1.12914508e-05]                                    \n",
      "                                                 [ 1.11877597e-05  1.10569550e-05  1.11463857e-05 ...  1.11755346e-05 \n",
      "                                                   1.10995380e-05  1.12120750e-05]                                    \n",
      "                                                 [ 1.12072212e-05  1.09833744e-05  1.09596289e-05 ... -1.00096986e-04 \n",
      "                                                   1.10877263e-05  1.11129093e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11422582e-05  1.12257540e-05  1.11697849e-05 ... -9.99763098e-05 \n",
      "                                                   1.09598367e-05  1.10185389e-05]                                    \n",
      "                                                 [ 1.10730554e-05  1.12609338e-05  1.11919970e-05 ...  1.11373241e-05 \n",
      "                                                   1.10563839e-05  1.09854716e-05]                                    \n",
      "                                                 [ 1.10904939e-05  1.11187183e-05  1.09086146e-05 ...  1.10523590e-05 \n",
      "                                                   1.11761415e-05  1.11750554e-05]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "Epoch 1, loss: 11.437589\n",
      "== W == 0.0879819368435957\n",
      "loss , grand (prediction) =  11.407673471270376 [[ 1.11293795e-05  1.10237393e-05  1.10355364e-05 ...  1.10558311e-05 \n",
      "                                                   1.12455718e-05  1.13416399e-05]                                    \n",
      "                                                 [ 1.10429354e-05  1.12144242e-05 -1.00019777e-04 ...  1.10816399e-05 \n",
      "                                                   1.09013002e-05  1.10114483e-05]                                    \n",
      "                                                 [ 1.10225090e-05  1.12668055e-05  1.12051386e-05 ...  1.11898526e-05 \n",
      "                                                   1.10767753e-05  1.08617367e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11274001e-05  1.11356973e-05  1.10957970e-05 ...  1.11667128e-05 \n",
      "                                                   1.10659980e-05  1.11633613e-05]                                    \n",
      "                                                 [ 1.11830873e-05  1.10611599e-05  1.11307573e-05 ...  1.10010415e-05 \n",
      "                                                  -1.00093465e-04  1.10313045e-05]                                    \n",
      "                                                 [-1.00033139e-04  1.09149298e-05  1.09525655e-05 ...  1.11793989e-05 \n",
      "                                                   1.13509469e-05  1.13521838e-05]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "Epoch 2, loss: 11.436954\n",
      "== W == 0.08668823375556936\n",
      "loss , grand (prediction) =  11.40763390344159 [[ 1.13043942e-05 -9.98567012e-05  1.13058246e-05 ...  1.11600097e-05 \n",
      "                                                  1.11488578e-05  1.10955574e-05]                                    \n",
      "                                                [ 1.11076118e-05  1.12351345e-05  1.11825478e-05 ...  1.11151417e-05 \n",
      "                                                  1.09005163e-05  1.09258795e-05]                                    \n",
      "                                                [ 1.10891744e-05 -9.99045212e-05  1.12345010e-05 ...  1.11250182e-05 \n",
      "                                                  1.10084355e-05  1.09865174e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [ 1.11342978e-05  1.11193614e-05  1.11597228e-05 ...  1.10417510e-05 \n",
      "                                                  1.10393228e-05  1.10960628e-05]                                    \n",
      "                                                [ 1.09955017e-05  1.10620847e-05 -1.00196310e-04 ...  1.11771833e-05 \n",
      "                                                  1.09217041e-05  1.11212133e-05]                                    \n",
      "                                                [ 1.10900020e-05  1.10903688e-05  1.11350944e-05 ...  1.11689764e-05 \n",
      "                                                  1.11215374e-05  1.10466148e-05]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "Epoch 3, loss: 11.436332\n",
      "== W == 0.08541466594241873\n",
      "loss , grand (prediction) =  11.407594869169223 [[ 1.10445120e-05 -9.99097608e-05  1.11806397e-05 ...  1.11105996e-05 \n",
      "                                                   1.09992895e-05  1.07445387e-05]                                    \n",
      "                                                 [ 1.14255941e-05 -1.00196322e-04  1.11428800e-05 ...  1.10949086e-05 \n",
      "                                                   1.13021512e-05  1.15159040e-05]                                    \n",
      "                                                 [ 1.13737358e-05  1.10573151e-05  1.11457880e-05 ...  1.10477605e-05 \n",
      "                                                   1.14432758e-05  1.13276633e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.09112274e-05  1.12799153e-05  1.11520726e-05 ...  1.11164506e-05 \n",
      "                                                   1.10126918e-05  1.09106829e-05]                                    \n",
      "                                                 [ 1.10730806e-05  1.11020801e-05 -1.00077680e-04 ...  1.10618940e-05 \n",
      "                                                   1.12420918e-05  1.12943541e-05]                                    \n",
      "                                                 [ 1.11793469e-05  1.11189915e-05  1.11017086e-05 ...  1.11203796e-05 \n",
      "                                                   1.09903811e-05  1.12036760e-05]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "Epoch 4, loss: 11.435721\n",
      "== W == 0.08416088888093415\n",
      "loss , grand (prediction) =  11.407556359213995 [[ 1.11468960e-05  1.10991715e-05 -1.00065174e-04 ...  1.10843933e-05 \n",
      "                                                   1.10727642e-05  1.11624838e-05]                                    \n",
      "                                                 [-9.99872626e-05  1.10504069e-05  1.10549895e-05 ...  1.11468467e-05 \n",
      "                                                   1.11785115e-05  1.12202132e-05]                                    \n",
      "                                                 [ 1.09882089e-05  1.11903411e-05 -1.00130914e-04 ...  1.12071129e-05 \n",
      "                                                   1.10873200e-05  1.11348974e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.10375119e-05  1.11357756e-05  1.10595870e-05 ...  1.10619517e-05 \n",
      "                                                   1.11186592e-05  1.13900225e-05]                                    \n",
      "                                                 [ 1.11275066e-05  1.09641564e-05  1.12271558e-05 ...  1.09486943e-05 \n",
      "                                                   1.12604928e-05  1.14250095e-05]                                    \n",
      "                                                 [ 1.11569599e-05  1.11343398e-05  1.10674815e-05 ...  1.11525261e-05 \n",
      "                                                   1.11488207e-05  1.11017978e-05]]                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of L2  (3073, 10) 10.0\n",
      "Epoch 5, loss: 11.435123\n",
      "== W == 0.08292656472656296\n",
      "loss , grand (prediction) =  11.4075183645485 [[ 1.11328122e-05  1.12141167e-05  1.10700323e-05 ...  1.11675649e-05 \n",
      "                                                 1.11543414e-05  1.11259621e-05]                                    \n",
      "                                               [ 1.11947479e-05  1.10236702e-05  1.10185994e-05 ...  1.10686638e-05 \n",
      "                                                 1.11186672e-05  1.11707951e-05]                                    \n",
      "                                               [ 1.09868224e-05  1.11841283e-05  1.11226431e-05 ...  1.10929740e-05 \n",
      "                                                 1.10883757e-05  1.10644722e-05]                                    \n",
      "                                               ...                                                                  \n",
      "                                               [ 1.10433509e-05  1.11045627e-05  1.09429640e-05 ... -1.00045822e-04 \n",
      "                                                 1.12666345e-05  1.12375232e-05]                                    \n",
      "                                               [ 1.10858107e-05  1.11961418e-05  1.11967741e-05 ...  1.10772097e-05 \n",
      "                                                 1.09556075e-05  1.09701669e-05]                                    \n",
      "                                               [ 1.09260325e-05  1.13778891e-05  1.11104433e-05 ...  1.10189428e-05 \n",
      "                                                 1.10437568e-05  1.09574308e-05]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "Epoch 6, loss: 11.434537\n",
      "== W == 0.08171136216312969\n",
      "loss , grand (prediction) =  11.407480876351261 [[ 1.10409240e-05  1.10669286e-05 -9.99872432e-05 ...  1.11143983e-05 \n",
      "                                                   1.10508928e-05  1.11801248e-05]                                    \n",
      "                                                 [ 1.10576006e-05  1.11126474e-05  1.09098024e-05 ...  1.11477059e-05 \n",
      "                                                   1.11201387e-05  1.11546375e-05]                                    \n",
      "                                                 [ 1.09964612e-05  1.12815120e-05 -9.98667262e-05 ...  1.11794388e-05 \n",
      "                                                   1.09562142e-05  1.08456768e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11402756e-05  1.11892216e-05  1.11567175e-05 ...  1.11241610e-05 \n",
      "                                                   1.10174603e-05  1.09942404e-05]                                    \n",
      "                                                 [ 1.10543241e-05  1.12019674e-05 -1.00002812e-04 ...  1.11228515e-05 \n",
      "                                                   1.10813246e-05  1.10943379e-05]                                    \n",
      "                                                 [ 1.10516957e-05  1.10538877e-05  1.10498260e-05 ...  1.12083889e-05 \n",
      "                                                   1.13124010e-05  1.14522431e-05]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "Epoch 7, loss: 11.433961\n",
      "== W == 0.08051495625647649\n",
      "loss , grand (prediction) =  11.407443886000964 [[ 1.11506599e-05 -1.00084996e-04  1.10170782e-05 ...  1.11046689e-05 \n",
      "                                                   1.11683644e-05  1.12503890e-05]                                    \n",
      "                                                 [ 1.11581350e-05  1.10937901e-05  1.10875138e-05 ...  1.10760786e-05 \n",
      "                                                   1.11058976e-05  1.11458429e-05]                                    \n",
      "                                                 [ 1.11182332e-05  1.10924895e-05  1.10639846e-05 ...  1.11138355e-05 \n",
      "                                                   1.10750478e-05  1.11055344e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.10212993e-05  1.11269378e-05  1.11001905e-05 ... -1.00002840e-04 \n",
      "                                                   1.09989712e-05  1.09585582e-05]                                    \n",
      "                                                 [ 1.10508462e-05 -9.99029861e-05  1.11724828e-05 ...  1.11537128e-05 \n",
      "                                                   1.09510355e-05  1.09619715e-05]                                    \n",
      "                                                 [-1.00214452e-04  1.10361002e-05  1.10478580e-05 ...  1.11772444e-05 \n",
      "                                                   1.12179118e-05  1.12119880e-05]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "Epoch 8, loss: 11.433398\n",
      "== W == 0.0793370283119055\n",
      "loss , grand (prediction) =  11.407407385070854 [[ 1.10702666e-05  1.11971568e-05  1.11636696e-05 ...  1.10999624e-05 \n",
      "                                                   1.11058344e-05  1.11009816e-05]                                    \n",
      "                                                 [ 1.10479210e-05  1.11624742e-05  1.12331792e-05 ...  1.11586093e-05 \n",
      "                                                   1.09722493e-05  1.09375215e-05]                                    \n",
      "                                                 [ 1.10506132e-05  1.12461913e-05  1.11755781e-05 ...  1.12148988e-05 \n",
      "                                                   1.10233430e-05  1.09870967e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11242043e-05  1.11477851e-05 -1.00006813e-04 ...  1.11261791e-05 \n",
      "                                                   1.10840802e-05  1.10498091e-05]                                    \n",
      "                                                 [ 1.08572695e-05  1.10981383e-05 -1.00009329e-04 ...  1.11822890e-05 \n",
      "                                                   1.11962131e-05  1.09925162e-05]                                    \n",
      "                                                 [ 1.10682461e-05  1.12153333e-05  1.11915581e-05 ...  1.11230957e-05 \n",
      "                                                   1.10517395e-05  1.10177966e-05]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "Epoch 9, loss: 11.432845\n",
      "== W == 0.07817726573531526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'THERE WERE 10 EPOCHS   !!!!!\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function \n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "'''THERE WERE 10 EPOCHS   !!!!!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.438236907928653, 11.43758921489213, 11.436954237927331, 11.43633172400609, 11.435721425194641, 11.435123098549628, 11.434536506016233, 11.433961414328484, 11.43339759491159, 11.432844823786322]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21481668>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "print(loss_history)\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [ 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9,1e-10, 1e-11,1e-12,]\n",
    "reg_strengths = [1e-7, 1e-8, 1e-9,1e-10, 1e-11,]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "data:list = list()\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for l in learning_rates:\n",
    "    for r in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=100, learning_rate=l, batch_size=300, reg=r)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        data.append({'learn rate':l,'regul':r,'accuracy':accuracy})\n",
    "        \n",
    "\n",
    "\n",
    "frame = pd.DataFrame(data)\n",
    "print(frame)\n",
    "frame.plot()\n",
    "axes = plt.gca()\n",
    "axes.set_ylim(0,0.3)\n",
    "\n",
    "# print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      learn rate         regul  accuracy\n",
      "0   1.000000e-04  1.000000e-07     0.089\n",
      "1   1.000000e-04  1.000000e-08     0.112\n",
      "2   1.000000e-04  1.000000e-09     0.117\n",
      "3   1.000000e-04  1.000000e-10     0.081\n",
      "4   1.000000e-04  1.000000e-11     0.131\n",
      "5   1.000000e-05  1.000000e-07     0.089\n",
      "6   1.000000e-05  1.000000e-08     0.123\n",
      "7   1.000000e-05  1.000000e-09     0.078\n",
      "8   1.000000e-05  1.000000e-10     0.116\n",
      "9   1.000000e-05  1.000000e-11     0.102\n",
      "10  1.000000e-06  1.000000e-07     0.083\n",
      "11  1.000000e-06  1.000000e-08     0.088\n",
      "12  1.000000e-06  1.000000e-09     0.108\n",
      "13  1.000000e-06  1.000000e-10     0.107\n",
      "14  1.000000e-06  1.000000e-11     0.091\n",
      "15  1.000000e-07  1.000000e-07     0.112\n",
      "16  1.000000e-07  1.000000e-08     0.092\n",
      "17  1.000000e-07  1.000000e-09     0.113\n",
      "18  1.000000e-07  1.000000e-10     0.131\n",
      "19  1.000000e-07  1.000000e-11     0.094\n",
      "20  1.000000e-08  1.000000e-07     0.096\n",
      "21  1.000000e-08  1.000000e-08     0.110\n",
      "22  1.000000e-08  1.000000e-09     0.101\n",
      "23  1.000000e-08  1.000000e-10     0.133\n",
      "24  1.000000e-08  1.000000e-11     0.123\n",
      "25  1.000000e-09  1.000000e-07     0.084\n",
      "26  1.000000e-09  1.000000e-08     0.090\n",
      "27  1.000000e-09  1.000000e-09     0.131\n",
      "28  1.000000e-09  1.000000e-10     0.096\n",
      "29  1.000000e-09  1.000000e-11     0.091\n",
      "30  1.000000e-10  1.000000e-07     0.093\n",
      "31  1.000000e-10  1.000000e-08     0.094\n",
      "32  1.000000e-10  1.000000e-09     0.087\n",
      "33  1.000000e-10  1.000000e-10     0.075\n",
      "34  1.000000e-10  1.000000e-11     0.110\n",
      "35  1.000000e-11  1.000000e-07     0.087\n",
      "36  1.000000e-11  1.000000e-08     0.085\n",
      "37  1.000000e-11  1.000000e-09     0.111\n",
      "38  1.000000e-11  1.000000e-10     0.145\n",
      "39  1.000000e-11  1.000000e-11     0.093\n",
      "40  1.000000e-12  1.000000e-07     0.075\n",
      "41  1.000000e-12  1.000000e-08     0.089\n",
      "42  1.000000e-12  1.000000e-09     0.098\n",
      "43  1.000000e-12  1.000000e-10     0.090\n",
      "44  1.000000e-12  1.000000e-11     0.132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'accuracy')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOy9d5Qkd3nv/anOaXLYnZ3ZNLOzOUkbtJLANmAsIYwu+AUDvoCF7HttXideG7+O4ABOB78vF5vDccJHmGMjjK/96uKAjANXAml3NatdraRNMz2xJ4fOuarr/WP0K1X3dKgO0zO96u85e7Tq7eqq6q761vN7nuf7fSRVVWmiiSaaaKI+MG31ATTRRBNNvJHQJN0mmmiiiTqiSbpNNNFEE3VEk3SbaKKJJuqIJuk20UQTTdQRlhL/3mxtaKKJJpooH1Khf2hGuk000UQTdUSTdJtoookm6ogm6TbRRBNN1BFN0m2iiSaaqCOapNtEE000UUc0SbeJJppooo5okm4TTTTRRB3RJN0mmmiiiTqiSbpNNNFEE3VEk3SbaKKJJuqIJuk20UQTTdQRTdJtookmmqgjmqTbRBNNNFFHlHIZa6KJglBVlUwmQzKZRJZlLBYLJpMJs9mMyWTCZDIhSQXNlppo4g0JqcRgyqa1YxMboKoqiqIgy3LW38W/6YlWkLD40yTjJt4gKHiBN0m3CcPIJVtJkpAkCVmWkWUZk8m04f36P1NTU/T29uJyuZpk3MTdjoIXcjO90ERJqKqKLMv4fD5aW1txu90bCDYfBCkLpFIpAG1bWZZJp9NZ2zTJuIm7HU3SbaIgBNmK1EEwGMThcODxeCr6PEmSstIPuUQqVl2CjHPfazabtbyxIOcmGTfRaGiSbhMbkMlksvK0ImI1mUxkMpkN768V8ZUi49zUhqqqRSPjJiE3sR3RJN0mNGQyGWRZRlEUYGN6QBBdpah0e6NknLtNOBymq6urScZNbCs0SfcNDlHkSqfTWhRbiJhMJtOWkG6xz9P/V0Ds4/bt25w5c2bDNiI61qcqmmTcRL3QJN03KESPrSzLJclWQJKkvOkFo6g16Zbaj8gD6yHOW1EUrbAnoE9TiOi4ScZN1BpN0n2DIZdsBakYIZZ6keZmotC56sk4t9c4X8642VHRRKVoku4bBIV6bMshju2WXqglKiHjZntbE5WgSbp3OQTZrq2tEQ6H6e/vN9Rjmw+Nkl6oJYqRsciFp1IpJEnC6/UyNDTUJOMmiqJJuncpRI+tiNBkWSYcDld149eCNBuNdAshHxkHg8Gm8KOJkmiS7l2GXEGDqNZXmxoACvbpBoNBQqEQLS0tuFyuDcUrgXqTy1aQmVHhhx7i92kKP94YaJLuXYJCggaBQoRZDvTpBVVVWVtbw+v1YrVacbvdzMzMEIvFyGQyOBwO3G639sflctU1vVDviLrU/prCjyYEmqTb4CglaBCoJekuLS0xMTGB0+nk6NGjuFwuUqmUtrRWVZVEIkE0GiUajbK2tkYsFiORSGC32wmFQllkXCgyfiOgUuFHPkJuknFjoEm6DYhyBA0C1ZKuqqqEQiEWFhbo7u7mxIkTuFwugA2fK0kSTqcTp9NJd3e39vrExARWqxWHw5FFxoUi40Yi49zOhmpRiowzmYz24Ovp6dHe2xR+bH80SbeBUImgQaBS0s1kMiwsLDA5OYndbmfnzp0cPny47M8Rx2C1Wunu7s4i40KRcSaTwel0biDjSrsvNhO1Jt1C0JOxqqpapCuOoSn82P5okm4DoBpBg0C5pJvJZJibm2Nqaoru7m7OnDlDMBgkGAxWcgpA4e6HQpGxqqrE43GNjFdWVojFYqiquu3IuF6km7tP/TmX02usV+w1hR/1RZN0tzFEj+3S0hKSJNHR0VFxdGKUdBVFwefz4fP56O3t5dy5c9hsNqD+hjeSJOFyuXC5XNoSGoyRscvlQlEUMplMXch4K0jX6Lk1hR/bC03S3YbIVY9Fo1EkSaKzs7PizyxFurIsMzMzw+zsLH19fZw/fx6r1Wr4M+p5Mxoh40gkQjKZ5MqVK1lk7PF4cLvdOJ3OmpLxdibdQihH+CEgiqFOp7NJxhWiSbrbCLmCBn1hJLe3s1wUIsx0Os309DTz8/MMDAxw4cIFLJb8l8VWWTuW8/mCjLu6ulhbW+PMmTNkMhktZxyJRFhaWiIejwNsSFNUSsaNSLqFUIyM5+bm6Ozs3PDvzcjYOJqkuw1QSNAgUKt2Lz1SqRRTU1MsLS2xe/du7r///pLdAsUEFkZIZ6v6dE0mU97IOJPJZKUpqiHju4l0C0GSJBRFwWKxZF0rRoUfTTJeR5N0txClBA0CZrNZ68OtFslkkomJCdbW1tizZw/333+/4Ru3mPeC0ZtoO8mATSaTRqp6GCFjj8eDy+XSyPiNQLpin7kP52qEH/rWtjdKR0WTdLcARgUNArWIdOPxOPF4nCtXrrBv3z4OHTpU9sVdrPugmu23G4yQcTgcZnFxUSNjm81GIpFgeXlZi4w3mzy2gnQVRTHcP12N8CNfZHy3kHGTdOuESgQNAtVEurFYjPHxccLhMBaLhfvvv7/ii/dutnY0Aj0Z9/b2aq9nMhlWV1eZnp4mHA6zsLBAPB7Xcsy5aYpakcd2J91CMCL8SCaT2uvpdJpgMMiOHTvuCuFHk3Q3GdUIGgQqiXQjkQjj4+PE43EGBwc5duwYzz//fNUuY/mOI5VKsbq6itvtxuFwFNxHo5NuIZhMJq3PeHBwUHs9k8kQi8W0yLjWZNyopFsIhcg4Ho+zurpKT09PSeGHCGpaW1s35RhrgSbpbhJEcSwUCmlmL5U+lcuJdEOhEF6vF1mWGRwczFtprhS5pJlMJpmcnGRlZYX29nbm5+eJx+NZEaHIf4pe37sV+XK6JpMJj8ezYWS9ETIWrW3FHmJCKFNPbAXRy7KsRbe50PcaAzz99NO89NJL/O7v/m5dj7EcNEm3xtD32KbTaV599VXOnz9f1c1hJNINBAJ4vV4ABgcH6ejoqHh/xY5DVdWsYty+ffsYHh4mnU5r56goCrFYjEgkgt/vx+fzkUwmyWQy2s0jCPluIeNcdVgxlCLjSCRCMBhkbm6ORCKhdV/oH2QOh0P7rHqj3kRfLLrODWSCwSBtbW31OrSK0CTdGiHfOByr1YqiKFVfpIUiXVVV8fv9eL1eLBYLBw4cKHnBVVNlT6VShEIhrly5wv79+7ViXG7KwGw209LSQktLS9br8/PzmtH38vIyk5OTpNNpzRpSH+EV6hU2inp3E9QibVKIjMVDLBqNZpFxLBbjxo0bG8i40XKcpSAiXSMIBoO0t7dv8hFVhybpVolCgoZaIjfSVVWVlZUVxsfHcTgcHD58eAPBFfuccnNyiUSC8fFx/H5/VcU4i8WC3W5n165dWa+nUimtPWthYYFIJIKiKNjt9ixCcbvd29Z5bDOX+oUeYpcvX2b37t0byFhExuIB5na7sdvtNTm+rcjJl0u6e/fu3eQjqg5N0q0QpQQNtYSIdFVV1bxs3W43x48f39DWVAzlkm48HmdiYoJgMMj+/fsZHh7m6tWrNScXm82GzWbLSomoqkoqlSISiRCNRpmdnSUajWo2kHpC2WqzG4GtiDDzkXFuemd2dlYj49yHWK3IeDMhyzJ2u93Qe5uR7l0Io4KGWiOVSnHx4kXa2to4deoUTqez7M8w2j0Qj8cZHx8nFAoxODjIkSNHNDVSvQZTSpKE3W7HbrfT1dWlva63gYxEIprZDWQLFxwOR12jsq0QRxRCochYURRtRaHPtRsl460o3InjNhrphkKhJuneLShX0FCrfc7PzzM1NYWiKNxzzz1aAaUSlCrIiZ7eSCTC4OAgR48ezTrH7eC9UMgGUi9ciEQizM/PE4lEGBkZySpCeTyeTYnutoJ0y92f2WymtbV1QztVITI2m81Z353dbt+SFUU56YVQKNQspDUyqhE0CIje1nIu1kwmg8/nY2Zmhp6eHs6ePcvIyEhVhAuFSTeXbI8dO5b3HGtBmpsVfeaqyGRZ5vr165w+fbpgR4C+cCc6KSolzkYs3AkUImNZlrUCnt/vJxwOE4lEuHLlyobIuJrvrhTKzeluRudOLdEk3TyohaBBQORjjZCuoiiaveKOHTvy2itWg1zSjUajjI+PE4vFipKtQLU31VYsTQt1BAhCiUQimposlUphsVg2kLGR32ArSHez92exWLLIWDycDx8+nPe7M5vNm0LGsiwbrkM0c7oNBtH2FYlEWF5eZvfu3VWnEQTpFrtxZVlmenqaubk5+vv7ue+++6pumcoHQbrRaBSv10s8HmdoaIiurq66EMZ2UqTlEopAOp3OMrqJRCLIsozNZstKUbhcrqzfqN6kuxX5VdEvW+i7k2VZ++6KkbHH48FqtRo+/nJyuqXute2AJumyscdWURTNhataFFOTpdNppqamWFxcZGBgoKS9YrU3tqIo3LlzB0VR6kq2AvW2dqzk3KxWK+3t7VnRkkgxiU6Kubk5otEoiqJoAzVlWcZkMm2qTFaPcsQYtUKpc7NYLLS1tW3IqRYiY7GqyI2Mc2E0vbBdHuil8IYm3XyCBjE8Mdf9qFLkI129fNaovaIgrEqIJBKJ4PV6CQQC7Nu3j717926bSnsjQJIkbDYbnZ2dWdM7hDovGo1qEuirV69mDdTcrEkVW+W7UMk+C5FxOp3W0hQrKyuaWCaXjI1G9YJ0t/u1/YYk3VI9thaLZVNIN5FIMDExgd/vZ+/evQwPDxu+iMvJDQuEw2G8Xi/pdJqhoSGtv7XWF2U6nWZxcVGL+got77ZTeqEWkCQJh8OBw+HQZM4DAwMbxgbp/XjL8VYohu3ipVsNrFZrQTIWkfHy8jKJRILLly+XjIwTiQQul6tmx7dZeEORbj6yzXfB18K/VsBisWhL0lAoxP79+zl8+HDZN1o5xxQOhxkbG0OWZYaGhrTobHV1tWbnBdnpke7uboLBINFoVMuBCmIROdC7jXT10J+XMK7JN6lCdAOEQqGSBkHFrpG7zWFMj9wUTzAY5Ny5cxvIWB8Z37p1C6/Xi9lsZnV1Nau3uxC++c1v8nM/93MoisKP//iP88u//MtZ//7MM8/w8Y9/nOvXr/Pkk0/y3ve+N+vfQ6EQR44c4T3veQ9f+MIXDJ/fG4J0yxU01CoSFBdHJpPh0KFDG/pey4ERpzHhMCZytrmtM7V6mMiyzNTUFAsLC+zevZsLFy5kHVuummxmZoZYLKZ1g0xMTGQtu7f7ctAoSp1HKW+FXIMgUYDKbWuDu5t09dCnFvLl2wFtRNCtW7dYXV3lfe97H2tra3z605/mXe96V97PVRSFn/qpn+Jb3/oWAwMDnDt3jkcffZSjR49q79mzZw9PPPEEf/iHf5j3Mz75yU/yvd/7vWWf011NulshaIDXl/WpVIr29nba2tqyTK8rQTHCDAaDeL1eVFVlaGioYMtMtaQryzLJZJJLly5pc9WE85gsy1l+qPnUZKFQiKmpKdxud5aloT7SEwSzmX2fm4FqCluFFGT6AlSuQZDVaiWVShEMBmtiEGQEiqLU3RXOSBHNarVy7733kkqliMVi/Omf/ilA0Wv98uXLHDhwQPM//sAHPsBTTz2VRbr79u0D8ju5XblyhcXFRR5++GFGRkbKOqe7jnRrIWioFIL8MpmM5mU7MzNTk+gyX6QbDAYZGxsDMOQwVsiEvBRES9v8/DwAFy5cqCjiEXOxent7sx5CekVUbnVbn6KoF7lUgs1oGStUgEqlUiwsLLCyslJXg6BKC2nVoJwe3UAgkBVwFDvW2dlZdu/erf3/wMAAly5dMrSfTCbDL/zCL/CVr3yFf//3fze0jR7b8wquALUUNIhtjS7hhL2iyWRicHAw64c3m81Zo0cqhT5KFd65kiQZItt8n2EEiqJk9Q9fuHCBS5cuFbwJShFPoZxuIUWUvk1rfn5+Q5uWPl+81YY39ezTFT3DokAq9r/ZBkG1LqQZQTk9uuUII6qZ9ffFL36RRx55JIu0y0HDk65o+1IUhZdeeokTJ07UJLIVHQyFllOqqrK2tobX68Vms3Ho0KG89oq1muRrNpu1SNpsNjM8PFz2SBLRR1oKemXcrl27NkS2uQQjvm8jRbJyCmlWq5WOjo4N7mN6w5vV1VXN8EbfGVDvZXC9c6y5+6vWIMhIfn0rcrrl+i4YJd2BgQFmZma0//f5fBssRwvh+eef59lnn+WLX/wikUiEVCqFx+Ph93//9w1t37Ckm6/HNh6P16xpvBDpqqrK8vIyExMTOJ1Ojh49uqEwokctSNfv97OwsIDdbufo0aMVz38ymUxa0SEfFEXB5/Ph8/no6+vLq4yrpl+4FpFgMcMbvdF3OBzWDNfz5Ytrje2qSDNqEKTPrxcyCNrupBsMBunr6zP03nPnzjE6OsrExAT9/f08+eST/M3f/I2hbf/6r/9a+/sTTzzByMiIYcKFBiTdQoIGWI+M0ul0TS4Mi8WyoSK/uLjIxMQEra2tnDhxwlBPYDWkKyJpq9VKb28v7e3tVQ3cK5Re0Bvs7Ny5s6gMWXxGJQ+2zWwZ03cG7Nixg1QqxY0bNzh+/HjBYlRuvria62YrSLea4KLYmPlCBkGpVAqbzUY6na5bsXOzpkZYLBa+8IUv8NBDD6EoCo8//jjHjh3jU5/6FGfPnuXRRx/lhRde4D3veQ9+v59vfOMb/MZv/AavvvpqNaezvu+qP6HOUBRFm8eVe9HVUkkmIt1MJsPCwgKTk5N0dHSUba+YS96loE9b2O12jhw5gsfjYXJysuqCXC7plkO2AtUQ51b06RYrRuXLf1aqJNsKw5vNSGcUMwh66aWXsFqtNTEIMopyoutyzW4eeeQRHnnkkazXfvu3f1v7+7lz5/D5fEU/47HHHuOxxx4zvE9oQNIV8+7zwWKxFF0+l7ufhYUFbty4QXd3N2fOnDHsXq+H2Ww29CBQVZXV1VXGx8e1NIL+wq9FmkKQbiaTYXZ2lunp6bLdzO4WgUMhWW8hJVluiiLXk7fRIt1yYbFYkCSJvr6+rGulUoMgo5Bl2bDKrBEcxqABSbcYaiHfFXnN2dlZOjo6OHfuXFU5wFJkKcjW6/XidDo5duxY3hE8pfKxRhEKhXj++efp7e2tyDqyml7f7U7YhZRk+sGQevGCXpaaSCRqUjA1CjFZuZ7I171QqUGQvpOiWCRbbiFtu3vpwl1GuiKnWxSqiiR/B0mZQTUPoFreDJKELMvMzMwwNzfHzp072b9/PxaLpeqiS7FJvmK4pMvlKjnvzGw2k0gkKjoGMYFCTA2u5kHSSOmFWkWehcQL+igvGo0yMTHBxMSE1i+rX3LXOirdCkWa0ZSGEYOgSCTC2toasVisaFpns1rGthINR7rFbiQjka4p/j8wpf8ZVBmwIFt+AO/iDzE/P8/AwICW15ybmyOVSlV9vLmkK7ofxsfH8Xg8hgtylUSYqqoyPz/P5OQkXV1dHDt2jLm5uaoeJEKBpkcsFmNiYmJdturxMJWKs5hK0u5wcKa3D5dlPZre7pFuudBHedFolB07dtDa2qrli3OJJdfsphoJ9FaQbrXQGwTltrUVSuuI+k0qlSppEJROpytKAdYbDUe6xWC1WrUexLzILGBK/TPgREUilU5C8hs4bW/Z4GVrNBdbCuICySXbkydPluWIVE5ON5dsz549i81mIxKJVF2M06va4vE4Xq+XSCTCnj17yGQyPDMzyZWVRWyqSkpVecF1m3fvO0hHa2tdb4h6k7vI6Rbql9W3aBWTQBvtMd6qSHczUMwg6OrVq7hcrqIGQU6nsyHIVqDhSLdUpFs0vaDGUDGTSqc1h3mr1cWu7nbIySvVyt5R5LcuXrxIa2trxZN8jUS6qqqysLDAxMQEnZ2dG4p/tRoMmUwmmZmZIRAIMDQ0xLFjx5BlmZQsMzc3yan9g5gkCTWTYSqwhl9OIS8vEw6HCYfDXLt2TSOY7aIoqxal+mb1RFFMAj01NaU5Z4nvR2ynX2Y3YqRbLsT59fX1ZQVEuQZB3/jGN/j85z9PKBTiYx/7GMePH+etb31rlo9CLip1GJuamuKHfuiHtC6qn/mZn+Enf/InyzqvhiNdKEwexYgyHo8zORFiX7sFlz2KzepGIoFq6gbTRjlftaSr7+uVZZnz589XRLYCxSJd/b7a29sLdlpUa3iTSqUIhUIEg0GGh4c3WFSKv6mqCpKEZDJhs9vp7d3B7pZWFEXh6tWrHDlyRFt+5yrK9L2zmzG1d7NQafdCIQl0KpXKmmycW4iKRqOah289yHerRsznO7/cHPuBAwd4//vfzyOPPMJjjz3GK6+8wvz8fEHSrcZhrK+vj+eeew673U4kEuH48eM8+uijhtRskiRJqqqqDUm6hZCvkKafdLt//34c3X+GKf77SMokqvkIivOXQdpIUJW2aIloc3Jykra2Nu655x5efPHFqggX8hOmqqosLS0xPj6u7atYD3GlpJtOp5mcnGRpaQmbzcbw8HDeKrHZZOLsjj4uzvtwW20kZJkdbjc7XdkFwkLLb31T/uzsrNYh0AimN7UmJZvNhs1mKyiBXl1dZX5+nunpaaB25uiFsJWRtZHzCAaDdHZ2cuHCBS5cuFD0vdU4jOlTP+KhZxTqa5Hi9rt6q4A+Oo1EIoyPjxOPxzdMulU8f1TWZxmBfmmfK6KoRjoroH8IiPyw1+s1RLYC5ZKu3jdXjBW6detW0W0u7BygzebAFwnRbrdzqnsn1teWhqWW3/ma8ouZ3uhTFFvty1uPSFAv6V1YWGD//v3aOJt8KrJcP14xELISbIUEuByU07lQjcMYwMzMDO985zsZGxvjs5/9rNEodydwL/AvDUm6hdILVquVRCLB1atXkWVZs1es5GYwSrr6olW+PCq8TpjVRGiifUZEti0tLZw+fbqsCNoo6QrDG5/Px8DAQJbhTam8sEmSONbVw7Gung3/VklOuZjpjUhRLC0tEYvFtLypKEbVckpGKWylIq2YikykKIRfiF64oM8XlyLUrSDdcq6VUChk2G2vGocxgN27d3P9+nXm5uZ497vfzXvf+1527NhRarMu4EPAwYYk3XwQdoeJRIJjx45V3SRdKr0gel+npqbo6uoqqlgTnRCVkq6qqvj9fgKBAE6ns+zOB4FSpKuXBff19XHhwoUNx1ytOKIW0Ed8uSIGfQ9oJBLh8uXLWSQjiKbWS+XtqEjLJ4EWFpDieyomgRYjlmBrvHTLSWkEAgHDpFuNw5geu3bt4tixYzz77LMbRvnkwS3gj4APNSTp6tuwhJetxWLhwIEDvPrqqzVRpRS6gTKZDHNzc0xPT2e1YxVDNflhoVYTLTXHjx8v+3MESp3T1NQUO3bsKOrBsJ17bfVFqc7OTtLpNCdPnszqm52ZmSEajaKqqla4q0UedDuSbj7oW9pKSaBjsRiSJGl59HQ6TTKZrNtUj80yu6nGYczn89HV1YXT6cTv9/Pd736Xn//5ny+5naqqCnARuNiQpKvveXU4HBw+fDivl20toSemnp4eQ2QrUC7pCtObsbExnE6nplZ77rnnKj38gvsReeju7m5DSrVCpLuduwzyqaNE32y+PKieiD0ej6Ebv1FItxBKSaCXlpZIp9PcunUrSwJd7vdUDsoh3dypEcVQjcPYzZs3+YVf+AXtPvjEJz7BiRMnSu5TkqRjwFuA1oYkXZ/Px+rqal7pbDkTH4xAbw7T29tbkYS2HNIVZOtwOEpKgyuFvuuhWItZPtRyUvJWQt83q8/HybKsFe4WFxe1QZ+iVUtfuNNfY41OuoUg2rOSySSSJGkVfyGBjkQiWd9TLSXQ5fouHDhwwPBnV+ow9va3v53r168b3o8OB4EHgesNSbp79uwpmIMR9o7VeiZkMhlSqVRV5jACRkhXP4WilDF6pRADJC9dukRLS0vZNpWwvdMLtYDFYslr4KKfvrC8vJzlPubxeEilUppPb71QT5LPLaQVMrrJ9VaIRqNA9pQKj8djKJWzmbaOW4AJ4L+pqhppSNItBpF7qpR09cUkSZK45557Kipa6VGMdP1+P2NjY1itVs07dzOwurrK2NgY6XSas2fPVnxOhUhXDATdzmmGSlFo+oJeGZVOp7l9+/amGKRvBxghwELeCqUk0PrvSn/fbtaoni3CDwHfBv6jIUm3WtObfNCPqhGG3tevX68JieQj3UAgwNjYGGazeVNz0oFAgNHRUaxWK8eOHeP69etVPUTyGaHPzc0xOTkJoBHOZnYKbBfolVGzs7OcOnVKm7CQzyBdL2AwGu1tF1STziglgRbKRCGBtlqtuN1uZFnWVq6lyDcYDBruXtgi/ADwVbjLxBFg0N5RB/0Qxty5YLXyX9Cb54ix6SaTiYMHD5Y9fsdoNBkKhRgdHUWSpJqSuoh0c4twZ86cQXrNIlPfKRCLxbI6BWRZrksFfCv6ZsX+CtkaCk/ecDjM/Pz8BgGDeEjVM0VhFMKrpJYoJYGemZkhkUjw0ksvlcyrB4PB7e6lOwm8SZKkmYYk3VpEumK8uJh4m69Nqlaka7FYCAaDXLlyBUmSKprkC69HmcWWeZFIhNHRURRFYXh4OO/TvxpCkiSJcDjMxYsXaWtr04pwsixrufR8nQJiGa4oCjdv3iSVSr2homLRepUb7ekFDPrJC3a7PWvZvdWmQPUURwgJdCAQwOPx0NPTUzCvvra2xl/8xV+wsrLCpUuXOHv2LLt37y56fVdqdnPt2jU+9rGPEQqFMJvN/Nqv/Rrvf//7jZ7W54GfB3Y3JOkWQ6k5abIsMzr9Mtdif4vqitI3fJC+7hNYzBu/ilqMyAmFQtqy6dSpU1UtgYqRbjQa1cQhhbwRSn1GKfj9fiYnJ7FYLJw6dcpwmkKvmJqZmeH06dMAeftngaz+WZHna4RleKXKx3wChmQyWdQUqF4rBoGtngRcKK+eSqXo6uriZ3/2ZxkZGeGJJ57g4MGDG4xqBKoxu3G5XPzVX/0Vw8PDzM3NcebMGR566CFDuWRVVZ+XJOn/Bn7wriNdi8VCMpnc8Losy0xPT+Obn8K38x9IuENgMjMRHyG6tMIjOz+JSaqdvWM4HGZsbAxFUejr6yOZTFadc0UmkLYAACAASURBVBIPAf0yT3jaRqNRDhw4UFL2XAnpilSFyWSiv79f6+nUo5Ibv1RU7Pf7mZmZeUNGxaIglW/MfDgczlox6G0gN6twt9WkWwg2m40zZ85gNpv5nd/5nZLXYTVmNwcPHtT+vmvXLnp7e1leXjZEupIkWYA9wLcbknSLfbFWq1WLmCDbtKW/v5+D9/RzY3GNhJIio6ioQEwZIyKv0GrN1k9XQrp6sh0aGqKjowO/38/8/HxZn5MP+iJWIpFgfHycYDDI0NAQPT09hoivnD7baDTK6OgosixrqYqFhYXiRvFVopCPQG5UnJsrbrSouBKI78blcjE7O6utGIQpUD5Zrz5FUY0p0FbIgGVZLovojZxbtWY3ApcvXyaVSjE0NGTkuKzAx4FTwCMNSbpQ3FM3nU6TTqeZnp5mYWEhy7RlIjpCXEkiqxlUJEAlpsZ5IfBvvK3nv+b9LCOIRCJaS9aBAweylve1SFOIz0kkEkxPT7O2tsbg4CBHjhwp60YyQrqJRIKxsTEteta3/+gnR9QT5UbFDoeDRCJBOBy+66Li3E6CQqZAelnv4uLihjYt8cdIgazSlFQ1MGoSVU5/dC3UlPPz83z4wx/my1/+stHrqhf4YeADwL6GJd1i8Pv9XL58md27d3P//fdnfTF2UwsqsP7Vv/4DXA9+hwc7343D/LoCzMgwyEgkgtfrJZlMasv7XNSCdNPpNOFwmFdffZWhoSEOHTpUUdRSjHRTqRTj4+P4/f6C0bMRcUQ4lmQ1GMVsNtHX1YrFvDmEVywqXltbIxQK3ZVRsZH2rWKy3kLOY8WmeWzX9AKsdy4YLUxXa3YTCoV45zvfyWc+85mSvr06dABBYAlwNizp5t78wmh7YWEBSZJ44IEH8l6YnbYBMpgAQTwSKhIZVNJqEgevk26x9IK+cDU0NJQVDeaiGtLVp0esViuDg4NZeb5ykY90ZVlmcnKSxcVF9u/fX5TQS5HuciDKv4+MoWRUUDP0drbwlnuHNo1488Fms9HW1obL5dJydXdTrriantl8bVrCeaxQ4U6MmBd9s/V6UBmdPlwvs5tUKsV73vMePvKRj/C+973P0DavIQT8A2AGnmtY0hVIpVJMTk6yvLzMnj17uO+++7h69WrBH8thdtNq2UcgPQ6ooJpIqxa6HT14zNk/XD7SjcVieL1eYrGYRralLkKLxVI26epb2kTEPjo6WtZn5IOedPU9yvlWBaW2z4erd2ZxWC14XOteDgurYeaWQ+zZWV+1UG5b3N2UK94Ms5tS0zxkWWZ0dHSD2c12mOZRjjCiGrObv/3bv+WZZ55hdXWVJ554AoAnnnhCy60XQQZ4WVXVgCRJn2tY0k2n03i9XpaXl9m7d69GGKqqliS4d+38Sf5+7o8Jy34UVaHLtoMP9H8CScq+kPWkK8b+RKNRw2QrUE6kq5ch79q1K2tKcS3SFMIMfWZmhunpac031+jSsVSkm0jK2KyvX1Zmk0S6Br3OmwUjuWKfz0cymdwQFbtcri2R99bL7Ca31e/UqVNAttlNoWkeuX68m4lyHMagcrObD33oQ3zoQx+q5BDvA44Cz6iqOt6wpDs3N4fb7ebAgQNZF6CRH7nPsZ8f3fMpllOzOEwudtr35d3OYrGQSqV45ZVXiEQiDA0N0d3dXfaFZCQPqreOFDLkWhqIw+uqqFdeeYWdO3dWZOJT6lz27uzgO1fHQQFVUvF4HHS11d4pbTNRKioWail9VJxMJlldXa1LVFxq8vBmo5DZTalpHvn8FQqhHFOlBlCjJYBdkiQNAsGGJd19+/ZVFfV5LO14LIWfjolEgtHRUYLBIHv37s2asVZL6Mf9dHd3FyXCaszQV1ZWGBsbQ1VVhoeH6evrq+h4Cw3IXFxcJBQKoSQyxMIJEikFSZJoddix5uRzG9UYp5gvbzAYJBAI1CUqVlV12xnolJrmkTtivtT3U67ZTSUKzzpCAs4BnwMuNyzplrppK72x9f2vg4ODhEKhLNlmrSCIanx8nM7OTkOm6JVEun6/n9HRUZxOJ6dPn8bn81V1w+ZGuisrK4yOjtLW1obH4+H5l0Zpsyl0ONZbjCLBIC/fGOfYoYFN8QbeaohIzmq1ZvVsFouKq80Vb3WkWw6K+Svk5tKFKZC+jc3IfRwMBtmzZ8+mnUMNcBv4bcAO9DQs6RaDyMWWs3ROJBJMTEwQCATYv3+/1v/q9XorOoZURiYqJ2i1OjHrlG76qLOtrY17773XsKet2WzOq7bLB72KTO/PW22KQpBuMBjkzp072O12Tp06hcPhIJVK0dsbJNWu4HLaUBQZ3/wq8ms5ZBHx3Lhxg5aWliziudtQLCoOh8NVRcX1Hoe+GSRfapqH3+8nFovxwgsvaNM89MU7fRQcCAQ4efJkTY+vxrADfmAViDYs6ZZSpRltmE4mk1pv6uDgIIcPH676ArviH+VLk99CVhVarS4+fuC/AOtRoZh3Vo53gYARwsynIiv3M4ohmUyytrZGOp3m0KFDWgQjPvPo0A6eHZkgLSsoSoaO9jbuOXkQt3OdWEdGRhgYGCAWi2UtN4XBi554GiWaM5p/1Fsc6qEfFOnz+bJmuLndbu0BJaLiepNuvdRo+u/HbrdrTnx6U6DcaR5f+9rX8Pl8nDx50lBKolKzG4CHH36Yixcv8qY3vYl//Md/LOfUvgd4AEgCW9jnsYkwIt9NJpO86r3Fsn+V4/sP14RsAZaTQf588mlsJgtuk52IHOf/ufk/eSQ2iM/nq2oET7GcbjEVmR6Vkm4ikcDr9RIMBnE6nZw5cybv+/p3tvN99w3hmw9gtZoZ2tutEa7Yv9vtLmrwsry8rBVhBAm3tLRseWtSIVSboxauWvpikD7q00fFFosFk8mE2WwmHA7XpYNiK9RoegItZAoUj8c5deoU165d46tf/Sqf//zn2bt3L3//93+f9zOrMbsB+MVf/EVisRh/+qd/Wu7pPMW6gbkZsG6/K7gGKEa6qVQK7/g4/7ByidvWFaxOC5eDi3y89920WJx5tynnplpI+AGwm6zIskImLrOixjC77Rw5csTwLLJ8yEeYRlRkpT6jGNLpNBMTE6ysrDA4OMjg4CA3b94sus3OnlZ29hgvbBQyeFEURSPihYUFzRpSeAqICNBut28453oW6zZjX4VmuKXTaaampojFYllRsfhOxPdSyw6KrVKjFdunUNz98A//MF//+tf5sz/7M3bv3l200FyN2Q3A2972Nr797W+XdR6SJEmqqs4Cs+K1hiVdI+kFPYSIYmVlhUAPjDn8dFg9SEhMxBb56+n/5CcHH9nwWULYYDTC6rB5SCsKwVgISZIw2620mu20ZtxV99jqI91yVGR6mEwmQ34SQpwxNzfHnj17uHDhAiaTiWQyWZK0FSVDNJJEksDtcWAyvX5cxVrOIpEEspzB7bZjtZoxm80FIxwxxXd2dlaLAPV54nqingRvtVq1LoH+/n5g42Rj/XdSC/exrSDdcu65UCikrRKKHWetzG7KgaqqqiRJvwPIwDyw3LCkWwz6SFfIg5eWlti7dy8XLlzg7+a+CyEwvSaGcJnteKMLeT9LTH0wcgFEo1FWx2Y5ne7jqm0Om8WKCvzkvodhOlwzYcPExARzc3OGVWR6lOqzVVVVG7+TTzhRbHtJkkinZC4/P87qSgRQ6evv4N5z+zC/1jaWb3tVVbn+so87o4tIkoTHbedNbxrG4964KtB7Cui7SoTTVjgcZmZmhnA4TDKZ5MaNGxu6BWqNrZgErL8ei0XFue5jlUTFWxXpGi0wx2IxnM78q1Q9amF2UyFMwDDrIom2hiXdUpGu6LMVZKsnpx329f5ccbPElRRD7vx9q0byw3pp8IEDB/iZrlPMxVcJpKPsdHTQaWvhxtyNqqZQZDIZlpeXWVpawuPxlKUi06NQekFVVZaXl/F6vXR2dhYcNS9Uf4UwemeR1ZUI3T0tqKrK7Iyf7p4W9g+t927m+92WlsLcur1AT3cLJpNEIBDj2rVp3vTgsOHzynXaikajTExMsGfPHsLh8KYW7baCdI3sL5/7WCVR8XY2uxHXopHAo1qzm0qhquqv6P+/YUkX8kdNsiyzsrLC6uoqw8PDeSPBB7uOci04zsuhKUxIdNg8fHjPW/Puoxjp6nt6Dxw4kKVW2+XsYpfz9WJWNcIGMYtMqIBETqoS5CPd3F7eYlFDMWtHVVUJBeM4XTbtvQ6HlXAovuF9esTjKcwmk5aG8HjsBALZ21QCSZI2pBuEuUs4HK5Z0W4rSLdSEqwkKhbnlkgk8ubPNwPlEr2RY6rG7KZSSJJkB74A/F+sdy9kGpp09dC7cXV2dtLX15eVv9HDYjLzM0PvYia+QjojM+DsxmHOv+zMR7rJZJKJiYmyPG3LJV19P297e7s2+PGll14y/Bn5oCfdcDismejoe3mLoVR6obPTjW96jWh43RIznZZpa99VdHu3x04mo6IoGcxmE6FQgl39m2OQozd3acSiHWxO32yxqNjn8xGLxbh9+3ZNc8XFYDTSLcfovBqzG4A3v/nN3Lp1i0gkwsDAAF/60pd46KGHSu22C3hAVdWIJElmVVWVhiddMYZnfn6egYEB7r//fiKRCFNTU0W3M0km9rpKK830DmH6Sv6+ffvK8rQth3QLRZ6yLFdtIC5GhL/88svE4/Gi89TyoRDpiu9hR18bgW/dYHk5jAT09LbQ2ekuun1PdwunTu3mlVdmUVHp7vRw+mT+B+ZmoZqinclkqvvk4Xr2zYrzFIU7I7niQg8oozBKuuVKgCs1uwF49tlnDe9HBxuwJknSflVVJ6DB0wszMzNMTU3R39+fleOs1RRfeF0F5vV6WVhY0Ipx5V70Rki3kIqsnM8ohlQqxczMDKurqxw/ftzwiB89Cr1fURTC4TDT3jV29bXRt7ON6bEl1uaC/MvXR3j0R+7D4SpcxDp0cAf793UhKxmcjuo9W8sxTCmE7Vi0g/or0hRFyTqXWuWKi6EcA/NyHMa2AEvA3wOfliTpGcDV0KTb1taWt6BUaiKwUSiKQiAQ0Hpgy+0U0MNsNhds1SqlIhOoZr6VMHjfsWMHkiTVzE9CVVVmZ2eZnJzE7XZz66aP5YUIgcUEbo8Tu9NMYC3CtYvjXHjr4aLpCZvNQi1parOiz60s2sHWkG4poqymgyJfVGw0p1uOl+5WQFXVGPA5SZL+O/AgEG9o0m1vb88b+ZUz2ywfMpkMMzMz+Hw+WlpaGBgYqNpQI9/oH6MqskqRyWSYnZ1lenqa/v5+7r//fq3Tolroc86dnZ2cP38eVVXp6ezn3566RmxtFQmJSDiKw5PhxRdewdGRIJlat0AUgohGkfoWg1ju16NoB1tDupXur9KoWFEUQwXD7U66kiQdBfpUVf0z4M+gwdMLhVDpjawnKeFp6/f78fv9VR+T6PeF8lVk5ULvYNbT05NlF1mt9wKs34QjIyPY7XYt5ywIprevnTMPDrM8H8bT4uDA4QFcHjtySuHQ4UPcvHmTWCzG6OgoiURCu9FEjrQRxuXkolAhbTOKdrA1hje1LJQZiYrT6TRXr14tGRU3QHrhLcBJ4N8lSXKrqtq4hjdQu+Wj8LSdmJigt7c3i6QK5YfFEtnoMYjoe3R0lOXl5bILcUaxurrK6OgoLS0tnDlzZoPsuBrS1ZPlqVOnChYwDp3oB1Vl7NU5UCERS3P+ew/idDpxOBz09/drEWFufjQajQJkGb20tLRsS88FgXK7F6op2rnd7m2ZXqgF9FHxwsICZ8+e1Yz3830v//Zv/4bP56Ojo4N4PF5SIFGN2c2Xv/xlPvOZzwDw67/+6/zoj/6o0dPqAeYAVFWNwl0a6QqUuhlED+yrY6N0FxAESCYTtwJ+InOzDHd04rJZ+aNXn+N/L0zgMFv46MEzvHP34aLHoSgKi4uLLCwscPDgwYoKcaUQCoW4c+cOFouFEydOFDTVqYR09ZH58PAw0Wi0ZMX40MkB+nZ3kkrJeFocWhEtN6ebb/mpKAqxWIxwOMzy8jLj4+NlRYL1Ri1axowW7aLRqPbwEx7Gm22PWW9xhL4lTpKkglFxMBjk9u3beL1e3vKWt5BOp3nuuefy+ptUY3aztrbGb/3WbzEyMoIkSZw5c4ZHH33UaNfPC8CPSpL0IeBVGlmRVgqi0p8vQhLqq5ujo/yv8BJ3klFMAR/vyMT578fPYnrtB08rCv/j6ggjvhlaFuexW8wM93XwnZUJ2u0OlIzKF29cpM/Zyr3dG5Ut+nRFd3c3XV1dNTFb1t/k4gZMpVIcPHiwZH6rHNLV+y/oI/M7d+4Y2r61Y6N1pZHRRWazmZaWFlpaWrTXCkWCwotWEHHu6PB6YDP7dPM9lK5cucLu3buJx+N1scesN+ka6VywWq285S1v4bvf/S4/8iM/wrvf/e6ix1mN2c3TTz/N29/+ds379+1vfzvf/OY3+eAHP1jyXFRV/SdJklzAfwFOAOcbmnSLXVAiLaD/8VRVZXV1Fa/Xu15pd1sYXY3R43STQeWfJu6wr6Wdh/ety0+fn/XxyuoKbWYLHS4XgUSCb46NsbPHiVkyYTZDWFa57p/PIl29ikzkVBVF0Zqsq4EgTVmW8Xq9hEIhTQ1XzvbFoPdf2LVrV8WS43yolAQKRYJiAoHoGojFYlouNZlMEgwGN90Sst7iCFVVaW1t3TCjbDOKdrA9SVcgFAppgcZmmd3k23Z2drbIFq9DkqQuIA4EWZ+V9g8NTbrFkNs2JgQHDodD87T9y2e+iVUysxZPIAEmCW75VzTS9SfiSBKIuMxltUJUIqnI2M2vfXUqtNvWc0n5VGT6pU61hjewTpqjo6Osra1lTbgwilKRphi/09HRUdB/oVrUoodWIN8EAkVRWFpaYnZ2NqtQJUbB6NMTtcBWkG7u/jaraAfbs0VNwGghrRqzmyqNcn4ZsALvAf4TeFtDk26pSFfkfUZHRzGbzRw5ciRrydpitTG+GkBCAlSQoH3odWejAx2dmCQJOaOgqiqBRJy37RnkemKWQDKBisr+lg5+oH+YtbU1RkdHcblcef0LqhU2ZDIZpqenCQaDdHV1VZwXLvSdhUIhbt++jc1mq2iqhR5yWmZ11k9GydDW04qr9fXvwkh6oVrox7scOnQIIKsg4/f7mZmZIZVKYbPZsgpVlSzJVVUlIUWYT47TYdmBw7z5s+DKUUIWK9qFQqENLVu5nST1UsAJlBPpGp0EXI3ZzcDAQJaPrs/n4/u+7/sMbct698I5YBD4aVVV/Q1NusWQyWS4ffs2VquVgwcP5i38xOIyipJBlcBiMmGVTCjy64RwrKeXHzt1L3/83WdZjkW5Z2cfP3vuPiJyilf8C9hMFg7ZW7n50nVMJhPHjh0r6F9Qyp2rEPSdFTt37qSrq4udO3fW7CaIx+OMjo6STCYN5YT1x5Xvxk+nZEa+eY3AYgjJJGG2mDn/ztO0da9///Ug3XzIV5ARS3KRnhBLckHa+iV5scjrleQzXJL+Ecu8FZUMP7TjZ9nvPF6vUysb5RTtYP0amZ6erttMu80g3WrMbh566CF+9Vd/VWsd/dd//Vd+7/d+z9C2rJvc7GQ9xfA+SZKevutINxKJMDY2RigUoq+vj+Hh/PaA85Ew3/XNYJJMSAAZ6HK6CKdSWe/7gcEDuOYXOX/hArbXbjy31YYn08fo6CjT8kpRFVmlyE1ViKX+9evXq+6zhfUbzOv14vf7NziklYIgzrw2jdMrBJZCdA+sL/cjgSh3Xhjn3DtOV33MtYZ+Sa4XpsiynFdF5XK5NgzUXEsvcDn5T2QkmZS6ns76+8U/5uf2fgGLZHww6nZAISHD5cuXsVqtdZtpVw7pRiIRQ0ZN1ZjddHZ28slPfpJz584B8KlPfSornVUCfwDEgD9m3WnsXQ1NuvofOdfTVoy8LoT/mJzAajJhliQsJhNyJsNyPMaJ7o3yWJMkYX0tsozH43i9XqLRKMPDw+V8+YYRCAQYHR3NEh8IVJumUBSFZDLJ5cuXK+4VFsW43Gg7k8mQTspZr9scVpLx1x9kWxXplgOLxaLZaApkMhktPaEnn6B7Fskpge4rVMkQVUK0WWqrMNwKiHlsfX2v+01vZtEOjOd0y/HSherMbh5//HEef/xxQ/vJOcb/9dpfvyNJ0itAR0OTLrw+MDEcDjM0NKRFbOl0Wlse5YOcydBhd+C2WlmMRUnIMjbM/PmlF3GarDyw9/VqpdlsJh6PMzU1RSAQ2BQVGaw/tUdHR8lkMhw+fDgr/yxQqbhBn6YAqupIyCVOVVXJZDKoqkpLp4t0Kk00GMVqtxJYDHL4/oMFt20U6All586dwGtdKtEdXFvKngyrZlTmxheJepIa+TSayk6gUBFps4p2sB7pllNT2A692kagqmoACDQ86Y6NjdHT08PRo0ezvvxSTmNv3r2Xfxy7g8VkAgUsqomdDjd2i5k/fv4Sh3q66HK5kGWZZDLJiy++WJMR7fmW5XoPhlLRsxjZUw5WV1e5c+eOlqYYGRmpqgVIEKcgW/EQsFqtdPV1cu4dp7l9eYx4NMHQvfsYOLyTdDq9YZtGJSIBSZLo8+zljP8djMj/gkVaz+k+2v1/0i33EYlECqrsPB6Ppnrczijnd6pF0Q6Mpxc2w1e4Hmh40j158mTeyK+U09hgRwe//T1v4Y8vX+L24gpW1cxqKk4yodDd6mY+FCa0uMjc3Jym8soXeZaDXMFGOp1mfHyc1dVVhoaGOHbsmCEzdKORbjgc5vbt21gslqo7EvQwmUzIsqwVByVJyjruXYM72TW4Hg2KY5VlmdnZWVZXV+np6UFRFO3hIbaXJKkhifiI5UH61SO07HDQadmpdS/k5kaj0ahWsJuYmNDmgOnlzqWiwHqvEqrt0S1VtMt9MLlcLuLxOA6Ho2TRLhwOV31PbgUannQLwYjT2JHuHpwZK86MDavJhGSCcDKFOSwxfesmR17zzr1582ZNileCdCVJ0pRee/fuZXh4uKxoolSkW6ojQYzcKZfgxA1vsVi4efMmbW1ttLa2asMN80GSJFZWVvB6vZpQxGKxaOkI/X+BmhFxPclJVVXcpjZ22QcKvsdkMuVV2SUSCcLhMOFwmLm5uSyVnSBivcpuO/fMloNCRbtYLMbNmzcJhUIsLy8XLdoFAoHtbnaTFw1PuoWiAqNG5qFEkn3tbcwEQ68pvRTu6erk7Q8+WNL0plyYTCZmZ2eZn5+vWOlVLKerj5yHh4cLdiRUklfVpwWOHDmiFZVWVlaYmJggnU7jdDppaWnRiDiVSjE2NobNZuP06dNZ010FcejPX0/A+rRFpURcr6VnpeIISZK0cer5VHZiAopQ2bndblwuF4qilFXhrwabqUbLZFTSaRmbzaL9piK6HRoawm63FyzaPf3007zyyiuEQiGee+65kivRUmY3yWSSj3zkI1y5coWuri6+9rWvsW/fPlKpFD/xEz/ByMgIJpOJz3/+8+X06OZFw5NuIVitVkOeuvf07+TpW6MM2G2kJQmpxcJH33QhK9+mH9lTCVRVZWlpCb/fj9VqzXIxKxf5Il0hnJidnWXv3r0cPHiwKAkI4jZaIdYToLg5RNQmqtoiaguFQqyurnLz5k1kWdbymIFAQIvaCh1bKSLWR8SyLGufox+XsxXpiVqLBwqp7KLRKIFAgHQ6zUsvvZSlsjM6Tr1c1NrWUcA36+ef/uk6sXiKjnY373rXKbpeG+ukf6AUKtodPnyYr371q/zzP/8zX/nKV7h+/Tp/8id/wokTJzbsy4jZzZe+9CU6OjoYGxvjySef5Jd+6Zf42te+xp//+Z8D8PLLL7O0tMQ73vEOXnjhhap+77uWdI2IEdbW1jiuKvjaWrgTjdNqs/HRs6c50tuT9T69F265EEo1t9tNd3c3u3fvrqqAYjKZtIdJrnDCaORs1H8hl2yL3cySJGGz2YjFYgQCAQ4fPkxPTw+pVIpQKEQ4HGZxcZFYLIbFYtGi4VLV/UJEDOs3k744p39N/Lcey/F6yIDNZjOtra3YbDYCgQAnT57UilThcJhgMIjP59NUdrkmQNVMHan19xeLp3jqqWvYbBZ6e1oJBuM89dRVfvQjD2A2mwzts7W1lV27dvHggw/y6U9/uuh7jZjdPPXUU/zmb/4mAO9973v56Z/+aVRV5caNG7ztbW8DoLe3l/b2dkZGRjh//nzF59/wpFvJxaS3Qbz35Am+5wFPUX/cStIL4XCYO3fuZCnVbt26VbX/goh0hW9uW1tb2R4JxUhXH1XmK5IV2mZhYUEzyDl//rx209jtdnp6eujpef1Blk6nCYfDhEIhJicniUaj2tJSkLHH4yn4ABGfrb8xxflkMhlCoZBm4K6PiDerYFdP7wX9Q0RfpNLbHiaTyQ0qO7E60edGjTygNyO9EAzESMsK7e3rhd22NifLK2Hi8RQej0M7t1LQm90UgxGzG/17LBYLbW1trK6ucurUKZ566ik+8IEPMDMzw5UrV5iZmXljk245EGq13FlkaVlhJRzD47DR4txogmKxWIjH44b2IYpYiUSCgwcPZiX6qxU2wPoNNTs7SyQSKeqbWwz5SFc8dESEaJSYhJBDmKYbIX+r1Zp3+SwKSj6fj0gkgqqqWsQm/hRaJYgpx2NjY8RiMc1nY7MLdrB1pFsIhVR2ontifn5e+35zTYByf7/NIF2n04aqgqxksJhNJJMyZrMJu728FWAgEMh6mBeCEcOaQu95/PHHuXnzJmfPnmXv3r088MADVefSG550Sy15M5mMNs03Xx/s7FqIz37jWUKxBCrw/vtP8PDpg1mfYySnm0ql8Hq9BAKBgrLaagpyiUSC0dFR7el++nTlstpc0tWnEvQkVAzCx1dVVY4ePVoR+ethNpvzqsCi0SihUIilpSW8Xm9WnlgU7SwWCz6fj9nZWQYHB7Oc1+pRsNtupJsPInrTR4Z6ld3a2toGiW9LBGjxFAAAIABJREFUSwvxeLzmpNve7uLNbzrAs98Z1b7ndzx0HKvVXNZ3KWxNS8GI2Y14z8DAALIsEwwG6ezsRJIkPve5z2nve+CBBwpaCxhFw5NuMZjNZq39pJCK7AtPXyQST9LpcZFWFJ587mUO7+phX29H1ufkkmU8lWY+EMZpMRPzr7CwsMD+/fuLiicqiXTT6TQTExOsrKxw4MABBgYGmJubK+szciFIt9y8rf54xMNlM2TQ+uPM12YVi8UIhUJavjwWi+FwOOjt7UWSJBKJRNGhl+UU7PS/V6GCXSOQbj4UGqapT08sLS0hyzJLS0tZ/cTVquzOntnH/n3dhCNJ2tucWqqhnBxyLc1uHn30Ub785S9z//3383d/93e89a1vRZIkzU7A7XbzrW99C4vFkpULrgR3Jemm02kmJycJhUJ51WoCSibD7GqIntb1H9xqNiNJMB8IZ5FuboQ6sezn95763wSjUZLJFO84Psh/e/jNJS+WYmPYc6HvSNizZ49m5RgOh6tOUUiShCzL2jkZIVsxBcPn82m9xVuhBhKtUwALCwt4PB4t6hfpidnZWRKJhGbbKCLiSjonoHjBrhaDPsvBZhcGJWl9SrPD4dBWa06nk66uLq1tS6R/YF1lp09PlFMk7ury0NWVbVZTaNpLPhj10jVidvNjP/ZjfPjDH9YCiSeffBKApaUlHnroIUwmE/39/XzlK18xfH4Fj6fqT9hi6G8i/XiZPXv2sGPHDm2JkA9mk4kd7R4C0Tg2k5nMazdVT2v2UllPuqqq8gf/33+yGgzR7nbS2eLhmYlF3rq0xvDO4tMb8o1hz4UoSo2Pj2sTifUXYTU3uSANt9vNjRs3sgpXra2teW8Y4XYmxA3nzp3b0iGR+kg7N2eer99VdE7obRv1RGykc6JQwU5VVQKBACsrK3R2dmoP1M1U2G3VJGCLxZJXzBCNRrX+Wb3KTk/ExVYdudgMW0cobXbjcDj4+te/vmG7ffv2cfv2bUP7MIqGJ11Y//F9Ph8zMzNZooNYLFYysnzse+7hF//yX0gk0qgqnBzcyb6e7Ken2WxmdilM8sXbRIJLLAbD7Orq0KIiSZJYDEYNkW6xKHVtbY07d+7Q2trK2bNn8042qLQYp2+jGhgYoL+/X1umr6ysMD4+rhmNCCI2mUxMTk7mFTfUG2KE0PT0NHv27DEUadtsNrq7u7P6O0XnRDgcZmpqikgkgiRJWQ+glpaWkp0Tsixrqr9Tp05pY+jzpSdERFwLIt5OirRi/doiPTE/P08ikciaapyrstOj0lE9jYSGJ11VVbl06RLd3d0bRAdGClcXb0zT1+rBs8OGWTIRjCb4ziuTfN+pIWBdNfOVb7zAsy+MYrdP09bqYVdrB+FkinaXGVlZv8l2dZTWgBcizEgkwu3btzGZTCU7EsqNdAvlbQXR5ObzYrGYRv7JZBKbzaYp6VpbW2ltba3ZmBujCAQCWYY91UTahTonBEnMzc0RiUS0FYFeYWe1WlFVFZ/Ph8/nY3BwUMsjC5RTsIPX88RGiXQ7kW4+6FV2+doE86ns9FGxLMuG9xcOh5ukuxUwmUzcd999eS9EI6o033IQp9WKWTJhs5qxmk3MroQAiEaj/NuzL/Lci+O0eezs3LGDcDRBR9KM1CLhjyZQVZUfeeAUg72lC0q5pKt3Fzt48KChpZLRSLeSIlkmk2FxcZGlpSUOHDig3TSiAT8QCDAzM0MymdSMWgQhlbOENArRsSHLMseOHau6Q6IQ8rlj5RrUeL1ekskk6XQaj8fD4OAg7e3t2jlnVJXFeBizyUSP3Z1FpMUKduLv4jdVVRWz2VxQYVdvZ61atYwVetiJ9MTi4qL2HYt0RimV3Wap5TYbDU+6sH5R5+uzs1gsJJPJgtupqko8kebm1CJWswmb1UJHq4u+Djevvvoq4XAYd0v7uozVHyQSX8Zut+JwWPjcxx5lORzF47DR6jS27BaEKcsyExMTLC8vG3YXEygV6VZCtnpxQ39/f5a4AdjQgC8q3KFQSLPrSyQS2O32LCJ2Op0VEYSiKExOTrK8vFzWpONaQr90TiaT2py9vXv3kk6ns2wKM1YzfxS4yVQqDMADPfv4gzPvwGraSAjVFOyErWe9c7qbtT+hstOP0hLfqcvlIhgMav9vtVqz/HiNTIvYrrgrSLeQgYvVai1qZH5nepnVlTCtLgfxVJpoIk2LLYkttkzXriGOHj3KzEKAxf/5IuFwAqdTJbkWYVdPGyaTxK6OjXPXikF0H1y8eDGrI6Hcc82HSpRk8PqU5La2Ns6ePWuo+qyvcOsLV4KIRS4vHo+X1UEgPCrGx8fzkn+9kclkmJmZYX5+PssgH8gyMv+ta99iMhnSvvvvLIzzB898g/f1HdEeQG63u2TnRL6oVh8ZJ5NJ/H4/3d3dpNPpulhi1nv8uvCT6O3t3VAUFemJb3/723z2s5/F7/fzsY99jNOnT/Pwww+zb9++gp9bqeFNOp3mx3/8x3nxxReRZZmPfOQj/Mqv/EpV53hXkG4hlLJ3XFgNY5Ik+ls8+MNRVJOM22zhgQfu1y7iFpedVruNaCyOksnQ7nHgslhYWgnT12t8iKNYPimKwoMPPljTDgB9kczoDagXN9Rq6Z5P8qvvIFhaWtrgvSCIOBqNcvv2bdxut2Fl22ZC9AB3d3dz7ty5gsQjSRK3wiuoEphfi2zTGYVVl4nu7m7C4TAvzUzzl9N3WJXTHGxp42eO3sNAVzcej6fobyWiW/3DaGBggL6+vqIRcS2JuN6kK8vyhknasF4U7erqoquri7179/LII4/wvve9j8cee4xr166xtLRUkHSrMbz5+te/TjKZ5OWXXyYWi3H06FE++MEPFiX4UrjrSbdYIa2z1cniSohoJIHZZEY1SXS2t2VfrCo4TGbsMQWT2YQST5KyrMsYjcDv93Pnzh08Hg/33nsv165dqxnhbndxAxTvIAiFQoyNjREIBFBVla6uLlpbW7XJAlsR5Yo8sqIonDx5Mi8B5GKfp4PxyCqw/ptYTGYOtPXQ1dWFxePm/33xO4TlFGbJxNWQn8+8PMKvDh7VVmG5Umf99RGPx7l16xY2m63gw6gcYUe53+lWFO6M3B+iXezChQtcuHCh6HurMbyRJIloNIosy9rKLd9k8XJwV5BuIaIpVEhTVZXl5WWmxm6TjqbJxFVUad1cPBJIEIuncDnXL26X00poLUYiqmCWMiQAk2SixV08EotEIty5cwdJkrJGs9fKXFtRlLKLZEIqu5XiBlj/Xdrb2wmHw8TjcY4cOUJnZ6c20kW0cuWa4Ig2ts2AEKMsLCyUnUf+xLHv4UZwibVkDCQY9HTy+NBZAF5dXSapyDheIxKLSWIqHqH/wBAdDieZTEY774WFBW1Gnuu1UVHxeJyDBw8W9RjYjIKdHvW8Toy2jAWDQcOdC9UY3rz3ve/lqaeeoq+vj1gsxuc+97mqA5W7gnQLIV+kK3KYTqeT9u4BJPkObW4nkrTeHra2GiGWSGukGwzFyaQULCaQJBOSpJKKpZibC3Do4MYoKJlMMjY2RiQSMdyRYBTixrHZbIyMjGhFiNbW1oJFq1xxw/nz57e84itGy/f29mYdT27zvTDBCYVCWSoooz21RiEc23bs2FFRHrnb7uZvv+e/ciu4hFkycaStd332HuC0WNZJj9dqD4AK2Mzrt57JZNpQTFpdXeXWrVu0tLTQ3d3N1NQUY2NjG0zii432KVawy80VQ7bvRKEccz1glHTLmRpRjeHN5cuXMZvNzM3N4ff7efOb38z3f//3a1FzJbgrSNeI14GIPAHNgerqDR9mk0Q6Ja9fgKqKxWLGbNb1XZpMJOMpHHYL6bSKnM6QSih87auX+MQn3oHLvd6zqu9IGBwcLCg9rgS5Ecvp06e1CrqIkPRLH3FjigZ+h8Ox5eIGWG/BE5aaRo4nnwlOoZ7aXCI2cuPG43HtmhACh0rhMFs43blrw+snundwtKuHl1eWkDMZrCYTHzx0DHeegmUqleLOnTvIssy9996bdTy53rmidU90jIjfvFjHiJGCnagPwHoAsT5NRd70gp2A0T5doxJgqM7w5m/+5m94+OGHsVqt9Pb28uCDDzIyMtIk3UIQLmMiCZ4befb1tOKwWvCHo2QUFVUCxarwV08+z8ce+15sNgsej4OdO9qY9q0gp1Ukk4TTaSUYjPMf/36DR37wlKaG2717d0UdCcVQqEiWL1cqilaiCJROp3G73ZrxdambcrNQTLpbLor11OYu0fOJG8T7p6amWFxcZHh4OMsCsdawmEz80fe9g2+M38YXCXOsq4e37d6f9R5VVZmdnWVmZoahoaGsqr1AMe/cakzi4fWCnf54FhcXmZiYYO/evdp9BJtXsBMoJ6dr9DqqxvBmz549/Md//Acf+tCHiMViXLx4kY9//OMVnZvAXUu6qVSKiYkJjWzzOYzZbRasqoSUyGBSVSQJrKqJW3cWeX5knO994CB2u4UffOcpnvjysyRYn+fU2e6mrdXB+Pg8Fy/G6enp2eCRUAxGXKkqKZKZzWaNdA8dOkRPT4+hiHiziFhPJkalu5VA31Pb398PvG5bKAYcCltIi8VCLBaju7u7btG/zWzm/xjO70wVDoe5deuWZkZfbpG11ibxiUSCW7duYbFYNhTuNrNgp9+HkW3LId1qDG9+6qd+io9+9KMcP34cVVX56Ec/ysmTJys6N+14qtp6myDX9GZqaor5+Xn27duH2+3OS7gA6bRMIpjEajat59yAjJxBkTOsrEa0933/245y5dItro7M0uKy0N3hYHFxlT17Brj33nvLunFLzSerVNwwPz/P1NTUhv7WYhHxZhKxyJ13dHRsiUlOrm2h6AIA2L9/P4lEgldeeUUbqKk/93rInGVZxuv1Eg6HOXz4cE1HiVdqEh8MBotG/5tdsCsHoVCorLatSg1vPB5P3terwf/f3pfHRVXv7z8HGEBAEAGVRdZhE0VRyKzrzbRbiV3rmlubZppLara5X0utxF+WZllqabftq6bmTW8qlpplpWxqKfsqiyyyzQwwCzPz/v2Bn+MBZmCAmWEkntdrXsBw5mxzzvt8Pu/38zzvHhF0gdvWg4WFhc1Mb0pKSqBWq3WS/l369oG1tRU4Kw5ajRa4FeC0RPD3vX3RpV4pQnWhBH1trVBbLoGkQoL7Jg7DU0+Ph8i2Y6eQ5ZlbBt3OBFugc+IGUwZi1jlDq9WaVLprKJi6rbKyspWBPdA8V1pbW4vCwkKoVCrY29u3CsTGGKULObe+vr7tNhE1Ftoyia+oqMDVq1fBcRxEIhFKSkoglUr5Y2+LM92Vgl1X0hMdGelaGnpE0FUqlbh48aJO0xtGG9MVjOzsRIiO8sOfqcWoraqHWq2BlTWHURGDMTLSl1/u5x+vgbMmDBzUB8EhgyCtVWLoEO8OB1zgNqOCXcidVZIxcQMAowS3rgZiYXATi8UmzZMaAkYLzMvLg5eXF2JiYnTe3Lpypcwpi03RhTJn4bF31G+ioaEBGRkZsLOzswgBCMvd1tTUYOTIkejbt28rk/jr16+3egi157XRVsGuvfQEyx+3F4ilUmlv0O1OsItY17SwPYHE3Nl/w7o3/ot6GwUc+thiUD9HlOdVQlLbAKe+digoKEBFeQVsbW1g38cWjo4OaKhrbOL9dAJCRkVnlGSNjY3Iy8uDRCJBcHCwUSlpLWFoICYiqFQquLu7W8TotqGhAZmZmRCJRIiKiupwukDolMWKWsxvggXiljJnFpB0yZy1Wi3vJWFsGmFnwZzkvLy8EB0dze8zc/5ydHRsZdfY8iFkyLEL0VZ6golSXFxcoNFo2h0Rd8RL19LQI4KulZWV3hurvaDbz8UBzjbW8IkYDJFNUxGgqlKG5KR09HFUwsfHB4/N+DsOfvEr6tQKaBqtYWsnwpDhg/Wusy2w1j9dFTeYa1raEsJAzLoq29vbw83NDXK5HDk5OWYt1gmh0WiQn5+P6urqLrMkWkLoN6FP5szYA8ycxdnZmRddeHp66h1tmxONjY3IysqCSqUymCan6yEE6JZ4d8Qknq27oqIC+fn5PHPDkBFxeXl570jXUtGevSPHAXb2tiAtARwHuVyOWokEWq2aZyT4+hJUjUpc/CUVg329cc/94XAf0HEpICsqFBUVwcPDg5+itvcZNk1uKSboLrCuu3K5HKGhoTqLQOYo1jG09CaIiYkx2wNJn8y5qqoK+fn5aGxshI2NDW7evAmFQtHMAMecAVhIAwsICMDAgQO7fI46YhIvlDkzQYtSqUR6ejpEIlGzekRbeWKFQoGtW7eiqKjI7L7OxgLXjizVOJpVM0ClUulUlRQUFEAkEvFUIl1IScrH/i9/g7yhAZyVFcIjBmPBkgdgY3P7S6+vr0d2dnanuvAKi2RqtRo1NTV8QFKpVHz1nL1Yrk8qlfLihqCgoG4XNwhdtwICAloZeLcHYSCWSqVGCcTMKMfOzg7BwcEWkSdlJudCT2JhMJJKpTyNSzgqbM8Ap7NgzA12jjrSx8wYEApapFIp6urqoFQqoVarMXDgQAwaNKgZj1ofrly5gmXLlmHy5MlYtWqV2Y+jg9B7Eff4oFtSUoLGxka99BKmkiotkcLGyhnuHv0wbIQvRKLmo0nmNBQdHW3wPgkpNPqKZKx6LgxGKpUKGk2TF4S/vz8GDhzY7RcY47oOHDgQvr6+RhttdzYQMwVgTU0NQkNDLaKDgFQqRWZmJvr164fAwMB2z5FareZ9F5htIYBWgbiz51r4kAwJCTG5uZEhkMvlSE9Ph729Pby8vNDQ0MAHY9YuSjgitre3h1KpxDvvvINz585h9+7dXebJmgk9P+g2NjbqNPcuLy+HTCaDWCxu9r5SqURubi6kUqlBqiSNRoPk5GSMHj3aoP1pWSQzZPTGGAAVFRXw8fGBtbU1f0FqNBo4Ojryo2FDpa5dBXsoiUQiiMVis4y22wvESqWS75Ls7e3dbcY9DGq1mvfbCAsL65LBNhsVskAsk8l4Pi377p2cnNr97mUyGdLT09G/f38EBAR0e0qKzQBKSkr0PgCEAxCZTIaEhATExcVBpVIhJCQECxcuxLhx45op8iwYei/KHp/TbempywJbWVkZAgMDER4ebtBNy1z724MxxA262g8RES91LS8v57mwwpvRGOYvDIwlwR5K5ixa6GNNlJeXIycnh8+Nl5eXQy6Xm7VYJ0RLuWxoaGiXt69P5swCcWlpKWQyGS9zFtK4RCIRNBoN8vLyUFtbiyFDhlhEh4WGhgakp6fDycmpXW9iRt9zdXXFvn374OnpiQ0bNkChUODSpUtwc3O7U4KuXvSYkS5jBLQEswqMiIjAjRs3+MDm6+vb4fzZ77//jnvuuUfn/zorbqiurkZOTg5cXFwQGBjYoTQCI7dLJBI+V9ZyVNRRO0ShdNfPzw+enp4WMZJks5LQ0FDekcsUOWJDwTi39vb2EIvFZs8lC3u4sZEh6+Hm6uoKHx+fdoUNpgYRobCwEKWlpQgLCzP4wZ2SkoKXXnoJ06ZNw2uvvWZ2NaOR0PPTC/qCbn19Pa5evQqtVgt3d3cEBAR0Oj+qK+h2NtiywhzHcQgODoaDg0On9qkltFotfyOyQMxxt1uMu7i46K2cC6W7AQEB3X6xC3u3+fr6wsvLq91za+pArNVqkZ+fj8rKSoSGhloEbUnoTubr69vMBKexsZHPk7Ys1JoSdXV1SE9P568lQ2ZgCoUCcXFxuHDhAnbv3o2IiAiT76cJ8dcMuhKJBBkZGaivr8c999zT5XykMOgaUiTTBXOKGxiEvrQsEAt9XG1tbVFSUgIiQkhIiNEeAF2BTCZDZmYmnJycEBQU1KVCorECMfPdHTRoUKdmSsaGMC2lz52sZZ5UyJgx1Ju3I2AObjdv3kRYWJjBXRaSkpLwyiuvYMaMGXjllVe6/YFvBPT8oMu67AJNU7+srCxoNBoEBwfj2rVretMCHcGFCxcwevRoXvbakWArFDf4+/tj0KBB3TptV6vVqK2txfXr1yGVSiESiXiZK3u1pzAyBRobG5Gbm4u6ujq9HGBjoCOBWKlUIisrC1qtFqGhod1O3QNupzf69OkDsVjcoYcSU4AJAzHz5hWmpToqc2bFO3d3d/j7+xv0UJLL5di0aROSkpKwe/duhIeHG7w9C8dfo5CmUqmQm5vLjyKNrf9nLd3ZU9jQIpmliRvYPrGW61FRUbCysmpmCZibm4v6+nqIRCKDOlQYY59u3LiBwsJC+Pv7G6Uo1RYMlTgzbrWnpyd8fHy6nZDPFG7l5eWdTm8IFWZCvwmhzFko9W1vRqDVapGXl4eampoOFe8SEhLw2muv4cknn8TZs2d7wujWIPSYkW51dTWuXLmCgICAVsWftgpghoClEQoKClBaWtpsRODi4qL3RhSKG8RicbffsGyfWKPMwMDAdvN7bY0I2aujIyJd+5SZmQlnZ2cEBQVZxM0nkUiQmZkJFxcXuLq68uyB7pI4C/epIyPJrkIYiGUyGS9zZsfPcRwKCgo6lHKRy+V46623cOnSJXzyyScIDQ01+XF0A3p+ekGtVqOxsVHnl/77779jzJgxHb4x9BXJ2NRMKpWi6mY1rqeWQmQjQlCkH7z9BsHOrskoR6lUIiQkxGRT5I6A9W5TKBRd3idWqGEv5sDl4uLCB2JDHjAt5cSWQG9i6Y36+nq9+2Ru1oTQezc8PLzbDYVUKhUkEglvkG5ra9vMb4KlpnTdixcuXMDy5cvxzDPP4MUXX+z2WZ8J0fODLnO60oWEhASMGjWq3RGURt3UZp2tr70imbxeib3rDqOiqBpa0sLKhsPfnoyAjRMHW1tbPgi5uLgYlUPbEQi73AYGBuo1dO8K2NRUGIiUSqVeebOQlmYsHwBjHANjSnSGKmeqQHzz5k3k5ORg8ODBFiEEAZpYLpmZmfD29oaPjw84jmtT5pyamgp3d3ccP34cqamp+OSTTxASEtLdh2Fq/LWDbkpKCiIiIvQWQBpkcny79TiyknNh52CLRxb9AxH3hrabs/39f5dx8ovz8PB2RYNcjpsllQgYOhgvxD0FjuOacWiFyiIWjE2ltQeadwE2tnTX0O0LZwSsai4SidDQ0IB+/fohODi4S80gjQXm32Bvb29Ub4KuBGKlUonMzEwAQGhoqEWkppjyrqGhAeHh4e1+d2q1GjKZDNu2bcMPP/yA2tpaDBo0CCNHjsTHH3/c7ewPE+OvUUjTB+Y0pi/o/u+jU8hKzoWbtyuUchUOv/s93Dxd4SUe1OZ6ZbUN0JIWlZVVsLUVwWOgB9B4u8kfaxcj7NvFRgKFhYWtqFsuLi5GYQywzse2trbd1gW4ZbGGcUnlcjnPJb127RrUanUzebOzs7PZcrpMnVhVVWV0K0igc8bwffv2RU1NDYqKipoZ5nQ3qqqqkJWVBV9fX4OLnEqlEnFxcUhLS8ORI0cgFot5r+MeHnDbRI8Jum1dBG156hIRslLy0O+WVaN9H1vIAJTlV7QZdOvr66Gxl6OhvgEDXT1gZ2eLqrJa3PXQML2fsbKy4iWegwc3+fGq1Wr+JszNzW1WqOhooUoo3Q0JCbEIExjWRqm4uBiBgYGtnMmE8uaKigrk5uZCo9GYTN7MwDi3np6eiI6ONlsQaCsQV1ZW8n3cHB0dUVtbC41G021dnIGmayo7OxtKpRJRUVEGPcCJCL/++itWrlyJefPmYfv27fz35+DggKioKKPs23PPPYfvv/8eAwYMwLVr13Tux7Jly3DixAk4ODjg888/x8iRI42y7a6gxwRdoCnw6kqX6Aq6wiJZXzcn1NfUw6m/U9PnidCnr+6pk0ql4gPbmAdGYWB/T/z4f7+jTtKAMZNG4O9TYjq0zzY2Nq2aCApHQzdu3IBCoeDbpbARsZB10FK6210G5y1RW1uLrKws9O/fXy9VjqnlnJyc4OXlBaB5S/XS0lKeIyss1HTWfUuhUCArKwtE1G2zgJawsbGBVCqFRCJBVFQUXFxczOpHrA8sn9wRXnldXR3eeOMNZGVl4dtvv0VQUJDJ9u/ZZ5/FkiVLMGvWLJ3/P3nyJLKzs5GdnY2EhAQsWrQICQkJJtsfQ9FjcrqAfnvH/Px82NnZwcvLS6eSrDizFF++cRBqlRpaLSFstBjTVz4Ka+vbox9mk3fjxg2zixtYoYrlh1l+1MHBASKRCDU1NXB3d7cYupVSqUR2djYaGxsRGhpqFIWb0PSF5cgBtArE+kasRMR/f2KxuNlIsztRW1uLzMxMDBgwAH5+fm2OuM3FmlCpVMjMzAQRGZxPJiKcP38eq1atwvz587Fw4UKzzB4KCgrwyCOP6BzpLliwAOPGjcMTTzwBoCk3fu7cOb4NkYnx18jptjfS1deTbHCYFxZ/+BxKc8tg52gH/6GD+f8JxQ0DBw7sFnGDsFUMI7MzRZJCoYCLiwukUimSk5NNPi1vC0L/1qCgIKPmI4W5bwahDWJRURFkMhlfMWfLOjo68pLi/v37t+lyZU6wolR9fT2GDRtm0IPJlF2cGcrLy5GXl4fAwECD3bxkMhlef/115OXl4bvvvtPrXW1ulJSU8Gk8APDx8UFJSYm5gq5e9Kigqw82NjaoqamBSqWClZWVTlZCvwHOfF6XgQkJWB7KEirIrA9YVVVVq667QtexGzdutBoNtmV201VUV1cjOzsb7u7uZgtsumwQWcVcKpXyKikigoeHBxwcHCCXy+Ho6Nit6ReWuzaGHaSxArFSqURGRgasra2btc5pC0SEn3/+GatXr8aiRYuwc+dOiyqQ6RqAWULarUcHXZZG6Nu3LyoqKpCSkgKO49plCygUCuTk5EClUplU/98RCHmkrA9Yywtc2P6FQWh2w3pWWVtbG81jgXVx1Wg0iIyM7HYKmI2NDfr16weFQoGGhgaEhobC3d29WSA2p7xZCIVCwVfuTdmCvSOBuG+nJDcDAAAgAElEQVTfvtBqtaiqqkJwcLBO0xxdkMlk+Pe//43CwkIcO3YMfn5+JjmWrsDHxwdFRUX838XFxXzdoC2oVCqTOrH1qJwucxpry26RjYRYfpQpatgoUCKRoLa2FkFBQXB3d7eIJ6NEIkFWVhb69u3bZcct4Ha/LnYOGhoa+JEQ4xC35zolFF1YUo60vr4eGRkZcHBwaNMIprGxsVl+VHgOjCVvZhB2TbCkcyWVSpGeng4igp2dnUFeC0SEc+fOYc2aNViyZAnmzp3braPbtnK6x48fx44dO3DixAkkJCTgxRdfRGJiYpvr27FjByQSCdauXQuNRtOVGVvPF0cATTeSWq3usN2iQqHgu0nY2tryDvZCWWt3FKiE0l1Ty2R1SXvt7e2bnQP29Gd0q4EDB7Zb/DEXhO3XO9szjY0G2cOIyZuFgbijFoh1dXXIyMjgfSUsIZ/M2C7FxcWtjKH0FetYsEpKSkJtbS0++eQT+Pr6dtchAACeeOIJnDt3DpWVlRg4cCA2bNjAd4lZuHAhiAhLlixBfHw8HBwc8J///Edvj8Pc3FysXr0aCQkJUKlUKC0t7eru/TWC7vLly+Hk5ITo6GiMGjWKN+RoC6xzg6urK/z9/SESiUBEaGhoaHYDCilLLi4uJlWTmUO62x6EijJ2DlQqFdRqNUQiEQICAuDu7m4RbInKykrk5OTAy8sLPj4+Rv1eWqrqlEplM/qePp8JZnZeVVXVIV9ZU0MulyMtLQ1OTk4Qi8UGPQSUSiU+/fRT/Pe//4VSqYRGo4G7uzu++eYbixm1dwW//fYb5s6di8WLF2Pp0qWYPn06Zs+ejUmTJnVltX+NoJuZmYmLFy8iISEBly5dgkqlwtChQzFq1CjExMQgIiKCn27W1NSgoKAA1tbWBslRGWVJKOsV5kZdXFy6nBdsKd21lFEkM6YuKyvD4MFNzA4WhFifNjYi7mh7oK6A5Ug5jkNISIhZOLf65M0ODg78taDVapGbmwtPT0/+fHU3hJS5jrTOkUgkWLNmDSoqKrBr1y6eDVBRUQF3d3eLOLau4vr167C1tYWnpyfq6uqwbNkyTJo0CVOmTIFWq+3sMf41gm5LKBQKXLlyBRcvXkRSUhJSU1MhEol4w+4tW7YgLCys0xcOU5MJc6NsOsqq6oYm5IXSXXN13TUEbBSpz7pPyJ+VSCR8e6CWjAljjtSF1DRT+CZ3FKw7Q01NDQoLC/m0REv6nrE8HTqK+vp6pKen8334DBndEhF+/PFHvP7663jllVcwa9YskwXY+Ph4LFu2DBqNBvPmzcOqVaua/b+wsBCzZ8/mFXqbN29GbGys0fdDrVbDxsYGq1evRmpqKo4dO9aV1f01g25LfPvtt1i/fj1iY2Nhb2+P5ORkXL9+nWcDjBo1CtHR0XB1de1SNb/llJx5CzC3MeGUnFkJymQyi5HuAk3T0MzMTH4m0JGHAGNMCIuVbFbARsSdnRUwT1k3Nzf4+/tbTI60oqICeXl5vHAGAJ+iYi+NRtPMZ6LltWBsCA3Pw8LCDL62amtrsXr1alRXV2PXrl28d4gpoNFoEBISgh9//JG/D/fv348hQ4bwy8yfPx9RUVFYtGgR0tLSEBsbi4KCAqNsn41kWRxk/sDz58/HRx99hODg4M6u+q8hjmgPd999NxITE5ulEpg5eUJCAs6dO4d3332X9y1lQXj48OEGBx0mYmDUG5YflkgkfOt05jam1WohkUgQGBho8k4JhoKZwFRWViI4OLiZPNlQWFtbo1+/fs2msEK2AKMstSxStXWOmQeAQqFAREREt3vKMigUCmRkZEAkErWigTk6OsLR0ZEn42u1Wj4Qs2uBpWeMLWhhrXPc3Nx00gt1gYhw6tQprF+/Hq+99hqefvppk6cPEhMTIRaLERgYCACYOXMmjh492izochwHqVQKoOmhawjtyxC0ZCew+0+pVMLNzU1no1tj4C810jUUjY2NuHr1KhISEpCQkIA///wTNjY2GDlyJEaOHIno6GgEBwd3+uaorKxEVlYW7O3tYWNjg4aGhmYjQRcXF6PRlQyFUHlnioKULrSUNgs9eNmI2MbGhm++aAm95RiEOdKQkJBOPZwA/fLmloHY0O9CWMALDw83mGNeU1ODVatWQSqVYufOnUYLbO3h8OHDiI+Px549ewAAX331FRISErBjxw5+mdLSUjz44IOoqalBfX09Tp8+jVGjRnV6m8I8bUNDA5YsWYJRo0Zh8eLF/DJjxoxBbGws1q1b19nN9KYXugIigkwmQ3JyMhISEpCYmIicnBwMGDCAHw1HR0e3a8Ytl8uRlZUFAK3argtHghKJBHK5nK+SswBkKsI2s9sTiUQIDg7uNuWdsHMt40vX19fDzs4Onp6ecHV1hbOzc7enFGQyGTIyMtCvXz+Dc6QdgVDezAIxy5Oza0GXspDxbg3xcWAgIpw8eRIbNmzAypUr8eSTT5q1OHbo0CGcOnWqWdBNTEzEhx9+yC+zdetWEBFeffVVXLhwAXPnzsW1a9cMPj7hPSkc3X733Xd48803cf/99+Pdd98FcDsgX7hwAY6OjoiMjOzsofWmF7oCpmIbP348xo8fD+B2I0U2Gt65cyc/JWeUtZEjR8LBwYFXg8nlcr1TdpFIBDc3N74oJDS5YUyLxsZGODo68qPhrk5FhdxWU/jJdhSMH21nZ8cX5EaOHAkbGxtIJBKUlZU1S8+Yg74nhEajQV5eHmpraxEWFmYypaIuebMuZSHzo3BycoJEIkF9fT2GDh1qcOqluroaK1euhFwuxw8//NAtngSGqMb27t2L+Ph4AE0jUIVCgcrKynbVc7qYB9bW1qirq8PSpUuRnZ2N999/H2PHjm21/JgxY7p8bPrQO9I1IjQaDdLT05GQkICkpCSkpKSgsrISarUas2bNwj//+U8MGTKk08UT5j0rnJIDaDYCcnJyMqhDMSv8+Pj48C1XLAE3b95Ebm4uvLy8MHjwYJ37JTSDZyNBoSEOGwka85iqq6uRlZXV5n6ZG2q1GqWlpcjPz4etrS2ICDY2Nu1KvIkIx48fx5tvvok1a9Zg5syZ3XY8arUaISEhOHPmDLy9vRETE4N9+/YhIiKCX2bixImYMWMGnn32WaSnp2PChAkoKSkxaJ+Li4uxZ88ejBs3DqNGjYKDgwOeffZZuLq64oMPPgAA3nnQyA/u3vSCuUFEeOSRR+Dn54fY2FhkZmYiISEBGRkZcHFx4bnD0dHR8Pb27vQXLhwBsdEOu/FYIBbmh4WtacRisUk15h2BkHPbmfY0uuTdzF9B13kwFCqVCtnZ2VCpVAgPD7cYKp9Go0FOTg7q6uoQHh7Op6p0yZvZeUhKSoJYLMauXbug0Wjw0UcfGewkZkqcOHECL730EjQaDZ577jmsXbsWr7/+OqKjozF58mSkpaXh+eef52c/77zzDh588MFW62EqSkb92r9/P9577z3MmTMHV65cgUQiwcGDB1FRUcGPkrso9W0LvUG3OyCRSFrRdJgAgqUlEhMTUVJSAn9/fz43PHLkSLi4uHR69CGUckokEp43qtFooFKpEBYW1u3cVgZTcm71nQdDuhYLDYZ0dbzoTrBRt4+Pj0HNKlUqFWpqarB69WokJSVBrVYjLCwM48ePx8qVK82016bFunXr8Msvv+CHH37gv9Nt27Zh5syZKCgowEsvvYTY2Fi88cYbAEw2uhWiN+haMrRaLXJycvggnJKSgoaGBkRERPCBeOjQoZ0qcLHgkZeXB1dXV75LAetNJlSSmbtAxQy83d3dzcK5FXYtFvKohWoyZ2dnqNVqpKenG9yoUqPRQlGvhENf0zJO1Go1srKyoFQqOzTqrqysxKuvvgqO47Bjxw4MGDAApaWlKCgoMGnu0lz48MMPcfjwYbi6umLw4MH48MMPoVar8cADDwBoonGuWLEC48ePR0lJCTw8PMwxw+sNuncaVCoVrly5wgfia9euwd7eHlFRUXwgDgwMbPNJXVdXh8zMTDg4OCAoKKjZhSb03m2rQm6KICLk3IaGhnYr57alz8bNmzehUqnQr18/eHh4tPtA+uNCDva8+T+olGr0c3fCi5unwjvA+M0kKysrkZ2d3aH28ESE7777Dps3b8a6deswbdo0kz4U2lOWAcDBgwexfv16cByH4cOHY9++fV3ebn19PUpKSuDi4oLY2Fhs2LABjzzyCL788kssXboUEokEQJNR06uvvoo5c+bgvvvu6/J220Fv0L3TQUSora1FUlISX6jLy8uDt7c3Ro4cySvq3N3dUV1djdzcXD4/aqjZikajaTYdZ3lRFoQZf7grx8A4twEBAe1S7MwJqVSKjIwMuLm5wc/PD3K5vNkDCWhtBl9bWYd1z3wKzoqDyM4G8jolXPo7Iu6A8VrVNDY2IjMzExqNBmFhYQbPdioqKvDqq69CJBLhww8/NHlXYUOUZdnZ2Zg+fTrOnj0LV1fXZrlVY+HLL7/Ejh078Msvv8De3h6TJ0+GnZ0dAgIC8MMPP2Dq1Kn497//bdRt6kFv0O2JYPnQixcvIjExEYmJiSgoKAARYfr06YiNjcWIESO6ZMSjUqn44CORSHgBgzAvaoinALM4dHJyMoonsLGg0WiQm5sLqVSKsLAwvfaZwoKlVCpFXV0dirOqcerzP2HfxxbW1lbgrKwgr1fi/33zAlz6d330zjpMdCSnTEQ4cuQI3nnnHaxfvx5Tpkwxy4PtwoULWL9+PU6dOgUAiIuLAwCsXr2aX2bFihUICQnBvHnzTLYfWq0Ws2fPhouLC3bs2IGbN28iOTkZ6enpePjhh5s9BEyMXp5uT4SVlRX8/Pzg5+eHGTNmYMaMGRg+fDhmzJiBtLQ0HDhwAKtWrQLHcRgxYgQv5AgNDTU4f2prawsPDw9+pCQUMFRWViIvL49vma7LaUzIbe3IqNscYGY+3t7eCA4ObjM46ZI2F/QvxQ+fX4VGo4VarUZjoxog4EZZERSNrl1iTGRkZIDjuA51mCgvL8err76KPn364KeffjKr7aKufmQtO+8yYdC9994LjUaD9evX4+GHHzbqflhZWeH999/HpEmT8NRTTwFoKqhNnDjRqNvpCnqDbg/C3r17+ZHa2LFjsWDBAhAR6urqkJKSgoSEBGzevBlZWVlwc3PDqFGjMGrUKNx1110Gy2uZgMHBwYE3dhHmh4uLi3nerEgkQl1dHTw9PTFq1CiLsQFk3W61Wm2X2rD7h3jioRmjcfpwMqysrGFrY4Vnlj8Edw83SKVS3Lhxg6cxGaIsFDImxGKxwSkBrVaLb7/9Fu+++y42btyIxx57rFv8l1ui5T6o1WpkZ2fj3LlzKC4uxtixY3Ht2jWji3LUajXKy8tBRPj000+NnsLoKiwq6LaXiFcqlZg1axZSUlLg5uaGb775hu88GhcXh71798La2hoffPABHnrooTbXmZ+fj5kzZ6K6uhojR47EV199BVtb2za3YenQNTVmxbFx48Zh3LhxAG7f3ImJibh48SL27t3Lt91h/OGoqCiDhBZA895sPj4+kMvlyMjIgFarhaenJ+rr65GQkMC3RWIByNxyY2FOOSgoyCg34+MLxiFmfDiqK6Tw8nfHAG9XAGg2M2DOcy2VhS0ZE8w4x9DGkABQVlaGV155BX379sW5c+e6jQpoiLLMx8cHd999N2+CHxoaiuzsbMTExBh1X7Zs2YJ58+Zh7dq1Rl2vsWAxOV1DEvEff/wx/vzzT+zatQsHDhzAf//7X3zzzTdIS0vDE088gcTERNy4cQMPPPAAP5XRt87p06djypQpmDlzJhYuXIjhw4dj0aJFerfR06HRaJCVlcXnhy9fvgyVSoVhw4bxgXjIkCFtBgNhxwtdnNuWdC2lUsnTtYQGN6ZAQ0MD0tPT4ejoCLFY3K0dL4TOcyxNo1QqecYEkza3lQLSarU4ePAgtm3bhrfeeguTJ0/u1qKkIcqy+Ph47N+/H1988QUqKysRFRWFK1eutPmgaOmdYAhMKHjoCCy/kGZIIv6hhx7C+vXrMWbMGKjVagwaNAg3b97E5s2bmy3LlgOgc52rVq2Ch4cHysrKYGNj02zb+rZhKVV2c0KhUODy5cvNTOCdnJyamfwwY/Ps7GxUV1fDw8MD/v7+BpuRmLotEut6UVFRgdDQ0G73lxBCLpcjPT0dDg4OCAwMbObFXFdXBwA6TW7KysqwbNky9O/fH9u2beu0w5mx0Z6yjJnWxMfHw9raGmvXrsXMmTN1ris9PR3h4eEAOhd4LQCWX0gzJBEvXMbGxgYuLi6oqqpCSUkJ7r777mafLSkpAQCd66yqqkK/fv340Y5weX3b6Am9oDoKe3t7jBkzhifQExGqqqqQlJSEixcv4sCBA8jLy+NTGGvWrGm3ICUEx3E6PWdZW6SioqIutUWSSCTIyMiAh4eHwZ6y5oCwO3BoaChcXZtSEqwTr4+PD4DmZvAFBQV45513kJ6ejqqqKsyaNQvz58/nP2sJiI2NbdXRYePGjfzvHMdh69at2Lp1a7vrWrRoEby8vLBv3747NejqhcUEXUMS8fqW0fc+a8Fu6PKG7sdfFRzHwd3dHRMnTsTEiRORkpKCuXPnYu7cuejfvz9++uknbNmyBXV1dRgyZAg/Io6MjDS4WCU0rmEQtoyvqKho1RappZxXrVYjNzcXdXV1HXLdMgdY6xxnZ2fExMS0OQ0WMiZKS0uhVCoRExODxx57DBkZGVi5ciXef//9bu/Ka0w0NjZCJBLh22+/RXh4OH7++Wfcd999XelVZnGwmKBraCK+qKgIPj4+UKvVkEgk6N+/f5uf1fW+u7s7amtrcfz4cbzyyiuor6/nDUOE26ivr0dxcTFGjx5tssLd1q1bsWfPHtjY2MDDwwOfffYZ/Pz8jH+CTYAhQ4bg/PnzvMUho+ioVCreBP4///kPrl69CpFIhKioKD4/LBaLDb6JRCIR+vfv32wazabitbW1KCws5NsiWVtbo6amBn5+fggJCbGYByYR4fr16x1unaPVarFv3z7s2LEDcXFxiI2N7XZVGdBkPj5t2jQkJSXpbWveGYhEItTW1sLNzQ1vvfUWZs+ejYKCAr6ljqV8n10CM37Q8zIbGhsbKSAggPLy8kipVFJkZCRdu3at2TI7duygBQsWEBHR/v37adq0aUREdO3aNYqMjCSFQkF5eXkUEBBAarW6zXU+/vjjNGDAAMrNzaV58+aRt7c3paamNtvGnDlzKDAwkN/e9OnTiYgoNTW12fYCAwNJrVaTWq2mwMBAys3N5beXmppKRETTpk2j/fv3ExHRggUL6OOPPyYiorNnz1J9fT0REX388cf8NnoStFot1dbW0unTp+ntt9+mRx99lIYOHUoTJkygFStW0MGDBykvL4/q6uqovr6+U6+qqio6f/48nT17lhITE+mnn36iM2fOUGJiImVmZlJZWRnJZLJOr78rr/Lycjp37hz98ccfHdqH7OxsmjhxIj333HNUU1Nj8u+pretXCKlUSmPHjqXRo0dTUlJSl7er1Wr53/Pz82nEiBGUmZlJRETjx4+nhQsXEhGRRqPp8rbMCL1x1WKCLhHR8ePHKTg4mAIDA+mtt94iIqJ169bR0aNHiYhILpfT1KlTKSgoiGJiYig3N5f/7FtvvUWBgYEUEhJCJ06caHOdRESHDh0iZ2dnCgoKoqlTp9LGjRtp06ZNzbbh7OxMhw4dIqKmh4KbmxtptVratGkTbdq0iV/Xgw8+SL///jv9/vvv9OCDD/Lvs+W0Wi25ublRY2MjEVGr5RguXbpE99xzjzFOpcVDq9VSUVERHT58mF577TW67777aOjQoTRlyhTatGkTnTp1iioqKtoNxHV1dZSZmUmnT5+mgoKCZv+TyWR048YNSk9Pp4SEBDpz5gydO3eOLl26RLm5uVRZWdmlQN/eSyaT0dWrV+mnn36isrKyDn1u586dFBkZSSdPnmwWlEwJfddvSyxbtoz+97//0X333WeUoEtEJJFI+Pvj+eefp1mzZhERUUVFBfXr149+/fVXIrqjAq/euGox6QWg/US8vb09Dh06pPOza9eu1cnL07VOhmnTprXqzSTcxtChQ/kCnakKd0Ls3bvXopQzpgTHcbyB+uOPPw6gqXCUlpaGhIQEHD58GGvXrgURITIykmdLhIWF8eexpqYGubm56Nu3L2JiYlrRwKysrFp1YGB+s6wThanaIjEvBw8PD0RHRxucSikuLsaLL76IwYMH45dffjFrd2hDitmXL19GUVERHnnkEb7FTWdAglRBbm4utm3bhgceeACPPfYYPvjgA9x3333Ys2cP5s2bh7i4OEycOBFSqbRH5HUtKuiaE2QhhTuGr7/+GsnJyfj555/b3feeCmtrawwbNgzDhg3DvHnzeEpZSkoKEhMTsWXLFmRmZvLCioaGBuzcuRNisdjgXJ+p2yJptVrk5eWhpqYGQ4YM0evloOtzX375JXbv3o0tW7bgH//4h8WpyrRaLV5++WV8/vnnnd4G49ByHIfy8nL88MMPePrpp+Hm5obLly8jIiICwcHBWL16NRYuXIjx48dj4cKFSEtLQ2VlJfr373/HB96/bNDtjsIdc7Rvua3Tp0/j7bffxs8//9xtTSEtEYxS9ve//x1///vfAQDXrl3DM888g6CgIHh5eWH58uW4ceMGAgICmpnAOzs7Gyxrtre3h729Pd9FgQRtkUpLS5GZmQmg/bZItbW1yMjIgKenJ6Kjow0OmkVFRVi6dCkCAwNx/vz5bvOnaO+ekMlkuHbtGq9sLCsrw+TJk3Hs2DGDi2ns4bVnzx5cvXoVp06dwpAhQ/Dcc8/hvffew9mzZxEcHIzHHnsMS5YswZYtW7Bz506+tU5PgMWII8wNQxQ0H330Ea5evcqr044cOYKDBw8iNTUVTz75JK+AmzBhAt8wUd86p02bhscff5xXwEVGRuKFF17A5cuXMXXqVMTHxyM4OLgbz8idgYqKCshkMgQFBfHvMRN4pqZLSUmBQqFoZQLflbSBrrZIrIFk3759UV1dDblcjiFDhjTr8twWtFotPv/8c3z66ad47733MGHCBItXlQkxbtw4vPvuu+0GXGEqgYiwePFiFBcXY/78+fjmm2/Q0NCAr7/+GmfOnMGhQ4fg5eWFsrIyBAQEYO7cufD29m61njsA+ne0rYSvefLN3QdzFu5yc3MpJCSERCIROTo60ptvvklERBMmTKABAwbQ8OHDadiwYeTl5UVBQUF01113UX5+Pv/5TZs2UVBQEIWEhFB8fDz//smTJykkJISCgoIoLi6Ofz8vL4/uuusuEovFNH36dFIqlc2O/dChQwTAaIUQS4NCoaCLFy/S9u3b6emnn6YRI0bQ3XffTS+88AJ99tln9Oeff3aZzVBTU0PXrl2jEydO0I8//kinT5+mX3/9la5evUqFhYVUW1ur97Pp6ek0YcIEWrRoEclksu4+XTzauyeEaK+QptVqWxW+JBIJPfzww1RbW0tETWyFJUuW8NducnIyzZ49m7Zv385/5g4qnglxZ7AXejIMoeN89NFHzShxpqCoERmf8nMnQKvVUnV1NcXHx9OGDRvokUceoYiICHrwwQdpzZo1dOTIEbp+/brBbAaJRELJycl0/vx5qqys5JkUlZWVlJubS5cvX6Zz587RmTNn6OLFi5Senk7nzp2jsrIyev/992n48OF0+vRpszETuhNpaWm0ceNG+uOPP4iI6Mknn6Q33niDiJoC6htvvEGjR4+ms2fPEhGRSqXiP3uHBlyiO4W90JORmJgIsViMwMBAAMDMmTNx9OjRZoY+R48e5T0jpk6diiVLloCIcPToUcycOZN3wBeLxUhMTAQAnesMDw/H2bNn+VYos2fPxvr167Fo0SIATU38VqxY0aXq850GjuPg6uqKhx56iBeyMIOeixcv4vz589i6dSskEgnCwsJ4Ecfw4cPRp0+fZusSts4JCwvjp7wcx6FPnz7o06ePTtvLjz76CAkJCVAqlZg8eTKKioqgVqstxtDdGKBb6Up2Tnbu3Indu3fjqaeewrZt2+Dj44PFixdj48aN+O2333DvvffCzs4O0dHRiI+Px9/+9jf+fJBpG0d2G3qDrplgKd4SxqL89ARYWVnB398f/v7+vPFKY2MjUlNTcfHiRfzf//0fli9fDisrK0RFRSEsLAw//vgjZs2ahYceesggabOVlRUcHBywf/9+ZGVl4YsvvkBMTAyuXLmC5OTkbnU7MwVYsE1LS8P333+P+vp6XLlyBcePH8dnn32GDRs2YMyYMXjsscfwwgsvwMHBAUOHDkVMTAzy8/ObPYDuoPxth9CzvnELBhsBCGFuipoxKD89HSKRCCNGjMCIESOwcOFC3gT+gw8+wJYtWxAZGYm3334bn376Ke8tERMTo7ffW35+PpYuXYphw4bht99+430g7r33Xtx7771G3//2ZLzmkJ3v3bsXycnJGDVqFE6fPo3o6Gh4enpi//79GDFiBIqKirBw4UL885//RHV1NYYNG4Y5c+bA3d39TiuWdQq9QddMsASKmjEoP381cBzH20v++eef8PDw4M3QmQn8J598goqKCt4EPjo6GsOHD8f+/fvx1VdfYfv27Rg7dqzJg4lGo8HixYub+UdPnjy5WQorKioKycnJcHBwwM6dO7FixQqj+kXLZDJs27YNwcHBmDdvHqqrqxEfH49jx46B4zj88ccf2LFjB1auXAmxWIzy8nL84x//QExMDDZt2mS0/bBotJXwNWPSucfD3N4SU6dOpZUrV1JISAg5OzvTo48+2mqfxo4dSw888IDJ2RLffPMNhYeH05AhQ+iJJ57o+sm0QKjVakpNTaXPPvuMFixYQH5+fjRt2jTeV8McMFTGy2Aq2fnJkyfJ09OTysvLqbCwkObOnUsPP/ww/fvf/6bhw4fTjh07+GVLS0upsLDQ6NGo3V0AAAoQSURBVPtgAehlL1gCzElRy8rKIjs7O/L19aUpU6bQsGHDWrElxGIxTZkyhYhMx5bIysqiESNGUHV1NRERlZeXG++EWjC6g5Vw6NAhmjt3Lv/3l19+SYsXL9a7/OLFi3nqorGxfPly/npqbGykgwcP0jvvvNNTA6wu9AZdS8H169dp165dJt+OIaMeZtRDZDpDn+XLl9Onn35qsuPsxW0cPHiwVdBdsmSJzmW/+uorGj16NCkUCpPsS01NDY0bN47ee++9Vv+7g2lgHYHeuNrz+BgWjgMHDvAmO2q1GkQEjUajsyjWFehiS7Q02WmLLaHrs/reb4stkZWVhaysLNx77724++67ER8fb9Tj7MVtGFI3AG7Lzo8dO2Yy2Xm/fv3w2muvIScnB1qtli/uUg+lgXUEvYU0MyMpKYl31eI4DhzH6TRT0Wg04Diu0xcou8iF6A5DH3O13e4FEBMTg+zsbOTn58Pb2xsHDhzgudoMly9fxoIFCxAfH2/y1uSTJk3CpEmTmr3X05kJhuCv/cjpBiQlJeH+++8H0CSGeOqpp/D888/j999/bxa8rK2tWwVcNj0xBB1hSwAwiC2h730hW6Lltnx8fPDoo4+2arvdC+PDxsYGO3bswEMPPYTw8HBMnz4dEREReP3113Hs2DEAwPLly1FXV4dp06ZhxIgRmDx5ssn3y9izuDsebeUezJj/+EtAqVRS3759iYjo6NGjNH/+fMrMzKRvv/2Wnn32WaqsrCQiojfffJM2btxI33//PZ9z05UH02g0evNj3cGW2L9/P508eZJcXFzI3d2d4uLi6OTJk7wh9c2bN8nb25seffRRkzImrl+/TuPGjaMRI0bQsGHD6Pjx4wZ+Q73ohdHQW0izBJw7d44iIiKIiGj69Ok0ePBgeu6552jz5s0UGRlJ58+f55fbvXs3TZ8+nd59912+Er53714qLi4mtVpt0PbMbegTHR1NNjY29PDDD5NUKuWD8ssvv0zh4eE0dOhQmjNnjsn9JZ5//nn+99TUVPLz8zP0K+pFL4yF3qBrCXj99dfp+eefJ4lEQgsWLOCdqdatW0fLli2jpKQkUigU9Nlnn9Hhw4fp2LFjFBUVRUREhYWFxHEcrVmzhv72t7/RxYsXafXq1fTHH39QTk5Om9vVaDRmoTBZCmNi/vz5tHnzZv79MWPGmOaAe9EL/ehlL1gCjhw5goiICDg7O+PmzZuoqanBhAkTsHHjRrz//vsICQnBG2+8gR9//BEpKSlYsWIFPDw8AADnz5+Hr68vli1bhvPnz0OpVGL79u04cuQIpkyZgokTJ0KlUuncrpWVVasOAMKKsrFgKYyJ9evX4+uvv4aPjw9iY2Px4YcfGvU4TYn4+HiEhoZCLBZj8+bNrf6vVCoxY8YMiMVijB49GgUFBebfyV50Cb1B14z4+OOP8fTTTwMA5s6di+3bt+OJJ57AqlWrkJaWhuvXryM9PR3r16/Hpk2bMG3aNF7CefbsWUybNo2vOF+6dAljx47FihUrkJKSAn9/f5w4cQLAbVaCRCLBd999h++//x5JSUmQy+UAmoJwy0Dc0NCAwsLCLh2friDeVcZER98HgP379+PZZ59FcXExTpw4gWeeeeaOKOYwGe/JkyeRlpaG/fv3Iy0trdkye/fuhaurK3JycvDyyy9j5cqV3bS3vegseoOuGTF27Fi+N1dsbCz27duHf/3rX+A4DjY2Nhg2bBi0Wi02bNiAr7/+Gh988AF8fX0BNFF9JkyYwK/rl19+waOPPgoHBwfU19dDpVKhoaEBQNPNCwB1dXXYs2cPtm7diri4OOzatQtXr17FnDlzsGvXLuTl5fHry8zMxO7duwHcDoxsOmQoLIUxsXfvXkyfPh0AMGbMGCgUClRWVhp8HN0Fof2nra0tb9UpxNGjRzF79mwATfafZ86cMfqMpRemRW/Q7Ub4+vpi+vTpiIuLQ0hICABg+/btGDduHIqKivCvf/0LI0aMANAUdO+55x7+s3l5ebxJjUwmQ0lJCYYNGwbg9ojvxo0b0Gq1WL58OY4cOYKXXnoJHh4emDRpEgoLC7FhwwY0NDQgMTERy5Yt460mhf6wHeFVCnmiKpUKBw4caEVJmjx5Mr744gsAwOHDhzF+/HhwHIfJkyfjwIEDUCqVyM/PR3Z2Nu666y696+Q4Dvfffz8OHz6M+Ph4TJw4EZcuXcLmzZvh6+uLM2fOAADS09Mhl8uxdOlSnVPyuLg4iMVihIaG4tSpU/z7+qb5O3bs4BthCgM5EeHFF1+EWCxGZGQkLl26ZPB5Y+hKeqYXdxDaSviaLeXcizbR0NBAH3zwAf93ZWUlOTo68m1ekpKSaMKECXxRieHIkSO0cOFCunnzZrP33nvvPUpJSaHo6Gg6ceIEyWQyioyMpJiYGJo6dSplZmZSWVkZff755xQfH98h0xZLYEwcPXqU7rnnHoqMjKThw4fTkiVLjMaYuHTpEuXn55Ofn1+z83r8+HF6+OGHSavV0oULF+iuu+4y+JwxGCLjHTJkCBUVFfF/BwYG8lTDXlgUOs1e6H1Z0Au3G4naAriXvQdgBoCTt/62uvVTBGABgHcEn38awH8BrAPwGQAlgCG3/pcIwPnW7+EA1gNYCWAfgP8DMKC7j1/PORkD4JTg79UAVrdY5hSAMbd+twFQeeu8NVuWLWfgOgsAuAv+3g3gCcHfmQA8zXUs3f099L4Mf/WmF+4g0K07jYhURPQbe4+IvkFTQBWiHwAxgAYA4DjOFUAEgPNE9CaAFQCKARRzHDccgIiIpFxTPuEJADMBFAFYCqDPrc9aIrzRtJ8Mxbfe07kMEakBSAC4tfFZQ9bZmf1oD0kAgjmOC+A4zhZN38GxFsscAzD71u9TAZxl10Uv7gz0ei/0EBBR1a2f2ls/b3Ic9yGaRrwgohqO47IBvMRxnALAJAD1twJtJICyW6vyAuCJphHuPQAWoimAe5rzeDoAXUnnlkFI3zL63tc1GGkvsBmyH22CiNQcxy1B02jWGsBnRJTKcdxGAMlEdAzAXgBfcRyXA6AaTYG5F3cQeoNuDwYRteSA7QNwA02BtQjA+VvvOwGw4zhuIAA5ADWAPCLayD7IcVwfWCaKAQwW/O2DpmPUtUwxx3E2AFzQFLDa+mx76+zMfrQLIjoB4ESL914X/K4AMK2j6+2F5aA3vfAXAhEpiCieiD4jooVExMryJwFcArAVTQH4OICpHMdN5zjubo7jBqEpEFsiujIlPwZgJsdxdhzHBQAIRlNu25B1tsQxALO4JtwNQEJEpcY4wF70LPSOdHsBIioA8Br7m+O4CgDuAP4JIBTABiI63j171za6MiW/tdxBAGloeqgsJiINAOha5633X0RTPnwQgD85jjtBRPPQNDqNBZCDpjz6HPOcgV7caeB6c/C9aA8cx3G9xZpe9MI4+P+OHsnZvZgWKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(frame)\n",
    "ax=plt.subplot(projection = '3d')\n",
    "ax.scatter(frame['learn rate'],frame['regul'],frame['accuracy'],c = frame['accuracy'],cmap = 'viridis')\n",
    "ax.set_xlabel('learn rate')\n",
    "ax.set_ylabel('regul')\n",
    "ax.set_zlabel('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     learn rate    regul  accuracy\n",
      "0  1.000000e-01  0.00001     0.088\n",
      "1  1.000000e-02  0.00001     0.090\n",
      "2  1.000000e-03  0.00001     0.083\n",
      "3  1.000000e-04  0.00001     0.084\n",
      "4  1.000000e-05  0.00001     0.084\n",
      "5  1.000000e-06  0.00001     0.094\n",
      "6  1.000000e-07  0.00001     0.100\n",
      "7  1.000000e-08  0.00001     0.076\n",
      "8  1.000000e-09  0.00001     0.136\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'set_ylim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e4d38c4e96c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'set_ylim'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(frame)\n",
    "frame.plot()\n",
    "plt.set_ylim(0,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     learn rate    regul  accuracy\n",
      "0  1.000000e-01  0.00001     0.088\n",
      "1  1.000000e-02  0.00001     0.090\n",
      "2  1.000000e-03  0.00001     0.083\n",
      "3  1.000000e-04  0.00001     0.084\n",
      "4  1.000000e-05  0.00001     0.084\n",
      "5  1.000000e-06  0.00001     0.094\n",
      "6  1.000000e-07  0.00001     0.100\n",
      "7  1.000000e-08  0.00001     0.076\n",
      "8  1.000000e-09  0.00001     0.136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0.3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(frame)\n",
    "frame.plot()\n",
    "axes = plt.gca()\n",
    "axes.set_ylim(0,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.096\n"
     ]
    }
   ],
   "source": [
    "test_class = linear_classifer.LinearSoftmaxClassifier()\n",
    "test_pred = test_class.pred_rand(val_X)\n",
    "accuracy = multiclass_accuracy(test_pred, val_y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-51d73b5f6183>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulticlass_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Linear softmax classifier test set accuracy: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.17.0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
