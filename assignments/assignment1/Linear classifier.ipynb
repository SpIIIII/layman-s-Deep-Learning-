{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fmt_items(lines,max_lines=0):\n",
    "    max_width=max([len(line)for line in lines])\n",
    "    empty =' '*max_width\n",
    "    lines = [line.ljust(max_width)for line in lines]\n",
    "    lines += [empty]*(max_lines - len(lines))\n",
    "    return lines\n",
    "    \n",
    "def pp (*list):\n",
    "    lines = [ str(item).split('\\n') for item in list]\n",
    "    max_lines=max([len(item)for  item in lines])\n",
    "    lines = [fmt_items(item,max_lines=max_lines)for item in lines]\n",
    "    lines_t= np.array(lines).T\n",
    "    print('\\n'.join([' '.join(line) for  line in lines_t]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_check at (0,)  num = 6.000000000039306 anal = 6.0\n",
      "Gradient check passed!\n",
      "grad_check at (0,)  num = 1.0000000000065512 anal = 1.0\n",
      "grad_check at (1,)  num = 1.0000000000065512 anal = 1.0\n",
      "Gradient check passed!\n",
      "grad_check at (0, 0)  num = 1.0000000000065512 anal = 1.0\n",
      "grad_check at (0, 1)  num = 1.0000000000065512 anal = 1.0\n",
      "grad_check at (1, 0)  num = 1.0000000000065512 anal = 1.0\n",
      "grad_check at (1, 1)  num = 1.0000000000065512 anal = 1.0\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(array_sum(np.array([5.0, 2.0]))[0]-array_sum(np.array([1.0, 2.0]))[0])/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max = [2.06106005e-09 4.53978686e-05 9.99954600e-01]\n",
      "[2.06106005e-09 4.53978686e-05 9.99954600e-01]\n",
      "soft max = [1. 0. 0.]\n",
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import linear_classifer \n",
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "print(probs)\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "print(probs)\n",
    "# assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max = [4.50940412e-05 6.69254912e-03 9.93262357e-01]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-240e325b8705>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[1;34m(probs, target_index)\u001b[0m\n\u001b[0;32m     37\u001b[0m     '''\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# log = - np.log(probs[target_index])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mpp\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'log = '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "    \n",
    "linear_classifer.cross_entropy_loss(probs, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = linear_classifer.softmax(np.array([1, 0, 0]))\n",
    "linear_classifer.cross_entropy_loss(probs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [1. 0. 0.] 0\n",
      "soft max = [0.57611688 0.21194156 0.21194156]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6b65fea27065>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloss1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mloss2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss,grad = '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36msoftmax_with_cross_entropy\u001b[1;34m(predictions, target_index)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0msoftmax_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmax_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[1;34m(probs, target_index)\u001b[0m\n\u001b[0;32m     37\u001b[0m     '''\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# log = - np.log(probs[target_index])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mpp\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'log = '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "loss, grad= linear_classifer.softmax_with_cross_entropy(np.array([1., 0., 0.]), 0)\n",
    "loss1, grad1= linear_classifer.softmax_with_cross_entropy(np.array([3, 0, 0]), 0)\n",
    "loss2, grad2 = linear_classifer.softmax_with_cross_entropy(np.array([-1, 0, 0]), 0)\n",
    "print((loss1-loss2)/4)\n",
    "print('loss,grad = ',loss, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [1. 0. 0.] 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2837c8be80ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO Implement combined function orsoftmax and cross entropy and produces gradient #-2.0491237621641485\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcheck_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\gradient_check.py\u001b[0m in \u001b[0;36mcheck_gradient\u001b[1;34m(f, x, delta, tol)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0morig_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mfx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalytic_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Functions shouldn't modify input variables\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-2837c8be80ff>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO Implement combined function orsoftmax and cross entropy and produces gradient #-2.0491237621641485\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcheck_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36msoftmax_with_cross_entropy\u001b[1;34m(predictions, target_index)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0msoftmax_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmax_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[1;34m(probs, target_index)\u001b[0m\n\u001b[0;32m     37\u001b[0m     '''\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;31m# pp ('log = ',log)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "# TODO Implement combined function orsoftmax and cross entropy and produces gradient #-2.0491237621641485\n",
    "\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([ 1,0,0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[ 1.  2. -1.  1.]]\n",
      "targets =  [[2]]\n",
      "enter of the function =  [[ 1.  2. -1.  1.]] [[2]]\n",
      "soft max = [[0.20603191 0.56005279 0.02788339 0.20603191]]\n",
      "log =  3.5797242232074917\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797242232074917 [[ 0.20603191  0.56005279 -0.97211661  0.20603191]] \n",
      "                                                                                                    \n",
      "enter of the function =  [[ 1.00002  2.      -1.       1.     ]] [[2]]\n",
      "soft max = [[0.20603518 0.56005049 0.02788327 0.20603106]]\n",
      "log =  3.5797283438783922\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797283438783922 [[ 0.20603518  0.56005049 -0.97211673  0.20603106]] \n",
      "                                                                                                    \n",
      "enter of the function =  [[ 0.99998  2.      -1.       1.     ]] [[2]]\n",
      "soft max = [[0.20602864 0.5600551  0.0278835  0.20603276]]\n",
      "log =  3.5797201026020242\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797201026020242 [[ 0.20602864  0.5600551  -0.9721165   0.20603276]] \n",
      "                                                                                                    \n",
      "==GRAD_CHECK== at (0, 0)  num = 0.20603190920009948 anal = 0.20603190919001857\n",
      "enter of the function =  [[ 1.       2.00002 -1.       1.     ]] [[2]]\n",
      "soft max = [[0.2060296  0.56005772 0.02788307 0.2060296 ]]\n",
      "log =  3.579735424312667\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.579735424312667 [[ 0.2060296   0.56005772 -0.97211693  0.2060296 ]] \n",
      "                                                                                                   \n",
      "enter of the function =  [[ 1.       1.99998 -1.       1.     ]] [[2]]\n",
      "soft max = [[0.20603422 0.56004787 0.0278837  0.20603422]]\n",
      "log =  3.5797130222008735\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797130222008735 [[ 0.20603422  0.56004787 -0.9721163   0.20603422]] \n",
      "                                                                                                    \n",
      "==GRAD_CHECK== at (0, 1)  num = 0.5600527948401712 anal = 0.5600527948339517\n",
      "enter of the function =  [[ 1.       2.      -0.99998  1.     ]] [[2]]\n",
      "soft max = [[0.20603179 0.56005248 0.02788393 0.20603179]]\n",
      "log =  3.5797047808806486\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797047808806486 [[ 0.20603179  0.56005248 -0.97211607  0.20603179]] \n",
      "                                                                                                    \n",
      "enter of the function =  [[ 1.       2.      -1.00002  1.     ]] [[2]]\n",
      "soft max = [[0.20603202 0.56005311 0.02788284 0.20603202]]\n",
      "log =  3.5797436655451773\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797436655451773 [[ 0.20603202  0.56005311 -0.97211716  0.20603202]] \n",
      "                                                                                                    \n",
      "==GRAD_CHECK== at (0, 2)  num = -0.9721166132181657 anal = -0.9721166132139888\n",
      "enter of the function =  [[ 1.       2.      -1.       1.00002]] [[2]]\n",
      "soft max = [[0.20603106 0.56005049 0.02788327 0.20603518]]\n",
      "log =  3.5797283438783922\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797283438783922 [[ 0.20603106  0.56005049 -0.97211673  0.20603518]] \n",
      "                                                                                                    \n",
      "enter of the function =  [[ 1.       2.      -1.       0.99998]] [[2]]\n",
      "soft max = [[0.20603276 0.5600551  0.0278835  0.20602864]]\n",
      "log =  3.5797201026020242\n",
      "=N_SUMPLES= 1\n",
      "loss , grand (prediction) =  3.5797201026020242 [[ 0.20603276  0.5600551  -0.9721165   0.20602864]] \n",
      "                                                                                                    \n",
      "==GRAD_CHECK== at (0, 3)  num = 0.20603190920009948 anal = 0.20603190919001857\n",
      "Gradient check passed!\n",
      "predictions = [[ 2. -1. -1.  1.]\n",
      " [ 0.  1.  1.  1.]\n",
      " [ 1.  2. -1.  2.]]\n",
      "targets =  [[3]\n",
      " [3]\n",
      " [2]]\n",
      "enter of the function =  [[ 2. -1. -1.  1.]  [[3] \n",
      "                          [ 0.  1.  1.  1.]   [3] \n",
      "                          [ 1.  2. -1.  2.]]  [2]]\n",
      "soft max = [[0.19515646 0.00971627 0.00971627 0.07179405] \n",
      "            [0.02641156 0.07179405 0.07179405 0.07179405] \n",
      "            [0.07179405 0.19515646 0.00971627 0.19515646]]\n",
      "log =  9.901861007689417\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.3006203358964723 [[ 0.19515646  0.00971627  0.00971627 -0.26153928]  \n",
      "                                                 [ 0.02641156  0.07179405  0.07179405 -0.26153928]  \n",
      "                                                 [ 0.07179405  0.19515646 -0.32361707  0.19515646]] \n",
      "enter of the function =  [[ 2.00002 -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.1951596  0.00971623 0.00971623 0.07179377] \n",
      "            [0.02641145 0.07179377 0.07179377 0.07179377] \n",
      "            [0.07179377 0.1951557  0.00971623 0.1951557 ]]\n",
      "log =  9.901872717171424\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300624239057141 [[ 0.1951596   0.00971623  0.00971623 -0.26153956]  \n",
      "                                                [ 0.02641145  0.07179377  0.07179377 -0.26153956]  \n",
      "                                                [ 0.07179377  0.1951557  -0.3236171   0.1951557 ]] \n",
      "enter of the function =  [[ 1.99998 -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515332 0.00971631 0.00971631 0.07179433] \n",
      "            [0.02641166 0.07179433 0.07179433 0.07179433] \n",
      "            [0.07179433 0.19515722 0.00971631 0.19515722]]\n",
      "log =  9.901849298395895\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300616432798632 [[ 0.19515332  0.00971631  0.00971631 -0.261539  ]  \n",
      "                                                [ 0.02641166  0.07179433  0.07179433 -0.261539  ]  \n",
      "                                                [ 0.07179433  0.19515722 -0.32361703  0.19515722]] \n",
      "==GRAD_CHECK== at (0, 0)  num = 0.19515646273449147 anal = 0.19515646271894127\n",
      "enter of the function =  [[ 2.      -0.99998 -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515642 0.00971646 0.00971627 0.07179404] \n",
      "            [0.02641155 0.07179404 0.07179404 0.07179404] \n",
      "            [0.07179404 0.19515642 0.00971627 0.19515642]]\n",
      "log =  9.901861590671281\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.3006205302237603 [[ 0.19515642  0.00971646  0.00971627 -0.2615393 ]  \n",
      "                                                 [ 0.02641155  0.07179404  0.07179404 -0.2615393 ]  \n",
      "                                                 [ 0.07179404  0.19515642 -0.32361707  0.19515642]] \n",
      "enter of the function =  [[ 2.      -1.00002 -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.1951565  0.00971608 0.00971627 0.07179406] \n",
      "            [0.02641156 0.07179406 0.07179406 0.07179406] \n",
      "            [0.07179406 0.1951565  0.00971627 0.1951565 ]]\n",
      "log =  9.901860424719102\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300620141573034 [[ 0.1951565   0.00971608  0.00971627 -0.26153927]  \n",
      "                                                [ 0.02641156  0.07179406  0.07179406 -0.26153927]  \n",
      "                                                [ 0.07179406  0.1951565  -0.32361706  0.1951565 ]] \n",
      "==GRAD_CHECK== at (0, 1)  num = 0.009716268156712005 anal = 0.00971626815181842\n",
      "enter of the function =  [[ 2.      -1.      -0.99998  1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515642 0.00971627 0.00971646 0.07179404] \n",
      "            [0.02641155 0.07179404 0.07179404 0.07179404] \n",
      "            [0.07179404 0.19515642 0.00971627 0.19515642]]\n",
      "log =  9.901861590671281\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.3006205302237603 [[ 0.19515642  0.00971627  0.00971646 -0.2615393 ]  \n",
      "                                                 [ 0.02641155  0.07179404  0.07179404 -0.2615393 ]  \n",
      "                                                 [ 0.07179404  0.19515642 -0.32361707  0.19515642]] \n",
      "enter of the function =  [[ 2.      -1.      -1.00002  1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.1951565  0.00971627 0.00971608 0.07179406] \n",
      "            [0.02641156 0.07179406 0.07179406 0.07179406] \n",
      "            [0.07179406 0.1951565  0.00971627 0.1951565 ]]\n",
      "log =  9.901860424719102\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300620141573034 [[ 0.1951565   0.00971627  0.00971608 -0.26153927]  \n",
      "                                                [ 0.02641156  0.07179406  0.07179406 -0.26153927]  \n",
      "                                                [ 0.07179406  0.1951565  -0.32361706  0.1951565 ]] \n",
      "==GRAD_CHECK== at (0, 2)  num = 0.009716268156712005 anal = 0.00971626815181842\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.00002]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515618 0.00971625 0.00971625 0.07179538] \n",
      "            [0.02641152 0.07179395 0.07179395 0.07179395] \n",
      "            [0.07179395 0.19515618 0.00971625 0.19515618]]\n",
      "log =  9.901845315372428\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300615105124143 [[ 0.19515618  0.00971625  0.00971625 -0.26153795]  \n",
      "                                                [ 0.02641152  0.07179395  0.07179395 -0.26153939]  \n",
      "                                                [ 0.07179395  0.19515618 -0.32361708  0.19515618]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       0.99998]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515674 0.00971628 0.00971628 0.07179272] \n",
      "            [0.02641159 0.07179415 0.07179415 0.07179415] \n",
      "            [0.07179415 0.19515674 0.00971628 0.19515674]]\n",
      "log =  9.901876700086374\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.3006255666954583 [[ 0.19515674  0.00971628  0.00971628 -0.26154062]  \n",
      "                                                 [ 0.02641159  0.07179415  0.07179415 -0.26153918]  \n",
      "                                                 [ 0.07179415  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (0, 3)  num = -0.2615392828864138 anal = -0.2615392828872938\n",
      "enter of the function =  [[ 2.e+00 -1.e+00 -1.e+00  1.e+00]  [[3] \n",
      "                          [ 2.e-05  1.e+00  1.e+00  1.e+00]   [3] \n",
      "                          [ 1.e+00  2.e+00 -1.e+00  2.e+00]]  [2]]\n",
      "soft max = [[0.19515636 0.00971626 0.00971626 0.07179401] \n",
      "            [0.02641207 0.07179401 0.07179401 0.07179401] \n",
      "            [0.07179401 0.19515636 0.00971626 0.19515636]]\n",
      "log =  9.901862592398155\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300620864132718 [[ 0.19515636  0.00971626  0.00971626 -0.26153932]  \n",
      "                                                [ 0.02641207  0.07179401  0.07179401 -0.26153932]  \n",
      "                                                [ 0.07179401  0.19515636 -0.32361707  0.19515636]] \n",
      "enter of the function =  [[ 2.e+00 -1.e+00 -1.e+00  1.e+00]  [[3] \n",
      "                          [-2.e-05  1.e+00  1.e+00  1.e+00]   [3] \n",
      "                          [ 1.e+00  2.e+00 -1.e+00  2.e+00]]  [2]]\n",
      "soft max = [[0.19515657 0.00971627 0.00971627 0.07179409] \n",
      "            [0.02641104 0.07179409 0.07179409 0.07179409] \n",
      "            [0.07179409 0.19515657 0.00971627 0.19515657]]\n",
      "log =  9.901859423011537\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.3006198076705124 [[ 0.19515657  0.00971627  0.00971627 -0.26153924]  \n",
      "                                                 [ 0.02641104  0.07179409  0.07179409 -0.26153924]  \n",
      "                                                 [ 0.07179409  0.19515657 -0.32361706  0.19515657]] \n",
      "==GRAD_CHECK== at (1, 0)  num = 0.02641155514293558 anal = 0.026411555157523366\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.00002  1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515618 0.00971625 0.00971625 0.07179395] \n",
      "            [0.02641152 0.07179538 0.07179395 0.07179395] \n",
      "            [0.07179395 0.19515618 0.00971625 0.19515618]]\n",
      "log =  9.90186531537243\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.30062177179081 [[ 0.19515618  0.00971625  0.00971625 -0.26153939]  \n",
      "                                               [ 0.02641152  0.07179538  0.07179395 -0.26153939]  \n",
      "                                               [ 0.07179395  0.19515618 -0.32361708  0.19515618]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       0.99998  1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515674 0.00971628 0.00971628 0.07179415] \n",
      "            [0.02641159 0.07179272 0.07179415 0.07179415] \n",
      "            [0.07179415 0.19515674 0.00971628 0.19515674]]\n",
      "log =  9.901856700086373\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300618900028791 [[ 0.19515674  0.00971628  0.00971628 -0.26153918]  \n",
      "                                                [ 0.02641159  0.07179272  0.07179415 -0.26153918]  \n",
      "                                                [ 0.07179415  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (1, 1)  num = 0.07179405047130771 anal = 0.07179405044603954\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.00002  1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515618 0.00971625 0.00971625 0.07179395] \n",
      "            [0.02641152 0.07179395 0.07179538 0.07179395] \n",
      "            [0.07179395 0.19515618 0.00971625 0.19515618]]\n",
      "log =  9.90186531537243\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.30062177179081 [[ 0.19515618  0.00971625  0.00971625 -0.26153939]  \n",
      "                                               [ 0.02641152  0.07179395  0.07179538 -0.26153939]  \n",
      "                                               [ 0.07179395  0.19515618 -0.32361708  0.19515618]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       0.99998  1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515674 0.00971628 0.00971628 0.07179415] \n",
      "            [0.02641159 0.07179415 0.07179272 0.07179415] \n",
      "            [0.07179415 0.19515674 0.00971628 0.19515674]]\n",
      "log =  9.901856700086373\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300618900028791 [[ 0.19515674  0.00971628  0.00971628 -0.26153918]  \n",
      "                                                [ 0.02641159  0.07179415  0.07179272 -0.26153918]  \n",
      "                                                [ 0.07179415  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (1, 2)  num = 0.07179405047130771 anal = 0.07179405044603954\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.00002]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515618 0.00971625 0.00971625 0.07179395] \n",
      "            [0.02641152 0.07179395 0.07179395 0.07179538] \n",
      "            [0.07179395 0.19515618 0.00971625 0.19515618]]\n",
      "log =  9.901845315372428\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300615105124143 [[ 0.19515618  0.00971625  0.00971625 -0.26153939]  \n",
      "                                                [ 0.02641152  0.07179395  0.07179395 -0.26153795]  \n",
      "                                                [ 0.07179395  0.19515618 -0.32361708  0.19515618]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       0.99998]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515674 0.00971628 0.00971628 0.07179415] \n",
      "            [0.02641159 0.07179415 0.07179415 0.07179272] \n",
      "            [0.07179415 0.19515674 0.00971628 0.19515674]]\n",
      "log =  9.901876700086374\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.3006255666954583 [[ 0.19515674  0.00971628  0.00971628 -0.26153918]  \n",
      "                                                 [ 0.02641159  0.07179415  0.07179415 -0.26154062]  \n",
      "                                                 [ 0.07179415  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (1, 3)  num = -0.2615392828864138 anal = -0.2615392828872938\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.00002  2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515618 0.00971625 0.00971625 0.07179395] \n",
      "            [0.02641152 0.07179395 0.07179395 0.07179395] \n",
      "            [0.07179538 0.19515618 0.00971625 0.19515618]]\n",
      "log =  9.90186531537243\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.30062177179081 [[ 0.19515618  0.00971625  0.00971625 -0.26153939]  \n",
      "                                               [ 0.02641152  0.07179395  0.07179395 -0.26153939]  \n",
      "                                               [ 0.07179538  0.19515618 -0.32361708  0.19515618]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 0.99998  2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515674 0.00971628 0.00971628 0.07179415] \n",
      "            [0.02641159 0.07179415 0.07179415 0.07179415] \n",
      "            [0.07179272 0.19515674 0.00971628 0.19515674]]\n",
      "log =  9.901856700086373\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300618900028791 [[ 0.19515674  0.00971628  0.00971628 -0.26153918]  \n",
      "                                                [ 0.02641159  0.07179415  0.07179415 -0.26153918]  \n",
      "                                                [ 0.07179272  0.19515674 -0.32361705  0.19515674]] \n",
      "==GRAD_CHECK== at (2, 0)  num = 0.07179405047130771 anal = 0.07179405044603954\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.00002 -1.       2.     ]]  [2]]\n",
      "soft max = [[0.1951557  0.00971623 0.00971623 0.07179377] \n",
      "            [0.02641145 0.07179377 0.07179377 0.07179377] \n",
      "            [0.07179377 0.1951596  0.00971623 0.1951557 ]]\n",
      "log =  9.901872717171424\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300624239057141 [[ 0.1951557   0.00971623  0.00971623 -0.26153956]  \n",
      "                                                [ 0.02641145  0.07179377  0.07179377 -0.26153956]  \n",
      "                                                [ 0.07179377  0.1951596  -0.3236171   0.1951557 ]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       1.99998 -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515722 0.00971631 0.00971631 0.07179433] \n",
      "            [0.02641166 0.07179433 0.07179433 0.07179433] \n",
      "            [0.07179433 0.19515332 0.00971631 0.19515722]]\n",
      "log =  9.901849298395895\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300616432798632 [[ 0.19515722  0.00971631  0.00971631 -0.261539  ]  \n",
      "                                                [ 0.02641166  0.07179433  0.07179433 -0.261539  ]  \n",
      "                                                [ 0.07179433  0.19515332 -0.32361703  0.19515722]] \n",
      "==GRAD_CHECK== at (2, 1)  num = 0.19515646273449147 anal = 0.19515646271894127\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -0.99998  2.     ]]  [2]]\n",
      "soft max = [[0.19515642 0.00971627 0.00971627 0.07179404] \n",
      "            [0.02641155 0.07179404 0.07179404 0.07179404] \n",
      "            [0.07179404 0.19515642 0.00971646 0.19515642]]\n",
      "log =  9.90184159067128\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300613863557093 [[ 0.19515642  0.00971627  0.00971627 -0.2615393 ]  \n",
      "                                                [ 0.02641155  0.07179404  0.07179404 -0.2615393 ]  \n",
      "                                                [ 0.07179404  0.19515642 -0.32361687  0.19515642]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.00002  2.     ]]  [2]]\n",
      "soft max = [[0.1951565  0.00971627 0.00971627 0.07179406] \n",
      "            [0.02641156 0.07179406 0.07179406 0.07179406] \n",
      "            [0.07179406 0.1951565  0.00971608 0.1951565 ]]\n",
      "log =  9.901880424719103\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300626808239701 [[ 0.1951565   0.00971627  0.00971627 -0.26153927]  \n",
      "                                                [ 0.02641156  0.07179406  0.07179406 -0.26153927]  \n",
      "                                                [ 0.07179406  0.1951565  -0.32361726  0.1951565 ]] \n",
      "==GRAD_CHECK== at (2, 2)  num = -0.3236170651899073 anal = -0.3236170651815149\n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.00002]]  [2]]\n",
      "soft max = [[0.1951557  0.00971623 0.00971623 0.07179377] \n",
      "            [0.02641145 0.07179377 0.07179377 0.07179377] \n",
      "            [0.07179377 0.1951557  0.00971623 0.1951596 ]]\n",
      "log =  9.901872717171424\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300624239057141 [[ 0.1951557   0.00971623  0.00971623 -0.26153956]  \n",
      "                                                [ 0.02641145  0.07179377  0.07179377 -0.26153956]  \n",
      "                                                [ 0.07179377  0.1951557  -0.3236171   0.1951596 ]] \n",
      "enter of the function =  [[ 2.      -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       1.99998]]  [2]]\n",
      "soft max = [[0.19515722 0.00971631 0.00971631 0.07179433] \n",
      "            [0.02641166 0.07179433 0.07179433 0.07179433] \n",
      "            [0.07179433 0.19515722 0.00971631 0.19515332]]\n",
      "log =  9.901849298395895\n",
      "=N_SUMPLES= 3\n",
      "loss , grand (prediction) =  3.300616432798632 [[ 0.19515722  0.00971631  0.00971631 -0.261539  ]  \n",
      "                                                [ 0.02641166  0.07179433  0.07179433 -0.261539  ]  \n",
      "                                                [ 0.07179433  0.19515722 -0.32361703  0.19515332]] \n",
      "==GRAD_CHECK== at (2, 3)  num = 0.19515646273449147 anal = 0.19515646271894127\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "pp('predictions =',predictions)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "pp('targets = ',target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "print('predictions =',predictions)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "print('targets = ',target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[ 1. -1.]  [1 1]\n",
      "                          [ 0.  3.]]      \n",
      "soft max = [[0.11245721 0.01521943] \n",
      "            [0.0413707  0.83095266]]\n",
      "log =  4.370364905207625\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851824526038124 [[ 0.11245721 -0.48478057]  \n",
      "                                                 [ 0.0413707   0.33095266]] \n",
      "loss , grand (prediction), grad by W =  2.1851824526038124 [[ 0.11245721 -0.48478057]  [[-0.11245721  0.48478057]  \n",
      "                                                            [ 0.0413707   0.33095266]]  [-0.07108652  0.81573323]  \n",
      "                                                                                        [ 0.15382791 -0.15382791]] \n",
      "[[-0.11245721  0.48478057] \n",
      " [-0.07108652  0.81573323] \n",
      " [ 0.15382791 -0.15382791]]\n",
      "enter of the function =  [[ 1. -1.]  [1 1]\n",
      "                          [ 0.  3.]]      \n",
      "soft max = [[0.11245721 0.01521943] \n",
      "            [0.0413707  0.83095266]]\n",
      "log =  4.370364905207625\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851824526038124 [[ 0.11245721 -0.48478057]  \n",
      "                                                 [ 0.0413707   0.33095266]] \n",
      "loss , grand (prediction), grad by W =  2.1851824526038124 [[ 0.11245721 -0.48478057]  [[-0.11245721  0.48478057]  \n",
      "                                                            [ 0.0413707   0.33095266]]  [-0.07108652  0.81573323]  \n",
      "                                                                                        [ 0.15382791 -0.15382791]] \n",
      "enter of the function =  [[ 0.99998 -1.     ]  [1 1]\n",
      "                          [ 0.       3.     ]]      \n",
      "soft max = [[0.11245522 0.01521946] \n",
      "            [0.04137079 0.83095453]]\n",
      "log =  4.370360406959003\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851802034795016 [[ 0.11245522 -0.48478054]  \n",
      "                                                 [ 0.04137079  0.33095453]] \n",
      "loss , grand (prediction), grad by W =  2.1851802034795016 [[ 0.11245522 -0.48478054]  [[-0.11245522  0.48478054]  \n",
      "                                                            [ 0.04137079  0.33095453]]  [-0.07108443  0.81573507]  \n",
      "                                                                                        [ 0.15382601 -0.15382601]] \n",
      "enter of the function =  [[ 1.00002 -1.     ]  [1 1]\n",
      "                          [ 0.       3.     ]]      \n",
      "soft max = [[0.11245921 0.01521939] \n",
      "            [0.0413706  0.83095079]]\n",
      "log =  4.370369403536097\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851847017680486 [[ 0.11245921 -0.48478061]  \n",
      "                                                 [ 0.0413706   0.33095079]] \n",
      "loss , grand (prediction), grad by W =  2.1851847017680486 [[ 0.11245921 -0.48478061]  [[-0.11245921  0.48478061]  \n",
      "                                                            [ 0.0413706   0.33095079]]  [-0.07108861  0.8157314 ]  \n",
      "                                                                                        [ 0.15382981 -0.15382981]] \n",
      "==GRAD_CHECK== at (0, 0)  num = -0.11245721367458826 anal = -0.11245721367093255\n",
      "enter of the function =  [[ 1.      -1.00002]  [1 1]\n",
      "                          [ 0.       3.     ]]      \n",
      "soft max = [[0.11245725 0.01521913] \n",
      "            [0.04137071 0.83095291]]\n",
      "log =  4.370384296436465\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851921482182326 [[ 0.11245725 -0.48478087]  \n",
      "                                                 [ 0.04137071  0.33095291]] \n",
      "loss , grand (prediction), grad by W =  2.1851921482182326 [[ 0.11245725 -0.48478087]  [[-0.11245725  0.48478087]  \n",
      "                                                            [ 0.04137071  0.33095291]]  [-0.07108654  0.81573378]  \n",
      "                                                                                        [ 0.15382796 -0.15382796]] \n",
      "enter of the function =  [[ 1.      -0.99998]  [1 1]\n",
      "                          [ 0.       3.     ]]      \n",
      "soft max = [[0.11245718 0.01521973] \n",
      "            [0.04137068 0.83095241]]\n",
      "log =  4.370345513990775\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851727569953874 [[ 0.11245718 -0.48478027]  \n",
      "                                                 [ 0.04137068  0.33095241]] \n",
      "loss , grand (prediction), grad by W =  2.1851727569953874 [[ 0.11245718 -0.48478027]  [[-0.11245718  0.48478027]  \n",
      "                                                            [ 0.04137068  0.33095241]]  [-0.0710865   0.81573268]  \n",
      "                                                                                        [ 0.15382786 -0.15382786]] \n",
      "==GRAD_CHECK== at (0, 1)  num = 0.4847805711305497 anal = 0.48478057113584405\n",
      "enter of the function =  [[ 9.9998e-01 -1.0000e+00]  [1 1]\n",
      "                          [ 2.0000e-05  3.0000e+00]]      \n",
      "soft max = [[0.11245512 0.01521945] \n",
      "            [0.04137158 0.83095384]]\n",
      "log =  4.370362061806466\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.185181030903233 [[ 0.11245512 -0.48478055]  \n",
      "                                                [ 0.04137158  0.33095384]] \n",
      "loss , grand (prediction), grad by W =  2.185181030903233 [[ 0.11245512 -0.48478055]  [[-0.11245512  0.48478055]  \n",
      "                                                           [ 0.04137158  0.33095384]]  [-0.07108354  0.81573439]  \n",
      "                                                                                       [ 0.15382671 -0.15382671]] \n",
      "enter of the function =  [[ 1.00002e+00 -1.00000e+00]  [1 1]\n",
      "                          [-2.00000e-05  3.00000e+00]]      \n",
      "soft max = [[0.1124593  0.01521941] \n",
      "            [0.04136981 0.83095148]]\n",
      "log =  4.370367748727805\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851838743639025 [[ 0.1124593  -0.48478059]  \n",
      "                                                 [ 0.04136981  0.33095148]] \n",
      "loss , grand (prediction), grad by W =  2.1851838743639025 [[ 0.1124593  -0.48478059]  [[-0.1124593   0.48478059]  \n",
      "                                                            [ 0.04136981  0.33095148]]  [-0.07108949  0.81573207]  \n",
      "                                                                                        [ 0.15382911 -0.15382911]] \n",
      "==GRAD_CHECK== at (1, 0)  num = -0.07108651673970456 anal = -0.0710865167499724\n",
      "enter of the function =  [[ 1.      -1.00002]  [1 1]\n",
      "                          [ 0.       3.00002]]      \n",
      "soft max = [[0.11245538 0.01521888] \n",
      "            [0.04137002 0.83095572]]\n",
      "log =  4.370397534609193\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851987673045965 [[ 0.11245538 -0.48478112]  \n",
      "                                                 [ 0.04137002  0.33095572]] \n",
      "loss , grand (prediction), grad by W =  2.1851987673045965 [[ 0.11245538 -0.48478112]  [[-0.11245538  0.48478112]  \n",
      "                                                            [ 0.04137002  0.33095572]]  [-0.07108536  0.81573685]  \n",
      "                                                                                        [ 0.1538254  -0.1538254 ]] \n",
      "enter of the function =  [[ 1.      -0.99998]  [1 1]\n",
      "                          [ 0.       2.99998]]      \n",
      "soft max = [[0.11245905 0.01521998] \n",
      "            [0.04137137 0.8309496 ]]\n",
      "log =  4.370332275950659\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851661379753295 [[ 0.11245905 -0.48478002]  \n",
      "                                                 [ 0.04137137  0.3309496 ]] \n",
      "loss , grand (prediction), grad by W =  2.1851661379753295 [[ 0.11245905 -0.48478002]  [[-0.11245905  0.48478002]  \n",
      "                                                            [ 0.04137137  0.3309496 ]]  [-0.07108768  0.81572962]  \n",
      "                                                                                        [ 0.15383042 -0.15383042]] \n",
      "==GRAD_CHECK== at (1, 1)  num = 0.8157332316738318 anal = 0.8157332316797954\n",
      "enter of the function =  [[ 1.00002e+00 -1.00000e+00]  [1 1]\n",
      "                          [ 2.00000e-05  3.00000e+00]]      \n",
      "soft max = [[0.11245912 0.01521938] \n",
      "            [0.0413714  0.8309501 ]]\n",
      "log =  4.370371058376115\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851855291880575 [[ 0.11245912 -0.48478062]  \n",
      "                                                 [ 0.0413714   0.3309501 ]] \n",
      "loss , grand (prediction), grad by W =  2.1851855291880575 [[ 0.11245912 -0.48478062]  [[-0.11245912  0.48478062]  \n",
      "                                                            [ 0.0413714   0.3309501 ]]  [-0.07108772  0.81573072]  \n",
      "                                                                                        [ 0.15383051 -0.15383051]] \n",
      "enter of the function =  [[ 9.9998e-01 -1.0000e+00]  [1 1]\n",
      "                          [-2.0000e-05  3.0000e+00]]      \n",
      "soft max = [[0.11245531 0.01521948] \n",
      "            [0.04137    0.83095522]]\n",
      "log =  4.370358752143268\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.185179376071634 [[ 0.11245531 -0.48478052]  \n",
      "                                                [ 0.04137     0.33095522]] \n",
      "loss , grand (prediction), grad by W =  2.185179376071634 [[ 0.11245531 -0.48478052]  [[-0.11245531  0.48478052]  \n",
      "                                                           [ 0.04137     0.33095522]]  [-0.07108531  0.81573574]  \n",
      "                                                                                       [ 0.15382531 -0.15382531]] \n",
      "==GRAD_CHECK== at (2, 0)  num = 0.15382791058726752 anal = 0.15382791059189269\n",
      "enter of the function =  [[ 1.      -0.99998]  [1 1]\n",
      "                          [ 0.       3.00002]]      \n",
      "soft max = [[0.11245531 0.01521948] \n",
      "            [0.04137    0.83095522]]\n",
      "log =  4.370358752143268\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.185179376071634 [[ 0.11245531 -0.48478052]  \n",
      "                                                [ 0.04137     0.33095522]] \n",
      "loss , grand (prediction), grad by W =  2.185179376071634 [[ 0.11245531 -0.48478052]  [[-0.11245531  0.48478052]  \n",
      "                                                           [ 0.04137     0.33095522]]  [-0.07108531  0.81573574]  \n",
      "                                                                                       [ 0.15382531 -0.15382531]] \n",
      "enter of the function =  [[ 1.      -1.00002]  [1 1]\n",
      "                          [ 0.       2.99998]]      \n",
      "soft max = [[0.11245912 0.01521938] \n",
      "            [0.0413714  0.8309501 ]]\n",
      "log =  4.370371058376115\n",
      "=N_SUMPLES= 2\n",
      "loss , grand (prediction) =  2.1851855291880575 [[ 0.11245912 -0.48478062]  \n",
      "                                                 [ 0.0413714   0.3309501 ]] \n",
      "loss , grand (prediction), grad by W =  2.1851855291880575 [[ 0.11245912 -0.48478062]  [[-0.11245912  0.48478062]  \n",
      "                                                            [ 0.0413714   0.3309501 ]]  [-0.07108772  0.81573072]  \n",
      "                                                                                        [ 0.15383051 -0.15383051]] \n",
      "==GRAD_CHECK== at (2, 1)  num = -0.15382791058726752 anal = -0.1538279105918927\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of linear_classifer failed: Traceback (most recent call last):\n",
      "  File \"c:\\users\\сергей\\.virtualenvs\\layman-s-deep-learning--lrbitjvr\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\users\\сергей\\.virtualenvs\\layman-s-deep-learning--lrbitjvr\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 434, in superreload\n",
      "    module = reload(module)\n",
      "  File \"c:\\users\\сергей\\.virtualenvs\\layman-s-deep-learning--lrbitjvr\\lib\\imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"c:\\users\\сергей\\.virtualenvs\\layman-s-deep-learning--lrbitjvr\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 860, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 791, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"D:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\", line 100\n",
      "    loss/=\n",
      "         ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "pp(dW)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.06 [[ 0.01  0.02] \n",
      "                       [-0.01  0.01] \n",
      "                       [ 0.01  0.02]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.06 [[ 0.01  0.02] \n",
      "                       [-0.01  0.01] \n",
      "                       [ 0.01  0.02]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.060000200002 [[ 0.0100002  0.02     ] \n",
      "                                 [-0.01       0.01     ] \n",
      "                                 [ 0.01       0.02     ]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.059999800002 [[ 0.0099998  0.02     ] \n",
      "                                 [-0.01       0.01     ] \n",
      "                                 [ 0.01       0.02     ]]\n",
      "==GRAD_CHECK== at (0, 0)  num = 0.009999999999940612 anal = 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.06000040000200001 [[ 0.01       0.0200002] \n",
      "                                      [-0.01       0.01     ] \n",
      "                                      [ 0.01       0.02     ]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.059999600002 [[ 0.01       0.0199998] \n",
      "                                 [-0.01       0.01     ] \n",
      "                                 [ 0.01       0.02     ]]\n",
      "==GRAD_CHECK== at (0, 1)  num = 0.02000000000022817 anal = 0.02\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.059999800002 [[ 0.01       0.02     ] \n",
      "                                 [-0.0099998  0.01     ] \n",
      "                                 [ 0.01       0.02     ]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.060000200002 [[ 0.01       0.02     ] \n",
      "                                 [-0.0100002  0.01     ] \n",
      "                                 [ 0.01       0.02     ]]\n",
      "==GRAD_CHECK== at (1, 0)  num = -0.009999999999940612 anal = -0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.060000200002 [[ 0.01       0.02     ] \n",
      "                                 [-0.01       0.0100002] \n",
      "                                 [ 0.01       0.02     ]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.059999800002 [[ 0.01       0.02     ] \n",
      "                                 [-0.01       0.0099998] \n",
      "                                 [ 0.01       0.02     ]]\n",
      "==GRAD_CHECK== at (1, 1)  num = 0.009999999999940612 anal = 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.060000200002 [[ 0.01       0.02     ] \n",
      "                                 [-0.01       0.01     ] \n",
      "                                 [ 0.0100002  0.02     ]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.059999800002 [[ 0.01       0.02     ] \n",
      "                                 [-0.01       0.01     ] \n",
      "                                 [ 0.0099998  0.02     ]]\n",
      "==GRAD_CHECK== at (2, 0)  num = 0.009999999999940612 anal = 0.01\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.06000040000200001 [[ 0.01       0.02     ] \n",
      "                                      [-0.01       0.01     ] \n",
      "                                      [ 0.01       0.0200002]]\n",
      "enter of L2  (3, 2) 0.01\n",
      "L2 loss, grad =  0.059999600002 [[ 0.01       0.02     ] \n",
      "                                 [-0.01       0.01     ] \n",
      "                                 [ 0.01       0.0199998]]\n",
      "==GRAD_CHECK== at (2, 1)  num = 0.02000000000022817 anal = 0.02\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[-1.00355301e-02  2.94346634e-02 -1.24129261e-02 ... -6.82652294e-03 [3 4 5 ... 4 1 9]\n",
      "                            5.53978299e-06 -2.47033968e-03]                                                     \n",
      "                          [ 1.09904600e-03  7.81170892e-04  1.39807856e-03 ... -3.55486628e-03                  \n",
      "                           -1.16054377e-03 -4.82905733e-03]                                                     \n",
      "                          [ 2.90368233e-03 -1.33169579e-03  7.49085978e-03 ... -8.90739320e-03                  \n",
      "                           -3.63108012e-03 -1.19014362e-03]                                                     \n",
      "                          ...                                                                                   \n",
      "                          [-2.34375194e-04 -6.75812623e-03  2.93474844e-03 ... -5.56928079e-03                  \n",
      "                           -8.99455695e-03  1.00103666e-02]                                                     \n",
      "                          [-5.31508439e-03 -8.50418108e-03  1.27039373e-03 ... -4.26399561e-03                  \n",
      "                           -6.13364992e-03  7.51032140e-03]                                                     \n",
      "                          [-1.62644971e-04  2.75783053e-02 -1.50531348e-02 ...  1.28643743e-02                  \n",
      "                            3.88653569e-03 -4.59871460e-03]]                                                    \n",
      "soft max = [[1.10039335e-05 1.14469463e-05 1.09778039e-05 ... 1.10393020e-05\n",
      "             1.11149814e-05 1.10874961e-05]                                 \n",
      "            [1.11271423e-05 1.11236059e-05 1.11304702e-05 ... 1.10754779e-05\n",
      "             1.11020280e-05 1.10613746e-05]                                 \n",
      "            [1.11472409e-05 1.11001280e-05 1.11984928e-05 ... 1.10163545e-05\n",
      "             1.10746338e-05 1.11016993e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.11123151e-05 1.10400570e-05 1.11475872e-05 ... 1.10531898e-05\n",
      "             1.10153943e-05 1.12267430e-05]                                 \n",
      "            [1.10559998e-05 1.10207973e-05 1.11290491e-05 ... 1.10676268e-05\n",
      "             1.10469535e-05 1.11987107e-05]                                 \n",
      "            [1.11131122e-05 1.14257164e-05 1.09488584e-05 ... 1.12588300e-05\n",
      "             1.11582024e-05 1.10639228e-05]]                                \n",
      "log =  102666.43487909816\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.407381653233129 [[ 1.10039335e-05  1.14469463e-05  1.09778039e-05 ...  1.10393020e-05 \n",
      "                                                   1.11149814e-05  1.10874961e-05]                                    \n",
      "                                                 [ 1.11271423e-05  1.11236059e-05  1.11304702e-05 ...  1.10754779e-05 \n",
      "                                                   1.11020280e-05  1.10613746e-05]                                    \n",
      "                                                 [ 1.11472409e-05  1.11001280e-05  1.11984928e-05 ...  1.10163545e-05 \n",
      "                                                   1.10746338e-05  1.11016993e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11123151e-05  1.10400570e-05  1.11475872e-05 ...  1.10531898e-05 \n",
      "                                                   1.10153943e-05  1.12267430e-05]                                    \n",
      "                                                 [ 1.10559998e-05 -1.00090314e-04  1.11290491e-05 ...  1.10676268e-05 \n",
      "                                                   1.10469535e-05  1.11987107e-05]                                    \n",
      "                                                 [ 1.11131122e-05  1.14257164e-05  1.09488584e-05 ...  1.12588300e-05 \n",
      "                                                   1.11582024e-05 -1.00047188e-04]]                                   \n",
      "loss , grand (prediction), grad by W =  11.407381653233129 [[ 1.10039335e-05  1.14469463e-05  1.09778039e-05 ...  1.10393020e-05 [[-1.66253557e-04 -1.89918135e-04  3.76609228e-04 ...  1.08129553e-04 \n",
      "                                                              1.11149814e-05  1.10874961e-05]                                      -8.03437285e-04  3.10160819e-04]                                    \n",
      "                                                            [ 1.11271423e-05  1.11236059e-05  1.11304702e-05 ...  1.10754779e-05  [-7.16307358e-04  5.29970448e-04  8.93680910e-04 ... -2.31175428e-04 \n",
      "                                                              1.11020280e-05  1.10613746e-05]                                      -1.05371610e-03  3.02352522e-04]                                    \n",
      "                                                            [ 1.11472409e-05  1.11001280e-05  1.11984928e-05 ...  1.10163545e-05  [-1.05310542e-03  1.12476170e-03  1.47215146e-03 ... -4.02260089e-04 \n",
      "                                                              1.10746338e-05  1.11016993e-05]                                      -1.13091386e-03 -2.41499559e-04]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.11123151e-05  1.10400570e-05  1.11475872e-05 ...  1.10531898e-05  [-5.86846162e-05  5.77313476e-04 -2.91721545e-04 ...  6.26207844e-04 \n",
      "                                                              1.10153943e-05  1.12267430e-05]                                      -6.33597957e-04 -4.15481247e-04]                                    \n",
      "                                                            [ 1.10559998e-05 -1.00090314e-04  1.11290491e-05 ...  1.10676268e-05  [ 5.49987496e-04 -5.53360868e-04 -5.26244203e-04 ...  7.93259210e-04 \n",
      "                                                              1.10469535e-05  1.11987107e-05]                                      -1.08535125e-04 -3.22266188e-04]                                    \n",
      "                                                            [ 1.11131122e-05  1.14257164e-05  1.09488584e-05 ...  1.12588300e-05  [ 3.36236516e-02 -9.65667120e-02 -4.82203852e-02 ...  2.30871212e-02 \n",
      "                                                              1.11582024e-05 -1.00047188e-04]]                                      3.31803644e-02  3.75945926e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.030694838093339588 [[-0.00317687 -0.0063001  -0.01038537 ... -0.00394656 -0.00440887\n",
      "                                         0.00132788]                                                   \n",
      "                                       [ 0.00266164 -0.0036412  -0.01428018 ...  0.02223177  0.00119195\n",
      "                                         0.01103128]                                                   \n",
      "                                       [ 0.00035212  0.01740118 -0.01403043 ...  0.004906   -0.01898681\n",
      "                                        -0.00950954]                                                   \n",
      "                                       ...                                                             \n",
      "                                       [-0.01327777  0.00468301 -0.00734139 ...  0.00117763 -0.00202039\n",
      "                                        -0.00220177]                                                   \n",
      "                                       [ 0.00589102  0.01166612  0.01386822 ... -0.00259888 -0.00285735\n",
      "                                         0.00844515]                                                   \n",
      "                                       [-0.00769782  0.00563378 -0.0039049  ... -0.00601473 -0.00788454\n",
      "                                        -0.0110443 ]]                                                  \n",
      "Epoch 0, loss: 11.438076\n",
      "== W == -0.1565803540047206\n",
      "enter of the function =  [[ 2.32198993e-03 -3.97759223e-03  5.18897941e-05 ...  3.89917157e-03 [8 7 4 ... 7 1 1]\n",
      "                           -2.64213523e-03 -5.23823572e-03]                                                     \n",
      "                          [ 1.11020246e-02 -7.28691914e-03 -1.34314397e-02 ... -2.08199823e-03                  \n",
      "                            5.77546078e-03 -4.66706579e-03]                                                     \n",
      "                          [-1.47836221e-03  6.31164381e-03 -1.15552442e-02 ...  2.16876289e-03                  \n",
      "                           -1.56609181e-02 -1.47351948e-04]                                                     \n",
      "                          ...                                                                                   \n",
      "                          [ 4.34251694e-03 -9.83577799e-03  3.06630100e-03 ... -9.15051989e-03                  \n",
      "                            1.57639386e-03  1.80247772e-04]                                                     \n",
      "                          [-6.25690794e-03  3.20927503e-02 -3.30717104e-03 ...  1.68275913e-02                  \n",
      "                            1.40672053e-02 -1.35200627e-02]                                                     \n",
      "                          [-4.80912795e-03  7.90603596e-03 -1.56419006e-02 ...  4.46391290e-03                  \n",
      "                           -6.22184222e-03  2.81643285e-03]]                                                    \n",
      "soft max = [[1.11407938e-05 1.10708321e-05 1.11155318e-05 ... 1.11583787e-05\n",
      "             1.10856265e-05 1.10568845e-05]                                 \n",
      "            [1.12390410e-05 1.10342556e-05 1.09666633e-05 ... 1.10918378e-05\n",
      "             1.11793347e-05 1.10632016e-05]                                 \n",
      "            [1.10985352e-05 1.11853305e-05 1.09872582e-05 ... 1.11390869e-05\n",
      "             1.09422406e-05 1.11133173e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.11633268e-05 1.10061667e-05 1.11490891e-05 ... 1.10137113e-05\n",
      "             1.11324904e-05 1.11169586e-05]                                 \n",
      "            [1.10456269e-05 1.14774501e-05 1.10782567e-05 ... 1.13035755e-05\n",
      "             1.12724163e-05 1.09656914e-05]                                 \n",
      "            [1.10616301e-05 1.12031785e-05 1.09424487e-05 ... 1.11646821e-05\n",
      "             1.10460142e-05 1.11463037e-05]]                                \n",
      "log =  102666.74453401154\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.407416059334615 [[ 1.11407938e-05  1.10708321e-05  1.11155318e-05 ...  1.11583787e-05 \n",
      "                                                  -1.00025485e-04  1.10568845e-05]                                    \n",
      "                                                 [ 1.12390410e-05  1.10342556e-05  1.09666633e-05 ... -1.00019273e-04 \n",
      "                                                   1.11793347e-05  1.10632016e-05]                                    \n",
      "                                                 [ 1.10985352e-05  1.11853305e-05  1.09872582e-05 ...  1.11390869e-05 \n",
      "                                                   1.09422406e-05  1.11133173e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11633268e-05  1.10061667e-05  1.11490891e-05 ... -1.00097400e-04 \n",
      "                                                   1.11324904e-05  1.11169586e-05]                                    \n",
      "                                                 [ 1.10456269e-05 -9.96336610e-05  1.10782567e-05 ...  1.13035755e-05 \n",
      "                                                   1.12724163e-05  1.09656914e-05]                                    \n",
      "                                                 [ 1.10616301e-05 -9.99079326e-05  1.09424487e-05 ...  1.11646821e-05 \n",
      "                                                   1.10460142e-05  1.11463037e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.407416059334615 [[ 1.11407938e-05  1.10708321e-05  1.11155318e-05 ...  1.11583787e-05 [[-1.67734634e-04 -1.90114860e-04  3.75914488e-04 ...  1.10624867e-04 \n",
      "                                                             -1.00025485e-04  1.10568845e-05]                                      -8.05829909e-04  3.08145753e-04]                                    \n",
      "                                                            [ 1.12390410e-05  1.10342556e-05  1.09666633e-05 ... -1.00019273e-04  [-7.17775008e-04  5.29833328e-04  8.93038182e-04 ... -2.28547110e-04 \n",
      "                                                              1.11793347e-05  1.10632016e-05]                                      -1.05625034e-03  3.00183155e-04]                                    \n",
      "                                                            [ 1.10985352e-05  1.11853305e-05  1.09872582e-05 ...  1.11390869e-05  [-1.05446655e-03  1.12457350e-03  1.47150152e-03 ... -3.99374671e-04 \n",
      "                                                              1.09422406e-05  1.11133173e-05]                                      -1.13342845e-03 -2.43884579e-04]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.11633268e-05  1.10061667e-05  1.11490891e-05 ... -1.00097400e-04  [-5.97525494e-05  5.77937214e-04 -2.92768943e-04 ...  6.29182525e-04 \n",
      "                                                              1.11324904e-05  1.11169586e-05]                                      -6.35469390e-04 -4.17584285e-04]                                    \n",
      "                                                            [ 1.10456269e-05 -9.96336610e-05  1.10782567e-05 ...  1.13035755e-05  [ 5.49039672e-04 -5.52877058e-04 -5.27247889e-04 ...  7.96463131e-04 \n",
      "                                                              1.12724163e-05  1.09656914e-05]                                      -1.10371389e-04 -3.24571778e-04]                                    \n",
      "                                                            [ 1.10616301e-05 -9.99079326e-05  1.09424487e-05 ...  1.11646821e-05  [ 3.36265782e-02 -9.65753697e-02 -4.82251713e-02 ...  2.30892120e-02 \n",
      "                                                              1.10460142e-05  1.11463037e-05]]                                      3.31832741e-02  3.75976853e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.03131161606646397 [[-0.0032103  -0.006365   -0.01048545 ... -0.00398495 -0.004461  \n",
      "                                        0.00134426]                                                   \n",
      "                                      [ 0.00268109 -0.00367231 -0.01441405 ...  0.02245177  0.00119333\n",
      "                                        0.01114462]                                                   \n",
      "                                      [ 0.00034511  0.01758644 -0.01415601 ...  0.00495104 -0.01918798\n",
      "                                       -0.00960705]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.01341113  0.00473562 -0.00741772 ...  0.00119566 -0.00204693\n",
      "                                       -0.00222794]                                                   \n",
      "                                      [ 0.00595543  0.01177725  0.01400164 ... -0.00261694 -0.00288701\n",
      "                                        0.00852637]                                                   \n",
      "                                      [-0.00743856  0.00472445 -0.00442615 ... -0.005844   -0.00763158\n",
      "                                       -0.0107788 ]]                                                  \n",
      "Epoch 1, loss: 11.438728\n",
      "== W == -0.15937594815657624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[-0.00330276  0.01409032  0.0004134  ...  0.00978657  0.00369659 [1 1 4 ... 7 5 1]\n",
      "                            0.01004468]                                                                     \n",
      "                          [ 0.00457788 -0.00558165 -0.00365474 ...  0.0101945   0.00867194                  \n",
      "                           -0.01806381]                                                                     \n",
      "                          [-0.0011338  -0.0051951   0.00396353 ...  0.00145045 -0.00261756                  \n",
      "                           -0.00229275]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.00153447  0.012333   -0.00621393 ...  0.00822976 -0.01313063                  \n",
      "                           -0.00452212]                                                                     \n",
      "                          [-0.00192731 -0.0047053   0.00990329 ...  0.00347109 -0.00210299                  \n",
      "                           -0.00564847]                                                                     \n",
      "                          [ 0.00805176 -0.0104444   0.0003537  ...  0.00519152 -0.00181165                  \n",
      "                           -0.00281621]]                                                                    \n",
      "soft max = [[1.10783404e-05 1.12727124e-05 1.11195859e-05 ... 1.12243016e-05\n",
      "             1.11561536e-05 1.12271991e-05]                                 \n",
      "            [1.11659897e-05 1.10531228e-05 1.10744418e-05 ... 1.12288814e-05\n",
      "             1.12117977e-05 1.09160135e-05]                                 \n",
      "            [1.11023949e-05 1.10573962e-05 1.11591320e-05 ... 1.11311234e-05\n",
      "             1.10859338e-05 1.10895353e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.10979474e-05 1.12529200e-05 1.10461364e-05 ... 1.12068411e-05\n",
      "             1.09699972e-05 1.10648402e-05]                                 \n",
      "            [1.10935887e-05 1.10628134e-05 1.12256119e-05 ... 1.11536382e-05\n",
      "             1.10916398e-05 1.10523843e-05]                                 \n",
      "            [1.12048465e-05 1.09995047e-05 1.11189221e-05 ... 1.11728438e-05\n",
      "             1.10948717e-05 1.10837319e-05]]                                \n",
      "log =  102667.05798667377\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.407450887408196 [[ 1.10783404e-05 -9.98383987e-05  1.11195859e-05 ...  1.12243016e-05 \n",
      "                                                   1.11561536e-05  1.12271991e-05]                                    \n",
      "                                                 [ 1.11659897e-05 -1.00057988e-04  1.10744418e-05 ...  1.12288814e-05 \n",
      "                                                   1.12117977e-05  1.09160135e-05]                                    \n",
      "                                                 [ 1.11023949e-05  1.10573962e-05  1.11591320e-05 ...  1.11311234e-05 \n",
      "                                                   1.10859338e-05  1.10895353e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.10979474e-05  1.12529200e-05  1.10461364e-05 ... -9.99042700e-05 \n",
      "                                                   1.09699972e-05  1.10648402e-05]                                    \n",
      "                                                 [ 1.10935887e-05  1.10628134e-05  1.12256119e-05 ...  1.11536382e-05 \n",
      "                                                   1.10916398e-05  1.10523843e-05]                                    \n",
      "                                                 [ 1.12048465e-05 -1.00111606e-04  1.11189221e-05 ...  1.11728438e-05 \n",
      "                                                   1.10948717e-05  1.10837319e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.407450887408196 [[ 1.10783404e-05 -9.98383987e-05  1.11195859e-05 ...  1.12243016e-05 [[-1.69239414e-04 -1.90314659e-04  3.75205582e-04 ...  1.13166086e-04 \n",
      "                                                              1.11561536e-05  1.12271991e-05]                                      -8.08260842e-04  3.06094874e-04]                                    \n",
      "                                                            [ 1.11659897e-05 -1.00057988e-04  1.10744418e-05 ...  1.12288814e-05  [-7.19266510e-04  5.29693656e-04  8.92381582e-04 ... -2.25870795e-04 \n",
      "                                                              1.12117977e-05  1.09160135e-05]                                      -1.05882481e-03  2.97975844e-04]                                    \n",
      "                                                            [ 1.11023949e-05  1.10573962e-05  1.11591320e-05 ...  1.11311234e-05  [-1.05585069e-03  1.12438214e-03  1.47083741e-03 ... -3.96437829e-04 \n",
      "                                                              1.10859338e-05  1.10895353e-05]                                      -1.13598350e-03 -2.46310312e-04]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.10979474e-05  1.12529200e-05  1.10461364e-05 ... -9.99042700e-05  [-6.08401463e-05  5.78566225e-04 -2.93834587e-04 ...  6.32209235e-04 \n",
      "                                                              1.09699972e-05  1.10648402e-05]                                      -6.37374198e-04 -4.19724719e-04]                                    \n",
      "                                                            [ 1.10935887e-05  1.10628134e-05  1.12256119e-05 ...  1.11536382e-05  [ 5.48073273e-04 -5.52389463e-04 -5.28269484e-04 ...  7.99721949e-04 \n",
      "                                                              1.10916398e-05  1.10523843e-05]                                      -1.12240912e-04 -3.26917198e-04]                                    \n",
      "                                                            [ 1.12048465e-05 -1.00111606e-04  1.11189221e-05 ...  1.11728438e-05  [ 3.36295313e-02 -9.65841174e-02 -4.82300080e-02 ...  2.30913242e-02 \n",
      "                                                              1.10948717e-05  1.10837319e-05]]                                      3.31862115e-02  3.76008081e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.031940862748485635 [[-0.00324408 -0.00643056 -0.01058655 ... -0.00402369 -0.00451366\n",
      "                                         0.00136079]                                                   \n",
      "                                       [ 0.00270073 -0.00370373 -0.01454926 ...  0.022674    0.0011947 \n",
      "                                         0.01125906]                                                   \n",
      "                                       [ 0.00033802  0.01777355 -0.01428286 ...  0.00499655 -0.0193912 \n",
      "                                        -0.00970555]                                                   \n",
      "                                       ...                                                             \n",
      "                                       [-0.01354584  0.00478875 -0.00749482 ...  0.00121391 -0.00207375\n",
      "                                        -0.0022544 ]                                                   \n",
      "                                       [ 0.00602048  0.0118895   0.01413638 ... -0.00263514 -0.00291698\n",
      "                                         0.00860839]                                                   \n",
      "                                       [-0.00717668  0.00380594 -0.00495266 ... -0.00567155 -0.00737606\n",
      "                                        -0.01051061]]                                                  \n",
      "Epoch 2, loss: 11.439392\n",
      "== W == -0.16222053354313457\n",
      "enter of the function =  [[-4.10833570e-03 -5.94360945e-03  9.92537998e-03 ... -8.20283427e-03 [7 4 8 ... 2 2 1]\n",
      "                           -1.35051099e-02 -1.53599416e-03]                                                     \n",
      "                          [-3.74933395e-04  1.30557858e-02 -8.78899277e-04 ... -5.31614941e-03                  \n",
      "                           -1.22089840e-02 -1.98969579e-02]                                                     \n",
      "                          [-3.04865958e-03 -8.38300731e-04  3.05204679e-03 ... -5.10600588e-03                  \n",
      "                            2.60272078e-04  4.66929920e-03]                                                     \n",
      "                          ...                                                                                   \n",
      "                          [-8.31736922e-03 -9.56192647e-03  3.92364782e-03 ... -8.77292389e-03                  \n",
      "                           -4.60433123e-03  1.53488588e-03]                                                     \n",
      "                          [ 4.33189519e-04  1.33597237e-02 -1.96367314e-03 ...  3.28646327e-04                  \n",
      "                            5.56851279e-03 -1.48474876e-02]                                                     \n",
      "                          [-4.90191191e-03  1.86116015e-03  5.82856880e-03 ... -3.91505897e-03                  \n",
      "                           -8.78028364e-03  3.74182077e-06]]                                                    \n",
      "soft max = [[1.10694542e-05 1.10491574e-05 1.12258950e-05 ... 1.10242230e-05\n",
      "             1.09659242e-05 1.10979653e-05]                                 \n",
      "            [1.11108582e-05 1.12610916e-05 1.11052601e-05 ... 1.10560925e-05\n",
      "             1.09801467e-05 1.08960553e-05]                                 \n",
      "            [1.10811905e-05 1.11057110e-05 1.11490002e-05 ... 1.10584161e-05\n",
      "             1.11179181e-05 1.11670455e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.10229604e-05 1.10092503e-05 1.11587219e-05 ... 1.10179400e-05\n",
      "             1.10639652e-05 1.11320982e-05]                                 \n",
      "            [1.11198408e-05 1.12645148e-05 1.10932199e-05 ... 1.11186783e-05\n",
      "             1.11770916e-05 1.09512137e-05]                                 \n",
      "            [1.10606733e-05 1.11357309e-05 1.11799987e-05 ... 1.10715939e-05\n",
      "             1.10178589e-05 1.11150664e-05]]                                \n",
      "log =  102667.3752944745\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.4074861438305 [[ 1.10694542e-05  1.10491574e-05  1.12258950e-05 ... -1.00086888e-04 \n",
      "                                                 1.09659242e-05  1.10979653e-05]                                    \n",
      "                                               [ 1.11108582e-05  1.12610916e-05  1.11052601e-05 ...  1.10560925e-05 \n",
      "                                                 1.09801467e-05  1.08960553e-05]                                    \n",
      "                                               [ 1.10811905e-05  1.11057110e-05  1.11490002e-05 ...  1.10584161e-05 \n",
      "                                                -9.99931930e-05  1.11670455e-05]                                    \n",
      "                                               ...                                                                  \n",
      "                                               [ 1.10229604e-05  1.10092503e-05 -9.99523892e-05 ...  1.10179400e-05 \n",
      "                                                 1.10639652e-05  1.11320982e-05]                                    \n",
      "                                               [ 1.11198408e-05  1.12645148e-05 -1.00017891e-04 ...  1.11186783e-05 \n",
      "                                                 1.11770916e-05  1.09512137e-05]                                    \n",
      "                                               [ 1.10606733e-05 -9.99753802e-05  1.11799987e-05 ...  1.10715939e-05 \n",
      "                                                 1.10178589e-05  1.11150664e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.4074861438305 [[ 1.10694542e-05  1.10491574e-05  1.12258950e-05 ... -1.00086888e-04 [[-1.70768284e-04 -1.90517580e-04  3.74482245e-04 ...  1.15754039e-04 \n",
      "                                                            1.09659242e-05  1.10979653e-05]                                      -8.10730711e-04  3.04007559e-04]                                    \n",
      "                                                          [ 1.11108582e-05  1.12610916e-05  1.11052601e-05 ...  1.10560925e-05  [-7.20782257e-04  5.29551385e-04  8.91710847e-04 ... -2.23145618e-04 \n",
      "                                                            1.09801467e-05  1.08960553e-05]                                      -1.06144017e-03  2.95729936e-04]                                    \n",
      "                                                          [ 1.10811905e-05  1.11057110e-05  1.11490002e-05 ...  1.10584161e-05  [-1.05725822e-03  1.12418758e-03  1.47015887e-03 ... -3.93448649e-04 \n",
      "                                                           -9.99931930e-05  1.11670455e-05]                                      -1.13857966e-03 -2.48777447e-04]                                    \n",
      "                                                          ...                                                                   ...                                                                  \n",
      "                                                          [ 1.10229604e-05  1.10092503e-05 -9.99523892e-05 ...  1.10179400e-05  [-6.19477574e-05  5.79200542e-04 -2.94918789e-04 ...  6.35288891e-04 \n",
      "                                                            1.10639652e-05  1.11320982e-05]                                      -6.39312965e-04 -4.21903200e-04]                                    \n",
      "                                                          [ 1.11198408e-05  1.12645148e-05 -1.00017891e-04 ...  1.11186783e-05  [ 5.47087958e-04 -5.51898064e-04 -5.29309296e-04 ...  8.03036620e-04 \n",
      "                                                            1.11770916e-05  1.09512137e-05]                                      -1.14144280e-04 -3.29303130e-04]                                    \n",
      "                                                          [ 1.10606733e-05 -9.99753802e-05  1.11799987e-05 ...  1.10715939e-05  [ 3.36325109e-02 -9.65929559e-02 -4.82348959e-02 ...  2.30934581e-02 \n",
      "                                                            1.10178589e-05  1.11150664e-05]]                                      3.31891766e-02  3.76039612e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.03258282972773141 [[-0.00327821 -0.00649676 -0.01068866 ... -0.00406279 -0.00456688\n",
      "                                        0.00137746]                                                   \n",
      "                                      [ 0.00272054 -0.00373547 -0.01468583 ...  0.02289849  0.00119606\n",
      "                                        0.01137464]                                                   \n",
      "                                      [ 0.00033084  0.01796253 -0.01441098 ...  0.00504255 -0.01959647\n",
      "                                       -0.00980507]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.01368191  0.00484243 -0.00757271 ...  0.00123237 -0.00210086\n",
      "                                       -0.00228114]                                                   \n",
      "                                      [ 0.00608616  0.01200287  0.01427247 ... -0.0026535  -0.00294727\n",
      "                                        0.00869121]                                                   \n",
      "                                      [-0.00691215  0.00287816 -0.00548449 ... -0.00549735 -0.00711796\n",
      "                                       -0.01023971]]                                                  \n",
      "Epoch 3, loss: 11.440069\n",
      "== W == -0.16511494862720372\n",
      "enter of the function =  [[ 0.00774198  0.02898223 -0.00385957 ...  0.00932845  0.00369474 [1 4 3 ... 3 3 3]\n",
      "                           -0.00676246]                                                                     \n",
      "                          [-0.00437203  0.00274017  0.00170535 ... -0.0083405  -0.00042688                  \n",
      "                            0.00216316]                                                                     \n",
      "                          [-0.0038151   0.00573093 -0.00358442 ...  0.00154299 -0.00731461                  \n",
      "                           -0.0050812 ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.00937276  0.00636443 -0.00234736 ... -0.00139314  0.00739705                  \n",
      "                           -0.00740634]                                                                     \n",
      "                          [-0.00031869 -0.01453093 -0.00325632 ... -0.00556351  0.00167377                  \n",
      "                            0.00431836]                                                                     \n",
      "                          [-0.00681402 -0.01596503  0.00759536 ... -0.00586714 -0.00845483                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.00153674]]                                                                    \n",
      "soft max = [[1.12014459e-05 1.14419121e-05 1.10722428e-05 ... 1.12192307e-05\n",
      "             1.11562026e-05 1.10401479e-05]                                 \n",
      "            [1.10665701e-05 1.11455583e-05 1.11340307e-05 ... 1.10227398e-05\n",
      "             1.11103156e-05 1.11391290e-05]                                 \n",
      "            [1.10727351e-05 1.11789419e-05 1.10752897e-05 ... 1.11322231e-05\n",
      "             1.10340537e-05 1.10587248e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.12197279e-05 1.11860260e-05 1.10889989e-05 ... 1.10995854e-05\n",
      "             1.11975829e-05 1.10330416e-05]                                 \n",
      "            [1.11115177e-05 1.09547150e-05 1.10789241e-05 ... 1.10533924e-05\n",
      "             1.11336791e-05 1.11631621e-05]                                 \n",
      "            [1.10395786e-05 1.09390162e-05 1.11998037e-05 ... 1.10500367e-05\n",
      "             1.10214796e-05 1.11321535e-05]]                                \n",
      "log =  102667.69651596798\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.407521835107554 [[ 1.12014459e-05 -9.96691990e-05  1.10722428e-05 ...  1.12192307e-05 \n",
      "                                                   1.11562026e-05  1.10401479e-05]                                    \n",
      "                                                 [ 1.10665701e-05  1.11455583e-05  1.11340307e-05 ...  1.10227398e-05 \n",
      "                                                   1.11103156e-05  1.11391290e-05]                                    \n",
      "                                                 [ 1.10727351e-05  1.11789419e-05  1.10752897e-05 ...  1.11322231e-05 \n",
      "                                                   1.10340537e-05  1.10587248e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.12197279e-05  1.11860260e-05  1.10889989e-05 ...  1.10995854e-05 \n",
      "                                                   1.11975829e-05  1.10330416e-05]                                    \n",
      "                                                 [ 1.11115177e-05  1.09547150e-05  1.10789241e-05 ...  1.10533924e-05 \n",
      "                                                   1.11336791e-05  1.11631621e-05]                                    \n",
      "                                                 [ 1.10395786e-05  1.09390162e-05  1.11998037e-05 ...  1.10500367e-05 \n",
      "                                                   1.10214796e-05  1.11321535e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.407521835107554 [[ 1.12014459e-05 -9.96691990e-05  1.10722428e-05 ...  1.12192307e-05 [[-1.72321637e-04 -1.90723673e-04  3.73744212e-04 ...  1.18389573e-04 \n",
      "                                                              1.11562026e-05  1.10401479e-05]                                      -8.13240148e-04  3.01883175e-04]                                    \n",
      "                                                            [ 1.10665701e-05  1.11455583e-05  1.11340307e-05 ...  1.10227398e-05  [-7.22322647e-04  5.29406472e-04  8.91025706e-04 ... -2.20370698e-04 \n",
      "                                                              1.11103156e-05  1.11391290e-05]                                      -1.06409707e-03  2.93444766e-04]                                    \n",
      "                                                            [ 1.10727351e-05  1.11789419e-05  1.10752897e-05 ...  1.11322231e-05  [-1.05868954e-03  1.12398976e-03  1.46946562e-03 ... -3.90406199e-04 \n",
      "                                                              1.10340537e-05  1.10587248e-05]                                      -1.14121762e-03 -2.51286690e-04]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.12197279e-05  1.11860260e-05  1.10889989e-05 ...  1.10995854e-05  [-6.30757390e-05  5.79840200e-04 -2.96021863e-04 ...  6.38422423e-04 \n",
      "                                                              1.11975829e-05  1.10330416e-05]                                      -6.41286283e-04 -4.24120389e-04]                                    \n",
      "                                                            [ 1.11115177e-05  1.09547150e-05  1.10789241e-05 ...  1.10533924e-05  [ 5.46083378e-04 -5.51402842e-04 -5.30367640e-04 ...  8.06408113e-04 \n",
      "                                                              1.11336791e-05  1.11631621e-05]                                      -1.16082088e-04 -3.31730266e-04]                                    \n",
      "                                                            [ 1.10395786e-05  1.09390162e-05  1.11998037e-05 ...  1.10500367e-05  [ 3.36355173e-02 -9.66018863e-02 -4.82398358e-02 ...  2.30956138e-02 \n",
      "                                                              1.10214796e-05  1.11321535e-05]]                                      3.31921698e-02  3.76071449e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.03323777366684696 [[-0.0033127  -0.00656364 -0.01079181 ... -0.00410227 -0.00462066\n",
      "                                        0.00139427]                                                   \n",
      "                                      [ 0.00274054 -0.00376753 -0.01482377 ...  0.02312524  0.00119741\n",
      "                                        0.01149134]                                                   \n",
      "                                      [ 0.00032357  0.0181534  -0.01454039 ...  0.00508904 -0.01980382\n",
      "                                       -0.00990561]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.01381935  0.00489664 -0.00765138 ...  0.00125105 -0.00212826\n",
      "                                       -0.00230817]                                                   \n",
      "                                      [ 0.00615249  0.01211738  0.0144099  ... -0.002672   -0.00297789\n",
      "                                        0.00877483]                                                   \n",
      "                                      [-0.00664495  0.00194101 -0.00602168 ... -0.00532139 -0.00685725\n",
      "                                       -0.00996606]]                                                  \n",
      "Epoch 4, loss: 11.440760\n",
      "== W == -0.16806004579635211\n",
      "enter of the function =  [[-0.00490369  0.00895383 -0.00073163 ... -0.01362485  0.0022053  [3 7 2 ... 9 1 9]\n",
      "                            0.00123037]                                                                     \n",
      "                          [-0.00453362  0.03485883 -0.00136413 ... -0.01591443  0.01026944                  \n",
      "                           -0.00830778]                                                                     \n",
      "                          [-0.00220802  0.00594668 -0.00640985 ... -0.00305357  0.00109627                  \n",
      "                           -0.00208326]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.00050123 -0.00457749 -0.00238491 ... -0.01120254 -0.00053944                  \n",
      "                            0.00555002]                                                                     \n",
      "                          [-0.00718161  0.01899484 -0.00483691 ...  0.00243412 -0.00968276                  \n",
      "                           -0.00688785]                                                                     \n",
      "                          [ 0.00931645 -0.01595037 -0.01538565 ...  0.02007801 -0.00623174                  \n",
      "                           -0.00878118]]                                                                    \n",
      "soft max = [[1.10607222e-05 1.12150633e-05 1.11069646e-05 ... 1.09646793e-05\n",
      "             1.11396329e-05 1.11287779e-05]                                 \n",
      "            [1.10648162e-05 1.15093853e-05 1.10999416e-05 ... 1.09396035e-05\n",
      "             1.12298276e-05 1.10231345e-05]                                 \n",
      "            [1.10905785e-05 1.11813886e-05 1.10440755e-05 ... 1.10812048e-05\n",
      "             1.11272855e-05 1.10919623e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.11095239e-05 1.10643308e-05 1.10886168e-05 ... 1.09912713e-05\n",
      "             1.11090995e-05 1.11769542e-05]                                 \n",
      "            [1.10355554e-05 1.13282411e-05 1.10614608e-05 ... 1.11421822e-05\n",
      "             1.10079883e-05 1.10387977e-05]                                 \n",
      "            [1.12191309e-05 1.09392103e-05 1.09453896e-05 ... 1.13405182e-05\n",
      "             1.10460427e-05 1.10179174e-05]]                                \n",
      "log =  102668.02171090357\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.407557967878175 [[ 1.10607222e-05  1.12150633e-05  1.11069646e-05 ...  1.09646793e-05 \n",
      "                                                   1.11396329e-05  1.11287779e-05]                                    \n",
      "                                                 [ 1.10648162e-05  1.15093853e-05  1.10999416e-05 ... -1.00171508e-04 \n",
      "                                                   1.12298276e-05  1.10231345e-05]                                    \n",
      "                                                 [ 1.10905785e-05  1.11813886e-05 -1.00067036e-04 ...  1.10812048e-05 \n",
      "                                                   1.11272855e-05  1.10919623e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11095239e-05  1.10643308e-05  1.10886168e-05 ...  1.09912713e-05 \n",
      "                                                   1.11090995e-05 -9.99341569e-05]                                    \n",
      "                                                 [ 1.10355554e-05 -9.97828700e-05  1.10614608e-05 ...  1.11421822e-05 \n",
      "                                                   1.10079883e-05  1.10387977e-05]                                    \n",
      "                                                 [ 1.12191309e-05  1.09392103e-05  1.09453896e-05 ...  1.13405182e-05 \n",
      "                                                   1.10460427e-05 -1.00093194e-04]]                                   \n",
      "loss , grand (prediction), grad by W =  11.407557967878175 [[ 1.10607222e-05  1.12150633e-05  1.11069646e-05 ...  1.09646793e-05 [[-1.73899874e-04 -1.90932988e-04  3.72991208e-04 ...  1.21073550e-04 \n",
      "                                                              1.11396329e-05  1.11287779e-05]                                      -8.15789801e-04  2.99721078e-04]                                    \n",
      "                                                            [ 1.10648162e-05  1.15093853e-05  1.10999416e-05 ... -1.00171508e-04  [-7.23888087e-04  5.29258870e-04  8.90325888e-04 ... -2.17545137e-04 \n",
      "                                                              1.12298276e-05  1.10231345e-05]                                      -1.06679619e-03  2.91119658e-04]                                    \n",
      "                                                            [ 1.10905785e-05  1.11813886e-05 -1.00067036e-04 ...  1.10812048e-05  [-1.06014505e-03  1.12378862e-03  1.46875738e-03 ... -3.87309533e-04 \n",
      "                                                              1.11272855e-05  1.10919623e-05]                                      -1.14389805e-03 -2.53838755e-04]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.11095239e-05  1.10643308e-05  1.10886168e-05 ...  1.09912713e-05  [-6.42244538e-05  5.80485233e-04 -2.97144131e-04 ...  6.41610781e-04 \n",
      "                                                              1.11090995e-05 -9.99341569e-05]                                      -6.43294756e-04 -4.26376958e-04]                                    \n",
      "                                                            [ 1.10355554e-05 -9.97828700e-05  1.10614608e-05 ...  1.11421822e-05  [ 5.45059180e-04 -5.50903781e-04 -5.31444836e-04 ...  8.09837416e-04 \n",
      "                                                              1.10079883e-05  1.10387977e-05]                                      -1.18054942e-04 -3.34199312e-04]                                    \n",
      "                                                            [ 1.12191309e-05  1.09392103e-05  1.09453896e-05 ...  1.13405182e-05  [ 3.36385504e-02 -9.66109098e-02 -4.82448281e-02 ...  2.30977917e-02 \n",
      "                                                              1.10460427e-05 -1.00093194e-04]]                                      3.31951913e-02  3.76103594e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.03390595640521369 [[-0.00334755 -0.00663118 -0.01089599 ... -0.0041421  -0.004675  \n",
      "                                        0.00141123]                                                   \n",
      "                                      [ 0.00276072 -0.00379991 -0.0149631  ...  0.02335429  0.00119874\n",
      "                                        0.01160919]                                                   \n",
      "                                      [ 0.00031622  0.01834617 -0.01467109 ...  0.00513603 -0.02001327\n",
      "                                       -0.01000718]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.01395817  0.00495141 -0.00773086 ...  0.00126994 -0.00215596\n",
      "                                       -0.00233549]                                                   \n",
      "                                      [ 0.00621948  0.01223304  0.01454869 ... -0.00269066 -0.00300883\n",
      "                                        0.00885926]                                                   \n",
      "                                      [-0.00637504  0.0009944  -0.0065643  ... -0.00514365 -0.0065939 \n",
      "                                       -0.00968965]]                                                  \n",
      "Epoch 5, loss: 11.441464\n",
      "== W == -0.17105669158169623\n",
      "enter of the function =  [[-0.00164403 -0.01116371 -0.00843057 ... -0.00542719 -0.00757412 [7 5 2 ... 5 2 4]\n",
      "                           -0.01275469]                                                                     \n",
      "                          [-0.00645721 -0.01019882  0.00450081 ... -0.00975347 -0.01433901                  \n",
      "                            0.00121889]                                                                     \n",
      "                          [-0.0145267   0.03379089 -0.0171938  ...  0.01036362 -0.00920017                  \n",
      "                            0.00376655]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.00635388 -0.00106992 -0.01838389 ...  0.00954644 -0.02011068                  \n",
      "                           -0.01238978]                                                                     \n",
      "                          [-0.00454895 -0.00180666  0.00045584 ... -0.004132    0.00125323                  \n",
      "                            0.00370653]                                                                     \n",
      "                          [-0.00431486 -0.01636891  0.00624728 ... -0.00964134 -0.01095621                  \n",
      "                            0.00509564]]                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max = [[1.10968693e-05 1.09917318e-05 1.10218148e-05 ... 1.10549673e-05\n",
      "             1.10312585e-05 1.09742581e-05]                                 \n",
      "            [1.10435864e-05 1.10023427e-05 1.11652676e-05 ... 1.10072438e-05\n",
      "             1.09568851e-05 1.11286842e-05]                                 \n",
      "            [1.09548288e-05 1.14971357e-05 1.09256501e-05 ... 1.12309197e-05\n",
      "             1.10133357e-05 1.11570725e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.10447275e-05 1.11032419e-05 1.09126554e-05 ... 1.12217458e-05\n",
      "             1.08938277e-05 1.09782634e-05]                                 \n",
      "            [1.10646805e-05 1.10950647e-05 1.11201956e-05 ... 1.10692949e-05\n",
      "             1.11290664e-05 1.11564029e-05]                                 \n",
      "            [1.10672709e-05 1.09346663e-05 1.11847845e-05 ... 1.10084781e-05\n",
      "             1.09940129e-05 1.11719110e-05]]                                \n",
      "log =  102668.35094025751\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.407594548917501 [[ 1.10968693e-05  1.09917318e-05  1.10218148e-05 ... -1.00056144e-04 \n",
      "                                                   1.10312585e-05  1.09742581e-05]                                    \n",
      "                                                 [ 1.10435864e-05  1.10023427e-05  1.11652676e-05 ...  1.10072438e-05 \n",
      "                                                   1.09568851e-05  1.11286842e-05]                                    \n",
      "                                                 [ 1.09548288e-05  1.14971357e-05 -1.00185461e-04 ...  1.12309197e-05 \n",
      "                                                   1.10133357e-05  1.11570725e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.10447275e-05  1.11032419e-05  1.09126554e-05 ...  1.12217458e-05 \n",
      "                                                   1.08938277e-05  1.09782634e-05]                                    \n",
      "                                                 [ 1.10646805e-05  1.10950647e-05 -9.99909155e-05 ...  1.10692949e-05 \n",
      "                                                   1.11290664e-05  1.11564029e-05]                                    \n",
      "                                                 [ 1.10672709e-05  1.09346663e-05  1.11847845e-05 ...  1.10084781e-05 \n",
      "                                                   1.09940129e-05  1.11719110e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.407594548917501 [[ 1.10968693e-05  1.09917318e-05  1.10218148e-05 ... -1.00056144e-04 [[-1.75503400e-04 -1.91145575e-04  3.72222957e-04 ...  1.23806847e-04 \n",
      "                                                              1.10312585e-05  1.09742581e-05]                                      -8.18380324e-04  2.97520611e-04]                                    \n",
      "                                                            [ 1.10435864e-05  1.10023427e-05  1.11652676e-05 ...  1.10072438e-05  [-7.25478991e-04  5.29108532e-04  8.89611112e-04 ... -2.14668022e-04 \n",
      "                                                              1.09568851e-05  1.11286842e-05]                                      -1.06953822e-03  2.88753923e-04]                                    \n",
      "                                                            [ 1.09548288e-05  1.14971357e-05 -1.00185461e-04 ...  1.12309197e-05  [-1.06162516e-03  1.12358412e-03  1.46803386e-03 ... -3.84157685e-04 \n",
      "                                                              1.10133357e-05  1.11570725e-05]                                      -1.14662164e-03 -2.56434369e-04]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.10447275e-05  1.11032419e-05  1.09126554e-05 ...  1.12217458e-05  [-6.53942708e-05  5.81135673e-04 -2.98285919e-04 ...  6.44854928e-04 \n",
      "                                                              1.08938277e-05  1.09782634e-05]                                      -6.45338997e-04 -4.28673593e-04]                                    \n",
      "                                                            [ 1.10646805e-05  1.10950647e-05 -9.99909155e-05 ...  1.10692949e-05  [ 5.44015004e-04 -5.50400862e-04 -5.32541207e-04 ...  8.13325533e-04 \n",
      "                                                              1.11290664e-05  1.11564029e-05]                                      -1.20063459e-04 -3.36710982e-04]                                    \n",
      "                                                            [ 1.10672709e-05  1.09346663e-05  1.11847845e-05 ...  1.10084781e-05  [ 3.36416106e-02 -9.66200273e-02 -4.82498736e-02 ...  2.30999921e-02 \n",
      "                                                              1.09940129e-05  1.11719110e-05]]                                      3.31982412e-02  3.76136051e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.034587645063436784 [[-3.38276487e-03 -6.69940149e-03 -1.10012156e-02 ... -4.18231408e-03\n",
      "                                        -4.72990599e-03  1.42834202e-03]                                   \n",
      "                                       [ 2.78108810e-03 -3.83262037e-03 -1.51038235e-02 ...  2.35856556e-02\n",
      "                                         1.20005973e-03  1.17281897e-02]                                   \n",
      "                                       [ 3.08783326e-04  1.85408712e-02 -1.48031183e-02 ...  5.18351792e-03\n",
      "                                        -2.02248435e-02 -1.01097910e-02]                                   \n",
      "                                       ...                                                                 \n",
      "                                       [-1.40983946e-02  5.00672619e-03 -7.81113847e-03 ...  1.28905989e-03\n",
      "                                        -2.18395217e-03 -2.36310838e-03]                                   \n",
      "                                       [ 6.28712474e-03  1.23498582e-02  1.46888643e-02 ... -2.70946752e-03\n",
      "                                        -3.04009647e-03  8.94450714e-03]                                   \n",
      "                                       [-6.10240733e-03  3.82362989e-05 -7.11238932e-03 ... -4.96410645e-03\n",
      "                                        -6.32788706e-03 -9.41044548e-03]]                                  \n",
      "Epoch 6, loss: 11.442182\n",
      "== W == -0.17410576687968815\n",
      "enter of the function =  [[ 0.00858233 -0.00805173  0.0022035  ...  0.00101908 -0.00235457 [0 7 4 ... 6 9 6]\n",
      "                            0.00219112]                                                                     \n",
      "                          [-0.00474272  0.00534185 -0.00739643 ...  0.00111397 -0.00813434                  \n",
      "                           -0.00534752]                                                                     \n",
      "                          [-0.00467236  0.01146853  0.00318765 ...  0.00259598 -0.0036399                   \n",
      "                            0.0014893 ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.00765506 -0.00043828  0.00846847 ... -0.00039181  0.01015562                  \n",
      "                            0.00441499]                                                                     \n",
      "                          [ 0.00071208  0.00332962  0.01376433 ... -0.01293192  0.01178535                  \n",
      "                            0.0030256 ]                                                                     \n",
      "                          [-0.00524467 -0.01455066 -0.0031842  ... -0.00840038  0.00306722                  \n",
      "                            0.00621562]]                                                                    \n",
      "soft max = [[1.12109662e-05 1.10260247e-05 1.11396808e-05 ... 1.11264946e-05\n",
      "             1.10890210e-05 1.11395430e-05]                                 \n",
      "            [1.10625703e-05 1.11746960e-05 1.10332524e-05 ... 1.11275505e-05\n",
      "             1.10251138e-05 1.10558817e-05]                                 \n",
      "            [1.10633487e-05 1.12433699e-05 1.11506494e-05 ... 1.11440539e-05\n",
      "             1.10747771e-05 1.11317278e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.12005753e-05 1.11102912e-05 1.12096897e-05 ... 1.11108075e-05\n",
      "             1.12286181e-05 1.11643435e-05]                                 \n",
      "            [1.11230793e-05 1.11522326e-05 1.12692121e-05 ... 1.09723467e-05\n",
      "             1.12469327e-05 1.11488426e-05]                                 \n",
      "            [1.10570189e-05 1.09545997e-05 1.10798251e-05 ... 1.10221811e-05\n",
      "             1.11493066e-05 1.11844644e-05]]                                \n",
      "log =  102668.68426626557\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.407631585140619 [[-9.99001449e-05  1.10260247e-05  1.11396808e-05 ...  1.11264946e-05 \n",
      "                                                   1.10890210e-05  1.11395430e-05]                                    \n",
      "                                                 [ 1.10625703e-05  1.11746960e-05  1.10332524e-05 ... -9.99835606e-05 \n",
      "                                                   1.10251138e-05  1.10558817e-05]                                    \n",
      "                                                 [ 1.10633487e-05  1.12433699e-05  1.11506494e-05 ...  1.11440539e-05 \n",
      "                                                   1.10747771e-05  1.11317278e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.12005753e-05  1.11102912e-05  1.12096897e-05 ...  1.11108075e-05 \n",
      "                                                   1.12286181e-05  1.11643435e-05]                                    \n",
      "                                                 [ 1.11230793e-05  1.11522326e-05  1.12692121e-05 ...  1.09723467e-05 \n",
      "                                                   1.12469327e-05 -9.99622685e-05]                                    \n",
      "                                                 [ 1.10570189e-05  1.09545997e-05  1.10798251e-05 ...  1.10221811e-05 \n",
      "                                                   1.11493066e-05  1.11844644e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.407631585140619 [[-9.99001449e-05  1.10260247e-05  1.11396808e-05 ...  1.11264946e-05 [[-1.77132629e-04 -1.91361487e-04  3.71439176e-04 ...  1.26590357e-04 \n",
      "                                                              1.10890210e-05  1.11395430e-05]                                      -8.21012384e-04  2.95281109e-04]                                    \n",
      "                                                            [ 1.10625703e-05  1.11746960e-05  1.10332524e-05 ... -9.99835606e-05  [-7.27095779e-04  5.28955411e-04  8.88881095e-04 ... -2.11738423e-04 \n",
      "                                                              1.10251138e-05  1.10558817e-05]                                      -1.07232385e-03  2.86346863e-04]                                    \n",
      "                                                            [ 1.10633487e-05  1.12433699e-05  1.11506494e-05 ...  1.11440539e-05  [-1.06313029e-03  1.12337620e-03  1.46729478e-03 ... -3.80949674e-04 \n",
      "                                                              1.10747771e-05  1.11317278e-05]                                      -1.14938912e-03 -2.59074272e-04]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.12005753e-05  1.11102912e-05  1.12096897e-05 ...  1.11108075e-05  [-6.65855653e-05  5.81791556e-04 -2.99447557e-04 ...  6.48155848e-04 \n",
      "                                                              1.12286181e-05  1.11643435e-05]                                      -6.47419630e-04 -4.31010988e-04]                                    \n",
      "                                                            [ 1.11230793e-05  1.11522326e-05  1.12692121e-05 ...  1.09723467e-05  [ 5.42950482e-04 -5.49894067e-04 -5.33657084e-04 ...  8.16873490e-04 \n",
      "                                                              1.12469327e-05 -9.99622685e-05]                                      -1.22108266e-04 -3.39266007e-04]                                    \n",
      "                                                            [ 1.10570189e-05  1.09545997e-05  1.10798251e-05 ...  1.10221811e-05  [ 3.36446978e-02 -9.66292400e-02 -4.82549728e-02 ...  2.31022151e-02 \n",
      "                                                              1.11493066e-05  1.11844644e-05]]                                      3.32013197e-02  3.76168821e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.035283112149946366 [[-0.00341835 -0.00676831 -0.01110751 ... -0.0042229  -0.00478539\n",
      "                                         0.0014456 ]                                                   \n",
      "                                       [ 0.00280164 -0.00386566 -0.01524597 ...  0.02381937  0.00120136\n",
      "                                         0.01184836]                                                   \n",
      "                                       [ 0.00030125  0.01873752 -0.01493647 ...  0.00523151 -0.02043856\n",
      "                                        -0.01021345]                                                   \n",
      "                                       ...                                                             \n",
      "                                       [-0.01424003  0.0050626  -0.00789223 ...  0.0013084  -0.00221225\n",
      "                                        -0.00239103]                                                   \n",
      "                                       [ 0.00635544  0.01246785  0.01483043 ... -0.00272843 -0.0030717 \n",
      "                                         0.00903059]                                                   \n",
      "                                       [-0.00582702 -0.00092758 -0.00766601 ... -0.00478275 -0.00605918\n",
      "                                        -0.00912841]]                                                  \n",
      "Epoch 7, loss: 11.442915\n",
      "== W == -0.17720816717692597\n",
      "enter of the function =  [[-0.02135109 -0.00039023  0.00342297 ... -0.00992336 -0.01344479 [2 5 6 ... 8 8 5]\n",
      "                           -0.0029949 ]                                                                     \n",
      "                          [ 0.01079666  0.01024648 -0.00828331 ...  0.01602485  0.0128251                   \n",
      "                           -0.01012029]                                                                     \n",
      "                          [ 0.00166971 -0.00032984  0.00516205 ... -0.00784876  0.00843641                  \n",
      "                            0.00561412]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.00140258 -0.00222227 -0.00359514 ...  0.00510221 -0.00258874                  \n",
      "                           -0.00110684]                                                                     \n",
      "                          [ 0.0127059   0.01431708 -0.00697242 ... -0.00572714 -0.001876                    \n",
      "                            0.00213675]                                                                     \n",
      "                          [ 0.00503326  0.0001556  -0.00128265 ...  0.0029922   0.00115965                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.00178031]]                                                                    \n",
      "soft max = [[1.08803892e-05 1.11108584e-05 1.11533073e-05 ... 1.10054405e-05\n",
      "             1.09667537e-05 1.10819560e-05]                                 \n",
      "            [1.12358523e-05 1.12296722e-05 1.10235048e-05 ... 1.12947492e-05\n",
      "             1.12586667e-05 1.10032734e-05]                                 \n",
      "            [1.11337698e-05 1.11115294e-05 1.11727207e-05 ... 1.10282961e-05\n",
      "             1.12093642e-05 1.11777726e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.10996161e-05 1.10905215e-05 1.10753062e-05 ... 1.11720521e-05\n",
      "             1.10864580e-05 1.11028991e-05]                                 \n",
      "            [1.12573247e-05 1.12754768e-05 1.10379648e-05 ... 1.10517187e-05\n",
      "             1.10943625e-05 1.11389709e-05]                                 \n",
      "            [1.11712818e-05 1.11169248e-05 1.11009474e-05 ... 1.11485038e-05\n",
      "             1.11280924e-05 1.11350012e-05]]                                \n",
      "log =  102669.02175245652\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.40766908360628 [[ 1.08803892e-05  1.11108584e-05 -9.99578038e-05 ...  1.10054405e-05 \n",
      "                                                  1.09667537e-05  1.10819560e-05]                                    \n",
      "                                                [ 1.12358523e-05  1.12296722e-05  1.10235048e-05 ...  1.12947492e-05 \n",
      "                                                  1.12586667e-05  1.10032734e-05]                                    \n",
      "                                                [ 1.11337698e-05  1.11115294e-05  1.11727207e-05 ...  1.10282961e-05 \n",
      "                                                  1.12093642e-05  1.11777726e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [ 1.10996161e-05  1.10905215e-05  1.10753062e-05 ...  1.11720521e-05 \n",
      "                                                 -1.00024653e-04  1.11028991e-05]                                    \n",
      "                                                [ 1.12573247e-05  1.12754768e-05  1.10379648e-05 ...  1.10517187e-05 \n",
      "                                                 -1.00016749e-04  1.11389709e-05]                                    \n",
      "                                                [ 1.11712818e-05  1.11169248e-05  1.11009474e-05 ...  1.11485038e-05 \n",
      "                                                  1.11280924e-05  1.11350012e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.40766908360628 [[ 1.08803892e-05  1.11108584e-05 -9.99578038e-05 ...  1.10054405e-05 [[-1.78787981e-04 -1.91580775e-04  3.70639579e-04 ...  1.29424990e-04 \n",
      "                                                             1.09667537e-05  1.10819560e-05]                                      -8.23686659e-04  2.93001893e-04]                                    \n",
      "                                                           [ 1.12358523e-05  1.12296722e-05  1.10235048e-05 ...  1.12947492e-05  [-7.28738877e-04  5.28799460e-04  8.88135548e-04 ... -2.08755392e-04 \n",
      "                                                             1.12586667e-05  1.10032734e-05]                                      -1.07515378e-03  2.83897766e-04]                                    \n",
      "                                                           [ 1.11337698e-05  1.11115294e-05  1.11727207e-05 ...  1.10282961e-05  [-1.06466086e-03  1.12316480e-03  1.46653984e-03 ... -3.77684499e-04 \n",
      "                                                             1.12093642e-05  1.11777726e-05]                                      -1.15220118e-03 -2.61759215e-04]                                    \n",
      "                                                           ...                                                                   ...                                                                  \n",
      "                                                           [ 1.10996161e-05  1.10905215e-05  1.10753062e-05 ...  1.11720521e-05  [-6.77987192e-05  5.82452915e-04 -3.00629384e-04 ...  6.51514541e-04 \n",
      "                                                            -1.00024653e-04  1.11028991e-05]                                      -6.49537289e-04 -4.33389851e-04]                                    \n",
      "                                                           [ 1.12573247e-05  1.12754768e-05  1.10379648e-05 ...  1.10517187e-05  [ 5.41865243e-04 -5.49383380e-04 -5.34792803e-04 ...  8.20482326e-04 \n",
      "                                                            -1.00016749e-04  1.11389709e-05]                                      -1.24190002e-04 -3.41865126e-04]                                    \n",
      "                                                           [ 1.11712818e-05  1.11169248e-05  1.11009474e-05 ...  1.11485038e-05  [ 3.36478123e-02 -9.66385489e-02 -4.82601266e-02 ...  2.31044610e-02 \n",
      "                                                             1.11280924e-05  1.11350012e-05]]                                      3.32044272e-02  3.76201908e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.035992635669754774 [[-0.0034543  -0.0068379  -0.01121487 ... -0.00426386 -0.00484145\n",
      "                                         0.00146301]                                                   \n",
      "                                       [ 0.00282239 -0.00389902 -0.01538954 ...  0.02405544  0.00120266\n",
      "                                         0.01196971]                                                   \n",
      "                                       [ 0.00029364  0.01893612 -0.01507116 ...  0.00528002 -0.02065444\n",
      "                                        -0.01031818]                                                   \n",
      "                                       ...                                                             \n",
      "                                       [-0.0143831   0.00511905 -0.00797415 ...  0.00132796 -0.00224084\n",
      "                                        -0.00241925]                                                   \n",
      "                                       [ 0.00642442  0.01258703  0.0149734  ... -0.00274754 -0.00310364\n",
      "                                         0.0091175 ]                                                   \n",
      "                                       [-0.00554884 -0.00190315 -0.00822522 ... -0.00459955 -0.00578776\n",
      "                                        -0.00884353]]                                                  \n",
      "Epoch 8, loss: 11.443662\n",
      "== W == -0.1803648027780096\n",
      "enter of the function =  [[ 0.00080001  0.00153766 -0.00537924 ...  0.01704954  0.01145574 [1 7 9 ... 1 5 4]\n",
      "                           -0.00546018]                                                                     \n",
      "                          [-0.0039538   0.00497888 -0.00502621 ... -0.00516841 -0.02111966                  \n",
      "                           -0.01149017]                                                                     \n",
      "                          [ 0.00327062  0.01015705  0.00996294 ... -0.01317911  0.00753276                  \n",
      "                            0.02370173]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.00050988 -0.00842913 -0.00100196 ... -0.00357382 -0.0012597                   \n",
      "                           -0.00136002]                                                                     \n",
      "                          [-0.00230865  0.01979494 -0.00541025 ...  0.00757159  0.01442899                  \n",
      "                           -0.00495878]                                                                     \n",
      "                          [-0.00077242 -0.01553481  0.00457119 ... -0.01289345  0.00073834                  \n",
      "                            0.0116705 ]]                                                                    \n",
      "soft max = [[1.11241241e-05 1.11323329e-05 1.10555973e-05 ... 1.13063625e-05\n",
      "             1.12432936e-05 1.10547025e-05]                                 \n",
      "            [1.10713677e-05 1.11707076e-05 1.10595009e-05 ... 1.10579284e-05\n",
      "             1.08829400e-05 1.09882433e-05]                                 \n",
      "            [1.11516415e-05 1.12287015e-05 1.12265221e-05 ... 1.09697005e-05\n",
      "             1.11992727e-05 1.13818253e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.11208971e-05 1.10219304e-05 1.11040968e-05 ... 1.10755754e-05\n",
      "             1.11012352e-05 1.11001216e-05]                                 \n",
      "            [1.10895966e-05 1.13374457e-05 1.10552545e-05 ... 1.11997077e-05\n",
      "             1.12767725e-05 1.10602468e-05]                                 \n",
      "            [1.11066459e-05 1.09438896e-05 1.11661544e-05 ... 1.09728346e-05\n",
      "             1.11234381e-05 1.12457085e-05]]                                \n",
      "log =  102669.36346368703\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.40770705152078 [[ 1.11241241e-05 -9.99787782e-05  1.10555973e-05 ...  1.13063625e-05 \n",
      "                                                  1.12432936e-05  1.10547025e-05]                                    \n",
      "                                                [ 1.10713677e-05  1.11707076e-05  1.10595009e-05 ... -1.00053183e-04 \n",
      "                                                  1.08829400e-05  1.09882433e-05]                                    \n",
      "                                                [ 1.11516415e-05  1.12287015e-05  1.12265221e-05 ...  1.09697005e-05 \n",
      "                                                  1.11992727e-05 -9.97292858e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [ 1.11208971e-05 -1.00089181e-04  1.11040968e-05 ...  1.10755754e-05 \n",
      "                                                  1.11012352e-05  1.11001216e-05]                                    \n",
      "                                                [ 1.10895966e-05  1.13374457e-05  1.10552545e-05 ...  1.11997077e-05 \n",
      "                                                  1.12767725e-05  1.10602468e-05]                                    \n",
      "                                                [ 1.11066459e-05  1.09438896e-05  1.11661544e-05 ...  1.09728346e-05 \n",
      "                                                  1.11234381e-05  1.12457085e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.40770705152078 [[ 1.11241241e-05 -9.99787782e-05  1.10555973e-05 ...  1.13063625e-05 [[-1.80469882e-04 -1.91803493e-04  3.69823874e-04 ...  1.32311672e-04 \n",
      "                                                             1.12432936e-05  1.10547025e-05]                                      -8.26403836e-04  2.90682274e-04]                                    \n",
      "                                                           [ 1.10713677e-05  1.11707076e-05  1.10595009e-05 ... -1.00053183e-04  [-7.30408719e-04  5.28640628e-04  8.87374179e-04 ... -2.05717965e-04 \n",
      "                                                             1.08829400e-05  1.09882433e-05]                                      -1.07802874e-03  2.81405908e-04]                                    \n",
      "                                                           [ 1.11516415e-05  1.12287015e-05  1.12265221e-05 ...  1.09697005e-05  [-1.06621730e-03  1.12294987e-03  1.46576873e-03 ... -3.74361143e-04 \n",
      "                                                             1.11992727e-05 -9.97292858e-05]                                      -1.15505857e-03 -2.64489963e-04]                                    \n",
      "                                                           ...                                                                   ...                                                                  \n",
      "                                                           [ 1.11208971e-05 -1.00089181e-04  1.11040968e-05 ...  1.10755754e-05  [-6.90341211e-05  5.83119783e-04 -3.01831742e-04 ...  6.54932026e-04 \n",
      "                                                             1.11012352e-05  1.11001216e-05]                                      -6.51692619e-04 -4.35810903e-04]                                    \n",
      "                                                           [ 1.10895966e-05  1.13374457e-05  1.10552545e-05 ...  1.11997077e-05  [ 5.40758907e-04 -5.48868782e-04 -5.35948705e-04 ...  8.24153102e-04 \n",
      "                                                             1.12767725e-05  1.10602468e-05]                                      -1.26309315e-04 -3.44509092e-04]                                    \n",
      "                                                           [ 1.11066459e-05  1.09438896e-05  1.11661544e-05 ...  1.09728346e-05  [ 3.36509542e-02 -9.66479552e-02 -4.82653355e-02 ...  2.31067302e-02 \n",
      "                                                             1.11234381e-05  1.12457085e-05]]                                      3.32075638e-02  3.76235313e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.036716499235413605 [[-0.00349063 -0.0069082  -0.01132331 ... -0.00430521 -0.0048981 \n",
      "                                         0.00148057]                                                   \n",
      "                                       [ 0.00284333 -0.00393272 -0.01553455 ...  0.02429391  0.00120393\n",
      "                                         0.01209224]                                                   \n",
      "                                       [ 0.00028593  0.01913672 -0.01520721 ...  0.00532904 -0.0208725 \n",
      "                                        -0.01042398]                                                   \n",
      "                                       ...                                                             \n",
      "                                       [-0.01452761  0.00517606 -0.0080569  ...  0.00134776 -0.00226975\n",
      "                                        -0.00244777]                                                   \n",
      "                                       [ 0.00649408  0.01270741  0.01511778 ... -0.00276682 -0.00313591\n",
      "                                         0.00920525]                                                   \n",
      "                                       [-0.00526785 -0.00288857 -0.00879008 ... -0.0044145  -0.0055136 \n",
      "                                        -0.00855576]]                                                  \n",
      "Epoch 9, loss: 11.444424\n",
      "== W == -0.183576599036464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'THERE WERE 10 EPOCHS   !!!!!\\n'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function \n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "'''THERE WERE 10 EPOCHS   !!!!!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.438259239499832, 11.438913775478367, 11.439581294729212, 11.440262058362324, 11.440956332804436, 11.441664389908976, 11.442386507068308, 11.4431229673284, 11.443874059505866, 11.444640078307525]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21aeb9b0>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "print(loss_history)\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.105\n",
      "enter of the function =  [[ 0.00036065  0.00741622  0.00767822 ... -0.01749723  0.01330699 [8 9 1 ... 2 3 5]\n",
      "                            0.00738546]                                                                     \n",
      "                          [-0.04605811  0.00709856 -0.05604387 ...  0.10508821 -0.06422001                  \n",
      "                           -0.08528427]                                                                     \n",
      "                          [-0.00661998 -0.01521634 -0.01648008 ... -0.00855813 -0.00954867                  \n",
      "                            0.00645837]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.05272406 -0.03365243  0.0381907  ... -0.06392462  0.05840768                  \n",
      "                            0.03319325]                                                                     \n",
      "                          [ 0.01711638 -0.00856228  0.00846979 ... -0.04517962 -0.00169259                  \n",
      "                            0.02445804]                                                                     \n",
      "                          [-0.00173948  0.01535566 -0.0728482  ...  0.07762971 -0.00621374                  \n",
      "                           -0.06452866]]                                                                    \n",
      "soft max = [[1.11141181e-05 1.11928119e-05 1.11957447e-05 ... 1.09174052e-05\n",
      "             1.12589407e-05 1.11924676e-05]                                 \n",
      "            [1.06100052e-05 1.11892569e-05 1.05045835e-05 ... 1.23412062e-05\n",
      "             1.04190467e-05 1.02018725e-05]                                 \n",
      "            [1.10368047e-05 1.09423350e-05 1.09285155e-05 ... 1.10154345e-05\n",
      "             1.10045286e-05 1.11820959e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.17115977e-05 1.07424493e-05 1.15426197e-05 ... 1.04221248e-05\n",
      "             1.17783515e-05 1.14850800e-05]                                 \n",
      "            [1.13019122e-05 1.10153887e-05 1.12046104e-05 ... 1.06193301e-05\n",
      "             1.10913216e-05 1.13851923e-05]                                 \n",
      "            [1.10908016e-05 1.12820302e-05 1.03295359e-05 ... 1.20069454e-05\n",
      "             1.10412892e-05 1.04158314e-05]]                                \n",
      "log =  102722.69542979677\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.413632825532975 [[ 1.11141181e-05  1.11928119e-05  1.11957447e-05 ...  1.09174052e-05 \n",
      "                                                  -9.98521704e-05  1.11924676e-05]                                    \n",
      "                                                 [ 1.06100052e-05  1.11892569e-05  1.05045835e-05 ...  1.23412062e-05 \n",
      "                                                   1.04190467e-05 -1.00909239e-04]                                    \n",
      "                                                 [ 1.10368047e-05 -1.00168776e-04  1.09285155e-05 ...  1.10154345e-05 \n",
      "                                                   1.10045286e-05  1.11820959e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.17115977e-05  1.07424493e-05 -9.95684914e-05 ...  1.04221248e-05 \n",
      "                                                   1.17783515e-05  1.14850800e-05]                                    \n",
      "                                                 [ 1.13019122e-05  1.10153887e-05  1.12046104e-05 ...  1.06193301e-05 \n",
      "                                                   1.10913216e-05  1.13851923e-05]                                    \n",
      "                                                 [ 1.10908016e-05  1.12820302e-05  1.03295359e-05 ...  1.20069454e-05 \n",
      "                                                   1.10412892e-05  1.04158314e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.413632825532975 [[ 1.11141181e-05  1.11928119e-05  1.11957447e-05 ...  1.09174052e-05 [[-0.00046829 -0.00022897  0.00020662 ...  0.00068279 -0.00129098 \n",
      "                                                             -9.98521704e-05  1.11924676e-05]                                      -0.00013426]                                                    \n",
      "                                                            [ 1.06100052e-05  1.11892569e-05  1.05045835e-05 ...  1.23412062e-05  [-0.00101902  0.00049903  0.00072915 ...  0.00037064 -0.00156675 \n",
      "                                                              1.04190467e-05 -1.00909239e-04]                                      -0.00017023]                                                    \n",
      "                                                            [ 1.10368047e-05 -1.00168776e-04  1.09285155e-05 ...  1.10154345e-05  [-0.00134233  0.00108562  0.00130452 ...  0.00024572 -0.00164511 \n",
      "                                                              1.10045286e-05  1.11820959e-05]                                      -0.00075153]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.17115977e-05  1.07424493e-05 -9.95684914e-05 ...  1.04221248e-05  [-0.00030144  0.00065934 -0.00051795 ...  0.00128466 -0.0010479  \n",
      "                                                              1.17783515e-05  1.14850800e-05]                                      -0.00087933]                                                    \n",
      "                                                            [ 1.13019122e-05  1.10153887e-05  1.12046104e-05 ...  1.06193301e-05  [ 0.00032379 -0.00049252 -0.00074702 ...  0.00149098 -0.00051988 \n",
      "                                                              1.10913216e-05  1.13851923e-05]                                      -0.00081971]                                                    \n",
      "                                                            [ 1.10908016e-05  1.12820302e-05  1.03295359e-05 ...  1.20069454e-05  [ 0.03399235 -0.09791243 -0.04898579 ...  0.02341245  0.03357567 \n",
      "                                                              1.10412892e-05  1.04158314e-05]]                                      0.0380285 ]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.188580603917184 [[-0.00823834 -0.01587855 -0.02520199 ... -0.00935232 -0.01230794\n",
      "                                      0.00354505]                                                   \n",
      "                                    [ 0.00539091 -0.00823797 -0.03406935 ...  0.05490221  0.00116279\n",
      "                                      0.02751793]                                                   \n",
      "                                    [-0.00080955  0.04467446 -0.03260067 ...  0.01182333 -0.04885511\n",
      "                                     -0.02410108]                                                   \n",
      "                                    ...                                                             \n",
      "                                    [-0.03302982  0.01247635 -0.01868638 ...  0.00412361 -0.00611235\n",
      "                                     -0.00626173]                                                   \n",
      "                                    [ 0.01528226  0.02807004  0.03342504 ... -0.00495113 -0.00740678\n",
      "                                      0.02019025]                                                   \n",
      "                                    [ 0.0307003  -0.12899163 -0.08106724 ...  0.01929657  0.02959114\n",
      "                                      0.02829601]]                                                  \n",
      "Epoch 0, loss: 11.602213\n",
      "== W == -0.7629216671375418\n",
      "enter of the function =  [[ 0.02968196 -0.0305608   0.01618833 ... -0.05450009  0.04922342 [6 9 5 ... 5 1 1]\n",
      "                            0.03546796]                                                                     \n",
      "                          [ 0.00171038 -0.01613364 -0.00991427 ... -0.03267527 -0.00247215                  \n",
      "                            0.01593426]                                                                     \n",
      "                          [-0.06659464 -0.01573447 -0.04482359 ...  0.07015256 -0.05712338                  \n",
      "                           -0.02880081]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.04667359 -0.03142819  0.0223683  ... -0.05841785  0.05196271                  \n",
      "                            0.05111679]                                                                     \n",
      "                          [ 0.00027696 -0.01882181 -0.04126313 ...  0.09726841 -0.04007378                  \n",
      "                           -0.04255784]                                                                     \n",
      "                          [ 0.03026295 -0.01247282  0.01106132 ... -0.02266499  0.05524069                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.02633483]]                                                                    \n",
      "soft max = [[1.14444756e-05 1.07753852e-05 1.12910853e-05 ... 1.05204932e-05\n",
      "             1.16703169e-05 1.15108854e-05]                                 \n",
      "            [1.11287912e-05 1.09319702e-05 1.10001720e-05 ... 1.07526250e-05\n",
      "             1.10823420e-05 1.12882169e-05]                                 \n",
      "            [1.03940190e-05 1.09363347e-05 1.06227889e-05 ... 1.19171405e-05\n",
      "             1.04929310e-05 1.07943664e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.16405974e-05 1.07660427e-05 1.13610799e-05 ... 1.04793570e-05\n",
      "             1.17023290e-05 1.16924340e-05]                                 \n",
      "            [1.11128505e-05 1.09026226e-05 1.06606783e-05 ... 1.22447049e-05\n",
      "             1.06733651e-05 1.06468847e-05]                                 \n",
      "            [1.14511267e-05 1.09720635e-05 1.12333440e-05 ... 1.08608023e-05\n",
      "             1.17407520e-05 1.14062335e-05]]                                \n",
      "log =  102723.88769065842\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.413765298962046 [[ 1.14444756e-05  1.07753852e-05  1.12910853e-05 ...  1.05204932e-05 \n",
      "                                                   1.16703169e-05  1.15108854e-05]                                    \n",
      "                                                 [ 1.11287912e-05  1.09319702e-05  1.10001720e-05 ...  1.07526250e-05 \n",
      "                                                   1.10823420e-05 -9.98228942e-05]                                    \n",
      "                                                 [ 1.03940190e-05  1.09363347e-05  1.06227889e-05 ...  1.19171405e-05 \n",
      "                                                   1.04929310e-05  1.07943664e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.16405974e-05  1.07660427e-05  1.13610799e-05 ...  1.04793570e-05 \n",
      "                                                   1.17023290e-05  1.16924340e-05]                                    \n",
      "                                                 [ 1.11128505e-05 -1.00208489e-04  1.06606783e-05 ...  1.22447049e-05 \n",
      "                                                   1.06733651e-05  1.06468847e-05]                                    \n",
      "                                                 [ 1.14511267e-05 -1.00139048e-04  1.12333440e-05 ...  1.08608023e-05 \n",
      "                                                   1.17407520e-05  1.14062335e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.413765298962046 [[ 1.14444756e-05  1.07753852e-05  1.12910853e-05 ...  1.05204932e-05 [[-0.00047466 -0.00022977  0.00020273 ...  0.00069583 -0.00130124 \n",
      "                                                              1.16703169e-05  1.15108854e-05]                                      -0.00014399]                                                    \n",
      "                                                            [ 1.11287912e-05  1.09319702e-05  1.10001720e-05 ...  1.07526250e-05  [-0.00102544  0.00049835  0.00072532 ...  0.00038427 -0.00157751 \n",
      "                                                              1.10823420e-05 -9.98228942e-05]                                      -0.00018052]                                                    \n",
      "                                                            [ 1.03940190e-05  1.09363347e-05  1.06227889e-05 ...  1.19171405e-05  [-0.00134856  0.00108479  0.0013006  ...  0.00026026 -0.00165596 \n",
      "                                                              1.04929310e-05  1.07943664e-05]                                      -0.00076254]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.16405974e-05  1.07660427e-05  1.13610799e-05 ...  1.04793570e-05  [-0.00030685  0.00066056 -0.00052284 ...  0.00129933 -0.001057   \n",
      "                                                              1.17023290e-05  1.16924340e-05]                                      -0.00088949]                                                    \n",
      "                                                            [ 1.11128505e-05 -1.00208489e-04  1.06606783e-05 ...  1.22447049e-05  [ 0.00031864 -0.00049167 -0.00075183 ...  0.0015064  -0.00052896 \n",
      "                                                              1.06733651e-05  1.06468847e-05]                                      -0.00083049]                                                    \n",
      "                                                            [ 1.14511267e-05 -1.00139048e-04  1.12333440e-05 ...  1.08608023e-05  [ 0.03399685 -0.09793693 -0.04900029 ...  0.02341846  0.03358139 \n",
      "                                                              1.17407520e-05  1.14062335e-05]]                                      0.03803516]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.19238562523638844 [[-0.0083254  -0.01603962 -0.02545195 ... -0.00943901 -0.01244393\n",
      "                                        0.00357916]                                                   \n",
      "                                      [ 0.00543463 -0.00831536 -0.03440275 ...  0.05545494  0.00115875\n",
      "                                        0.02779141]                                                   \n",
      "                                      [-0.00083106  0.04513206 -0.03291364 ...  0.01194402 -0.04936011\n",
      "                                       -0.02434961]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.03336313  0.01260771 -0.01887842 ...  0.0041777  -0.00618395\n",
      "                                       -0.00633314]                                                   \n",
      "                                      [ 0.01543832  0.02834581  0.03375182 ... -0.00498573 -0.00748604\n",
      "                                        0.02038396]                                                   \n",
      "                                      [ 0.03134722 -0.13126067 -0.08236777 ...  0.01972366  0.03022281\n",
      "                                        0.02895926]]                                                  \n",
      "Epoch 1, loss: 11.606151\n",
      "== W == -0.7760279226043376\n",
      "enter of the function =  [[-0.00065853  0.05985773 -0.04162881 ... -0.00443829 -0.00154037 [7 5 2 ... 7 4 2]\n",
      "                            0.01616203]                                                                     \n",
      "                          [ 0.00525671 -0.07547836 -0.00983435 ... -0.01822587 -0.00600191                  \n",
      "                            0.00859576]                                                                     \n",
      "                          [ 0.04431811  0.00466075  0.00177714 ... -0.04816281  0.06547497                  \n",
      "                            0.04560569]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.02233403 -0.0332619   0.03058481 ... -0.04730824  0.03999039                  \n",
      "                            0.02679905]                                                                     \n",
      "                          [ 0.01875616 -0.02189178  0.0418642  ... -0.10337042  0.00583784                  \n",
      "                            0.05001596]                                                                     \n",
      "                          [-0.01696458  0.00301235 -0.02365264 ...  0.01814411 -0.03836122                  \n",
      "                           -0.04471325]]                                                                    \n",
      "soft max = [[1.11021070e-05 1.17947104e-05 1.06564424e-05 ... 1.10602229e-05\n",
      "             1.10923210e-05 1.12904300e-05]                                 \n",
      "            [1.11679732e-05 1.03017633e-05 1.10007019e-05 ... 1.09087755e-05\n",
      "             1.10429424e-05 1.12053260e-05]                                 \n",
      "            [1.16128419e-05 1.11613196e-05 1.11291809e-05 ... 1.05870401e-05\n",
      "             1.18611506e-05 1.16278040e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.13603300e-05 1.07459779e-05 1.14544494e-05 ... 1.05960914e-05\n",
      "             1.15626934e-05 1.14111675e-05]                                 \n",
      "            [1.13197569e-05 1.08688583e-05 1.15843799e-05 ... 1.00183961e-05\n",
      "             1.11744651e-05 1.16791990e-05]                                 \n",
      "            [1.09225434e-05 1.11429363e-05 1.08497365e-05 ... 1.13128307e-05\n",
      "             1.06913202e-05 1.06236238e-05]]                                \n",
      "log =  102725.1026726875\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.413900296965277 [[ 1.11021070e-05  1.17947104e-05  1.06564424e-05 ... -1.00050888e-04 \n",
      "                                                   1.10923210e-05  1.12904300e-05]                                    \n",
      "                                                 [ 1.11679732e-05  1.03017633e-05  1.10007019e-05 ...  1.09087755e-05 \n",
      "                                                   1.10429424e-05  1.12053260e-05]                                    \n",
      "                                                 [ 1.16128419e-05  1.11613196e-05 -9.99819302e-05 ...  1.05870401e-05 \n",
      "                                                   1.18611506e-05  1.16278040e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.13603300e-05  1.07459779e-05  1.14544494e-05 ... -1.00515020e-04 \n",
      "                                                   1.15626934e-05  1.14111675e-05]                                    \n",
      "                                                 [ 1.13197569e-05  1.08688583e-05  1.15843799e-05 ...  1.00183961e-05 \n",
      "                                                   1.11744651e-05  1.16791990e-05]                                    \n",
      "                                                 [ 1.09225434e-05  1.11429363e-05 -1.00261375e-04 ...  1.13128307e-05 \n",
      "                                                   1.06913202e-05  1.06236238e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.413900296965277 [[ 1.11021070e-05  1.17947104e-05  1.06564424e-05 ... -1.00050888e-04 [[-0.00048113 -0.00023059  0.00019877 ...  0.00070912 -0.00131167 \n",
      "                                                              1.10923210e-05  1.12904300e-05]                                      -0.0001539 ]                                                    \n",
      "                                                            [ 1.11679732e-05  1.03017633e-05  1.10007019e-05 ...  1.09087755e-05  [-0.00103197  0.00049766  0.00072141 ...  0.00039815 -0.00158844 \n",
      "                                                              1.10429424e-05  1.12053260e-05]                                      -0.00019099]                                                    \n",
      "                                                            [ 1.16128419e-05  1.11613196e-05 -9.99819302e-05 ...  1.05870401e-05  [-0.00135489  0.00108395  0.00129661 ...  0.00027506 -0.00166698 \n",
      "                                                              1.18611506e-05  1.16278040e-05]                                      -0.00077373]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.13603300e-05  1.07459779e-05  1.14544494e-05 ... -1.00515020e-04  [-0.00031235  0.00066178 -0.00052781 ...  0.00131427 -0.00106626 \n",
      "                                                              1.15626934e-05  1.14111675e-05]                                      -0.00089982]                                                    \n",
      "                                                            [ 1.13197569e-05  1.08688583e-05  1.15843799e-05 ...  1.00183961e-05  [ 0.0003134  -0.00049081 -0.00075672 ...  0.0015221  -0.0005382  \n",
      "                                                              1.11744651e-05  1.16791990e-05]                                      -0.00084145]                                                    \n",
      "                                                            [ 1.09225434e-05  1.11429363e-05 -1.00261375e-04 ...  1.13128307e-05  [ 0.03400131 -0.09796177 -0.04901502 ...  0.02342455  0.03358712 \n",
      "                                                              1.06913202e-05  1.06236238e-05]]                                      0.03804186]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.19626747772700875 [[-0.0084134  -0.01620232 -0.02570444 ... -0.00952645 -0.01258138\n",
      "                                        0.00361351]                                                   \n",
      "                                      [ 0.00547872 -0.00839353 -0.03473952 ...  0.05601333  0.00115457\n",
      "                                        0.02806752]                                                   \n",
      "                                      [-0.00085286  0.04559423 -0.03322977 ...  0.01206607 -0.04987027\n",
      "                                       -0.02460073]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.03369983  0.01274039 -0.01907243 ...  0.00423247 -0.00625636\n",
      "                                       -0.00640537]                                                   \n",
      "                                      [ 0.01559589  0.02862435  0.03408182 ... -0.00502052 -0.00756619\n",
      "                                        0.02057949]                                                   \n",
      "                                      [ 0.03200066 -0.13355264 -0.08368145 ...  0.02015508  0.03086085\n",
      "                                        0.0296292 ]]                                                  \n",
      "Epoch 2, loss: 11.610168\n",
      "== W == -0.7893509795619263\n",
      "enter of the function =  [[ 1.95561285e-02 -2.26203398e-02  3.42985284e-03 ... -5.66610314e-02 [4 5 7 ... 2 2 6]\n",
      "                            2.06767177e-02  2.35265004e-02]                                                     \n",
      "                          [ 1.71022950e-02 -3.54130535e-02  3.20721835e-02 ... -3.54110031e-02                  \n",
      "                            6.88384017e-03  1.83570002e-02]                                                     \n",
      "                          [ 1.35263420e-02  1.07216064e-02 -5.29418416e-03 ... -6.57483294e-03                  \n",
      "                            1.93774472e-02  2.12928430e-03]                                                     \n",
      "                          ...                                                                                   \n",
      "                          [ 2.59061644e-05 -4.73478123e-02  1.95048378e-03 ... -4.24466591e-02                  \n",
      "                            3.36290559e-02  4.17296970e-02]                                                     \n",
      "                          [-6.59150466e-02 -7.79425944e-03 -7.73114311e-02 ...  3.88116990e-02                  \n",
      "                           -4.69589075e-02 -7.13238733e-02]                                                     \n",
      "                          [-4.48783755e-02 -2.90930968e-03 -5.40856103e-02 ...  1.31040478e-01                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -5.74315501e-02 -7.65730415e-02]]                                                    \n",
      "soft max = [[1.13284403e-05 1.08605823e-05 1.11472199e-05 ... 1.04971023e-05\n",
      "             1.13411419e-05 1.13735078e-05]                                 \n",
      "            [1.13006763e-05 1.07225309e-05 1.14711187e-05 ... 1.07225529e-05\n",
      "             1.11857888e-05 1.13148642e-05]                                 \n",
      "            [1.12603378e-05 1.12287997e-05 1.10503941e-05 ... 1.10362515e-05\n",
      "             1.13264163e-05 1.11327316e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.11093398e-05 1.05953207e-05 1.11307412e-05 ... 1.06473775e-05\n",
      "             1.14889917e-05 1.15824379e-05]                                 \n",
      "            [1.04004100e-05 1.10228018e-05 1.02825558e-05 ... 1.15486896e-05\n",
      "             1.05994421e-05 1.03443079e-05]                                 \n",
      "            [1.06215175e-05 1.10767793e-05 1.05241716e-05 ... 1.26644740e-05\n",
      "             1.04890172e-05 1.02901511e-05]]                                \n",
      "log =  102726.34095875916\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.414037884306573 [[ 1.13284403e-05  1.08605823e-05  1.11472199e-05 ...  1.04971023e-05 \n",
      "                                                   1.13411419e-05  1.13735078e-05]                                    \n",
      "                                                 [ 1.13006763e-05  1.07225309e-05  1.14711187e-05 ...  1.07225529e-05 \n",
      "                                                   1.11857888e-05  1.13148642e-05]                                    \n",
      "                                                 [ 1.12603378e-05  1.12287997e-05  1.10503941e-05 ... -1.00074860e-04 \n",
      "                                                   1.13264163e-05  1.11327316e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11093398e-05  1.05953207e-05 -9.99803699e-05 ...  1.06473775e-05 \n",
      "                                                   1.14889917e-05  1.15824379e-05]                                    \n",
      "                                                 [ 1.04004100e-05  1.10228018e-05 -1.00828555e-04 ...  1.15486896e-05 \n",
      "                                                   1.05994421e-05  1.03443079e-05]                                    \n",
      "                                                 [ 1.06215175e-05  1.10767793e-05  1.05241716e-05 ...  1.26644740e-05 \n",
      "                                                   1.04890172e-05  1.02901511e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.414037884306573 [[ 1.13284403e-05  1.08605823e-05  1.11472199e-05 ...  1.04971023e-05 [[-0.00048771 -0.00023142  0.00019474 ...  0.00072266 -0.00132227 \n",
      "                                                              1.13411419e-05  1.13735078e-05]                                      -0.00016397]                                                    \n",
      "                                                            [ 1.13006763e-05  1.07225309e-05  1.14711187e-05 ...  1.07225529e-05  [-0.0010386   0.00049696  0.00071744 ...  0.00041229 -0.00159955 \n",
      "                                                              1.11857888e-05  1.13148642e-05]                                      -0.00020163]                                                    \n",
      "                                                            [ 1.12603378e-05  1.12287997e-05  1.10503941e-05 ... -1.00074860e-04  [-0.00136133  0.0010831   0.00129255 ...  0.00029013 -0.00167817 \n",
      "                                                              1.13264163e-05  1.11327316e-05]                                      -0.00078511]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.11093398e-05  1.05953207e-05 -9.99803699e-05 ...  1.06473775e-05  [-0.00031795  0.00066301 -0.00053286 ...  0.00132948 -0.00107568 \n",
      "                                                              1.14889917e-05  1.15824379e-05]                                      -0.00091033]                                                    \n",
      "                                                            [ 1.04004100e-05  1.10228018e-05 -1.00828555e-04 ...  1.15486896e-05  [ 0.00030807 -0.00048995 -0.0007617  ...  0.00153808 -0.00054761 \n",
      "                                                              1.05994421e-05  1.03443079e-05]                                      -0.0008526 ]                                                    \n",
      "                                                            [ 1.06215175e-05  1.10767793e-05  1.05241716e-05 ...  1.26644740e-05  [ 0.03400574 -0.09798695 -0.04902997 ...  0.02343074  0.03359287 \n",
      "                                                              1.04890172e-05  1.02901511e-05]]                                      0.0380486 ]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.20022771380697035 [[-0.00850235 -0.01636665 -0.0259595  ... -0.00961462 -0.01272031\n",
      "                                        0.0036481 ]                                                   \n",
      "                                      [ 0.00552319 -0.00847249 -0.03507971 ...  0.05657744  0.00115023\n",
      "                                        0.02834628]                                                   \n",
      "                                      [-0.00087494  0.04606101 -0.0335491  ...  0.01218948 -0.05038564\n",
      "                                       -0.02485447]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.03403995  0.01287441 -0.01926843 ...  0.00428793 -0.00632959\n",
      "                                       -0.00647842]                                                   \n",
      "                                      [ 0.01575499  0.02890569  0.03441507 ... -0.00505551 -0.00764724\n",
      "                                        0.02077687]                                                   \n",
      "                                      [ 0.03266068 -0.13586779 -0.08500841 ...  0.02059088  0.03150533\n",
      "                                        0.03030591]]                                                  \n",
      "Epoch 3, loss: 11.614266\n",
      "== W == -0.8028941652163794\n",
      "enter of the function =  [[ 0.00357897 -0.02040462 -0.01131145 ... -0.00128735  0.00364414 [1 1 2 ... 5 6 2]\n",
      "                            0.02220442]                                                                     \n",
      "                          [-0.02147889 -0.01161998 -0.0434726  ...  0.02288501 -0.03654669                  \n",
      "                           -0.02128549]                                                                     \n",
      "                          [ 0.00054574 -0.04170256  0.02755389 ... -0.0257154  -0.0089825                   \n",
      "                            0.03050674]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.0273306  -0.01036411  0.01654648 ... -0.02801101  0.01076357                  \n",
      "                            0.01345418]                                                                     \n",
      "                          [ 0.02271117 -0.00382893  0.01379825 ... -0.07886613  0.04995767                  \n",
      "                            0.06128041]                                                                     \n",
      "                          [ 0.00150035 -0.00424069 -0.0388289  ... -0.00079533  0.0042382                   \n",
      "                            0.0027521 ]]                                                                    \n",
      "soft max = [[1.11484962e-05 1.08842961e-05 1.09837203e-05 ... 1.10943758e-05\n",
      "             1.11492227e-05 1.13580878e-05]                                 \n",
      "            [1.08726097e-05 1.09803319e-05 1.06360911e-05 ... 1.13658206e-05\n",
      "             1.07100115e-05 1.08747127e-05]                                 \n",
      "            [1.11147315e-05 1.06549342e-05 1.14190103e-05 ... 1.08266453e-05\n",
      "             1.10093306e-05 1.14527788e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.14164608e-05 1.09941305e-05 1.12940058e-05 ... 1.08018200e-05\n",
      "             1.12288821e-05 1.12591354e-05]                                 \n",
      "            [1.13638450e-05 1.10662144e-05 1.12630099e-05 ... 1.02662264e-05\n",
      "             1.16777266e-05 1.18107018e-05]                                 \n",
      "            [1.11253467e-05 1.10616587e-05 1.06855968e-05 ... 1.10998358e-05\n",
      "             1.11558480e-05 1.11392817e-05]]                                \n",
      "log =  102727.6031498233\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.414178127758145 [[ 1.11484962e-05 -1.00226815e-04  1.09837203e-05 ...  1.10943758e-05 \n",
      "                                                   1.11492227e-05  1.13580878e-05]                                    \n",
      "                                                 [ 1.08726097e-05 -1.00130779e-04  1.06360911e-05 ...  1.13658206e-05 \n",
      "                                                   1.07100115e-05  1.08747127e-05]                                    \n",
      "                                                 [ 1.11147315e-05  1.06549342e-05 -9.96921008e-05 ...  1.08266453e-05 \n",
      "                                                   1.10093306e-05  1.14527788e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.14164608e-05  1.09941305e-05  1.12940058e-05 ...  1.08018200e-05 \n",
      "                                                   1.12288821e-05  1.12591354e-05]                                    \n",
      "                                                 [ 1.13638450e-05  1.10662144e-05  1.12630099e-05 ...  1.02662264e-05 \n",
      "                                                   1.16777266e-05  1.18107018e-05]                                    \n",
      "                                                 [ 1.11253467e-05  1.10616587e-05 -1.00425514e-04 ...  1.10998358e-05 \n",
      "                                                   1.11558480e-05  1.11392817e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.414178127758145 [[ 1.11484962e-05 -1.00226815e-04  1.09837203e-05 ...  1.10943758e-05 [[-0.00049439 -0.00023226  0.00019064 ...  0.00073644 -0.00133304 \n",
      "                                                              1.11492227e-05  1.13580878e-05]                                      -0.00017422]                                                    \n",
      "                                                            [ 1.08726097e-05 -1.00130779e-04  1.06360911e-05 ...  1.13658206e-05  [-0.00104534  0.00049625  0.00071339 ...  0.00042668 -0.00161084 \n",
      "                                                              1.07100115e-05  1.08747127e-05]                                      -0.00021245]                                                    \n",
      "                                                            [ 1.11147315e-05  1.06549342e-05 -9.96921008e-05 ...  1.08266453e-05  [-0.00136788  0.00108224  0.00128841 ...  0.00030548 -0.00168955 \n",
      "                                                              1.10093306e-05  1.14527788e-05]                                      -0.00079667]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.14164608e-05  1.09941305e-05  1.12940058e-05 ...  1.08018200e-05  [-0.00032365  0.00066424 -0.000538   ...  0.00134496 -0.00108526 \n",
      "                                                              1.12288821e-05  1.12591354e-05]                                      -0.00092102]                                                    \n",
      "                                                            [ 1.13638450e-05  1.10662144e-05  1.12630099e-05 ...  1.02662264e-05  [ 0.00030264 -0.00048909 -0.00076675 ...  0.00155434 -0.00055717 \n",
      "                                                              1.16777266e-05  1.18107018e-05]                                      -0.00086393]                                                    \n",
      "                                                            [ 1.11253467e-05  1.10616587e-05 -1.00425514e-04 ...  1.10998358e-05  [ 0.03401012 -0.09801249 -0.04904516 ...  0.02343702  0.03359862 \n",
      "                                                              1.11558480e-05  1.11392817e-05]]                                      0.03805537]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.20426791733619729 [[-0.00859225 -0.01653263 -0.02621714 ... -0.00970354 -0.01286073\n",
      "                                        0.00368295]                                                   \n",
      "                                      [ 0.00556804 -0.00855224 -0.03542333 ...  0.05714734  0.00114573\n",
      "                                        0.02862773]                                                   \n",
      "                                      [-0.0008973   0.04653245 -0.03387166 ...  0.01231427 -0.05090628\n",
      "                                       -0.02511087]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.03438353  0.01300978 -0.01946645 ...  0.00434411 -0.00640364\n",
      "                                       -0.00655231]                                                   \n",
      "                                      [ 0.01591562  0.02918985  0.0347516  ... -0.00509068 -0.00772918\n",
      "                                        0.02097612]                                                   \n",
      "                                      [ 0.03332735 -0.13820633 -0.08634879 ...  0.02103109  0.03215631\n",
      "                                        0.03098946]]                                                  \n",
      "Epoch 4, loss: 11.618446\n",
      "== W == -0.8166608481558811\n",
      "enter of the function =  [[ 0.02388381 -0.05464694  0.04727295 ... -0.06583132  0.07697379 [2 2 0 ... 1 2 1]\n",
      "                            0.07200933]                                                                     \n",
      "                          [ 0.04196044 -0.00280475  0.01182852 ... -0.0037007   0.06859611                  \n",
      "                            0.03969263]                                                                     \n",
      "                          [-0.02023928 -0.00711503 -0.05300738 ...  0.10418722 -0.03029339                  \n",
      "                           -0.10577652]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.00733693 -0.01113642 -0.01912121 ... -0.03426629  0.00205699                  \n",
      "                            0.04137303]                                                                     \n",
      "                          [-0.03882096 -0.02325761 -0.10053189 ...  0.116907   -0.08175898                  \n",
      "                           -0.11279227]                                                                     \n",
      "                          [-0.02139078 -0.01658855 -0.0419244  ...  0.05062269 -0.04514209                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.03627693]]                                                                    \n",
      "soft max = [[1.13767671e-05 1.05175212e-05 1.16459962e-05 ... 1.04005446e-05\n",
      "             1.19970800e-05 1.19376685e-05]                                 \n",
      "            [1.15842908e-05 1.10771535e-05 1.12404403e-05 ... 1.10672334e-05\n",
      "             1.18969921e-05 1.15580495e-05]                                 \n",
      "            [1.08857023e-05 1.10295106e-05 1.05347795e-05 ... 1.23280446e-05\n",
      "             1.07768047e-05 9.99328107e-06]                                 \n",
      "            ...                                                             \n",
      "            [1.11900660e-05 1.09852457e-05 1.08978801e-05 ... 1.07340744e-05\n",
      "             1.11311389e-05 1.15774880e-05]                                 \n",
      "            [1.06852954e-05 1.08528952e-05 1.00458298e-05 ... 1.24858562e-05\n",
      "             1.02362005e-05 9.92341606e-06]                                 \n",
      "            [1.08731746e-05 1.09255157e-05 1.06521856e-05 ... 1.16850727e-05\n",
      "             1.06179653e-05 1.07125137e-05]]                                \n",
      "log =  102728.88986552518\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.414321096169465 [[ 1.13767671e-05  1.05175212e-05 -9.94651150e-05 ...  1.04005446e-05 \n",
      "                                                   1.19970800e-05  1.19376685e-05]                                    \n",
      "                                                 [ 1.15842908e-05  1.10771535e-05 -9.98706708e-05 ...  1.10672334e-05 \n",
      "                                                   1.18969921e-05  1.15580495e-05]                                    \n",
      "                                                 [-1.00225409e-04  1.10295106e-05  1.05347795e-05 ...  1.23280446e-05 \n",
      "                                                   1.07768047e-05  9.99328107e-06]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11900660e-05 -1.00125865e-04  1.08978801e-05 ...  1.07340744e-05 \n",
      "                                                   1.11311389e-05  1.15774880e-05]                                    \n",
      "                                                 [ 1.06852954e-05  1.08528952e-05 -1.01065281e-04 ...  1.24858562e-05 \n",
      "                                                   1.02362005e-05  9.92341606e-06]                                    \n",
      "                                                 [ 1.08731746e-05 -1.00185595e-04  1.06521856e-05 ...  1.16850727e-05 \n",
      "                                                   1.06179653e-05  1.07125137e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.414321096169465 [[ 1.13767671e-05  1.05175212e-05 -9.94651150e-05 ...  1.04005446e-05 [[-0.00050118 -0.00023312  0.00018646 ...  0.00075048 -0.00134398 \n",
      "                                                              1.19970800e-05  1.19376685e-05]                                      -0.00018463]                                                    \n",
      "                                                            [ 1.15842908e-05  1.10771535e-05 -9.98706708e-05 ...  1.10672334e-05  [-0.00105219  0.00049552  0.00070927 ...  0.00044135 -0.0016223  \n",
      "                                                              1.18969921e-05  1.15580495e-05]                                      -0.00022346]                                                    \n",
      "                                                            [-1.00225409e-04  1.10295106e-05  1.05347795e-05 ...  1.23280446e-05  [-0.00137454  0.00108136  0.0012842  ...  0.00032111 -0.00170111 \n",
      "                                                              1.07768047e-05  9.99328107e-06]                                      -0.00080844]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.11900660e-05 -1.00125865e-04  1.08978801e-05 ...  1.07340744e-05  [-0.00032944  0.00066549 -0.00054322 ...  0.00136073 -0.001095   \n",
      "                                                              1.11311389e-05  1.15774880e-05]                                      -0.00093189]                                                    \n",
      "                                                            [ 1.06852954e-05  1.08528952e-05 -1.01065281e-04 ...  1.24858562e-05  [ 0.00029711 -0.00048823 -0.0007719  ...  0.0015709  -0.0005669  \n",
      "                                                              1.02362005e-05  9.92341606e-06]                                      -0.00087545]                                                    \n",
      "                                                            [ 1.08731746e-05 -1.00185595e-04  1.06521856e-05 ...  1.16850727e-05  [ 0.03401446 -0.09803837 -0.04906059 ...  0.02344339  0.03360438 \n",
      "                                                              1.06179653e-05  1.07125137e-05]]                                      0.03806217]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.20838970425653977 [[-0.00868312 -0.01670028 -0.02647741 ... -0.00979321 -0.01300267\n",
      "                                        0.00371803]                                                   \n",
      "                                      [ 0.00561326 -0.0086328  -0.03577043 ...  0.05772308  0.00114108\n",
      "                                        0.02891188]                                                   \n",
      "                                      [-0.00091995  0.0470086  -0.0341975  ...  0.01244047 -0.05143224\n",
      "                                       -0.02536994]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.0347306   0.01314652 -0.01966649 ...  0.004401   -0.00647853\n",
      "                                       -0.00662704]                                                   \n",
      "                                      [ 0.0160778   0.02947685  0.03509145 ... -0.00512604 -0.00781205\n",
      "                                        0.02117724]                                                   \n",
      "                                      [ 0.03400072 -0.14056852 -0.08770273 ...  0.02147578  0.03281386\n",
      "                                        0.03167991]]                                                  \n",
      "Epoch 5, loss: 11.622711\n",
      "== W == -0.8306544384368111\n",
      "enter of the function =  [[-0.05609451  0.02617785 -0.04326737 ...  0.0977592  -0.01845607 [5 1 9 ... 4 8 4]\n",
      "                           -0.06196739]                                                                     \n",
      "                          [-0.04643662 -0.06034759 -0.0716928  ...  0.07702294 -0.05303112                  \n",
      "                           -0.04051928]                                                                     \n",
      "                          [ 0.00891782  0.03084227  0.00118441 ... -0.0329061   0.0052646                   \n",
      "                            0.02123126]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.03233656 -0.02263974 -0.02381783 ...  0.00096175  0.05955718                  \n",
      "                            0.02229361]                                                                     \n",
      "                          [-0.01317527  0.01635764  0.0066087  ...  0.02507613 -0.02448967                  \n",
      "                           -0.03614834]                                                                     \n",
      "                          [-0.03497903  0.00372136 -0.00374766 ...  0.11280325 -0.06736279                  \n",
      "                           -0.01245282]]                                                                    \n",
      "soft max = [[1.05019111e-05 1.14024656e-05 1.06374883e-05 ... 1.22485916e-05\n",
      "             1.09047197e-05 1.04404154e-05]                                 \n",
      "            [1.06038288e-05 1.04573405e-05 1.03393703e-05 ... 1.19972169e-05\n",
      "             1.05341318e-05 1.06667613e-05]                                 \n",
      "            [1.12073474e-05 1.14557758e-05 1.11210107e-05 ... 1.07482791e-05\n",
      "             1.11664792e-05 1.13462015e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.14729068e-05 1.08591933e-05 1.08464076e-05 ... 1.11185347e-05\n",
      "             1.17894957e-05 1.13582616e-05]                                 \n",
      "            [1.09624576e-05 1.12910390e-05 1.11814981e-05 ... 1.13899102e-05\n",
      "             1.08391230e-05 1.07134871e-05]                                 \n",
      "            [1.07260218e-05 1.11492599e-05 1.10662961e-05 ... 1.24342531e-05\n",
      "             1.03842369e-05 1.09703803e-05]]                                \n",
      "log =  102730.20174484886\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.414466860538763 [[ 1.05019111e-05  1.14024656e-05  1.06374883e-05 ...  1.22485916e-05 \n",
      "                                                   1.09047197e-05  1.04404154e-05]                                    \n",
      "                                                 [ 1.06038288e-05 -1.00653771e-04  1.03393703e-05 ...  1.19972169e-05 \n",
      "                                                   1.05341318e-05  1.06667613e-05]                                    \n",
      "                                                 [ 1.12073474e-05  1.14557758e-05  1.11210107e-05 ...  1.07482791e-05 \n",
      "                                                   1.11664792e-05 -9.97649096e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.14729068e-05  1.08591933e-05  1.08464076e-05 ...  1.11185347e-05 \n",
      "                                                   1.17894957e-05  1.13582616e-05]                                    \n",
      "                                                 [ 1.09624576e-05  1.12910390e-05  1.11814981e-05 ...  1.13899102e-05 \n",
      "                                                  -1.00271988e-04  1.07134871e-05]                                    \n",
      "                                                 [ 1.07260218e-05  1.11492599e-05  1.10662961e-05 ...  1.24342531e-05 \n",
      "                                                   1.03842369e-05  1.09703803e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.414466860538763 [[ 1.05019111e-05  1.14024656e-05  1.06374883e-05 ...  1.22485916e-05 [[-0.00050808 -0.00023399  0.00018221 ...  0.00076478 -0.0013551  \n",
      "                                                              1.09047197e-05  1.04404154e-05]                                      -0.00019523]                                                    \n",
      "                                                            [ 1.06038288e-05 -1.00653771e-04  1.03393703e-05 ...  1.19972169e-05  [-0.00105915  0.00049478  0.00070507 ...  0.00045628 -0.00163395 \n",
      "                                                              1.05341318e-05  1.06667613e-05]                                      -0.00023465]                                                    \n",
      "                                                            [ 1.12073474e-05  1.14557758e-05  1.11210107e-05 ...  1.07482791e-05  [-0.0013813   0.00108046  0.00127991 ...  0.00033702 -0.00171286 \n",
      "                                                              1.11664792e-05 -9.97649096e-05]                                      -0.00082039]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.14729068e-05  1.08591933e-05  1.08464076e-05 ...  1.11185347e-05  [-0.00033534  0.00066674 -0.00054852 ...  0.00137678 -0.00110491 \n",
      "                                                              1.17894957e-05  1.13582616e-05]                                      -0.00094295]                                                    \n",
      "                                                            [ 1.09624576e-05  1.12910390e-05  1.11814981e-05 ...  1.13899102e-05  [ 0.00029148 -0.00048736 -0.00077713 ...  0.00158775 -0.0005768  \n",
      "                                                             -1.00271988e-04  1.07134871e-05]                                      -0.00088717]                                                    \n",
      "                                                            [ 1.07260218e-05  1.11492599e-05  1.10662961e-05 ...  1.24342531e-05  [ 0.03401875 -0.09806462 -0.04907626 ...  0.02344986  0.03361015 \n",
      "                                                              1.03842369e-05  1.09703803e-05]]                                      0.038069  ]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.21259472324484988 [[-0.00877496 -0.01686961 -0.02674032 ... -0.00988364 -0.01314614\n",
      "                                        0.00375337]                                                   \n",
      "                                      [ 0.00565887 -0.00871418 -0.03612104 ...  0.05830472  0.00113627\n",
      "                                        0.02919877]                                                   \n",
      "                                      [-0.0009429   0.0474895  -0.03452663 ...  0.01256809 -0.05196357\n",
      "                                       -0.02563173]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.0350812   0.01328464 -0.01986859 ...  0.00445862 -0.00655426\n",
      "                                       -0.00670263]                                                   \n",
      "                                      [ 0.01624155  0.02976674  0.03543465 ... -0.00516159 -0.00789584\n",
      "                                        0.02138026]                                                   \n",
      "                                      [ 0.03468087 -0.14295459 -0.08907037 ...  0.02192497  0.03347804\n",
      "                                        0.03237733]]                                                  \n",
      "Epoch 6, loss: 11.627062\n",
      "== W == -0.8448783876452637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[-0.03144864 -0.00722059 -0.00640492 ...  0.01928402 -0.03556456 [2 4 0 ... 4 7 4]\n",
      "                           -0.0153312 ]                                                                     \n",
      "                          [ 0.01599411  0.03374432  0.01842594 ... -0.00459448  0.01243747                  \n",
      "                            0.00635316]                                                                     \n",
      "                          [-0.02513055 -0.00070922 -0.02776679 ...  0.05322319 -0.02777818                  \n",
      "                           -0.03261684]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.01773286 -0.02499616 -0.02657317 ...  0.07721143  0.04413411                  \n",
      "                           -0.02933485]                                                                     \n",
      "                          [-0.0384135   0.07470934 -0.04019617 ...  0.04301844 -0.01761849                  \n",
      "                           -0.07414947]                                                                     \n",
      "                          [ 0.00050304 -0.00745595 -0.04610994 ... -0.01941249  0.01097472                  \n",
      "                            0.03369743]]                                                                    \n",
      "soft max = [[1.07635320e-05 1.10274961e-05 1.10364946e-05 ... 1.13236835e-05\n",
      "             1.07193212e-05 1.09384181e-05]                                 \n",
      "            [1.12864907e-05 1.14886169e-05 1.13139710e-05 ... 1.10564936e-05\n",
      "             1.12464201e-05 1.11782011e-05]                                 \n",
      "            [1.08317522e-05 1.10995345e-05 1.08032347e-05 ... 1.17145959e-05\n",
      "             1.08031117e-05 1.07509653e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.13061322e-05 1.08332080e-05 1.08161373e-05 ... 1.19990060e-05\n",
      "             1.16086035e-05 1.07863078e-05]                                 \n",
      "            [1.06888259e-05 1.19690210e-05 1.06697883e-05 ... 1.15956593e-05\n",
      "             1.09134274e-05 1.03135949e-05]                                 \n",
      "            [1.11129981e-05 1.10249010e-05 1.06068758e-05 ... 1.08938662e-05\n",
      "             1.12299814e-05 1.14880782e-05]]                                \n",
      "log =  102731.5394467848\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.414615494087199 [[ 1.07635320e-05  1.10274961e-05 -1.00074617e-04 ...  1.13236835e-05 \n",
      "                                                   1.07193212e-05  1.09384181e-05]                                    \n",
      "                                                 [ 1.12864907e-05  1.14886169e-05  1.13139710e-05 ...  1.10564936e-05 \n",
      "                                                   1.12464201e-05  1.11782011e-05]                                    \n",
      "                                                 [-1.00279359e-04  1.10995345e-05  1.08032347e-05 ...  1.17145959e-05 \n",
      "                                                   1.08031117e-05  1.07509653e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.13061322e-05  1.08332080e-05  1.08161373e-05 ...  1.19990060e-05 \n",
      "                                                   1.16086035e-05  1.07863078e-05]                                    \n",
      "                                                 [ 1.06888259e-05  1.19690210e-05  1.06697883e-05 ... -9.95154518e-05 \n",
      "                                                   1.09134274e-05  1.03135949e-05]                                    \n",
      "                                                 [ 1.11129981e-05  1.10249010e-05  1.06068758e-05 ...  1.08938662e-05 \n",
      "                                                   1.12299814e-05  1.14880782e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.414615494087199 [[ 1.07635320e-05  1.10274961e-05 -1.00074617e-04 ...  1.13236835e-05 [[-0.00051509 -0.00023487  0.00017789 ...  0.00077935 -0.0013664  \n",
      "                                                              1.07193212e-05  1.09384181e-05]                                      -0.00020601]                                                    \n",
      "                                                            [ 1.12864907e-05  1.14886169e-05  1.13139710e-05 ...  1.10564936e-05  [-0.00106623  0.00049403  0.0007008  ...  0.00047149 -0.00164579 \n",
      "                                                              1.12464201e-05  1.11782011e-05]                                      -0.00024603]                                                    \n",
      "                                                            [-1.00279359e-04  1.10995345e-05  1.08032347e-05 ...  1.17145959e-05  [-0.00138818  0.00107956  0.00127554 ...  0.00035322 -0.0017248  \n",
      "                                                              1.08031117e-05  1.07509653e-05]                                      -0.00083255]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.13061322e-05  1.08332080e-05  1.08161373e-05 ...  1.19990060e-05  [-0.00034134  0.00066799 -0.00055392 ...  0.00139312 -0.00111499 \n",
      "                                                              1.16086035e-05  1.07863078e-05]                                      -0.00095419]                                                    \n",
      "                                                            [ 1.06888259e-05  1.19690210e-05  1.06697883e-05 ... -9.95154518e-05  [ 0.00028575 -0.00048649 -0.00078244 ...  0.0016049  -0.00058687 \n",
      "                                                              1.09134274e-05  1.03135949e-05]                                      -0.00089908]                                                    \n",
      "                                                            [ 1.11129981e-05  1.10249010e-05  1.06068758e-05 ...  1.08938662e-05  [ 0.03402298 -0.09809124 -0.04909219 ...  0.02345643  0.03361592 \n",
      "                                                              1.12299814e-05  1.14880782e-05]]                                      0.03807587]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.21688465637947846 [[-0.00886779 -0.01704065 -0.0270059  ... -0.00997483 -0.01329115\n",
      "                                        0.00378895]                                                   \n",
      "                                      [ 0.00570487 -0.00879637 -0.0364752  ...  0.05889233  0.00113129\n",
      "                                        0.02948841]                                                   \n",
      "                                      [-0.00096614  0.0479752  -0.0348591  ...  0.01269714 -0.05250034\n",
      "                                       -0.02589625]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.03543537  0.01342416 -0.02007276 ...  0.00451697 -0.00663085\n",
      "                                       -0.00677908]                                                   \n",
      "                                      [ 0.01640688  0.03005953  0.03578122 ... -0.00519733 -0.00798056\n",
      "                                        0.02158519]                                                   \n",
      "                                      [ 0.03536787 -0.14536478 -0.09045183 ...  0.02237872  0.03414893\n",
      "                                        0.03308179]]                                                  \n",
      "Epoch 7, loss: 11.631500\n",
      "== W == -0.8593361889324131\n",
      "enter of the function =  [[ 0.04847935 -0.05102007  0.00215305 ... -0.00365467  0.05612001 [9 5 4 ... 1 1 1]\n",
      "                            0.05796974]                                                                     \n",
      "                          [-0.06213339 -0.07631265 -0.08595454 ...  0.07851817 -0.0396592                   \n",
      "                           -0.03928993]                                                                     \n",
      "                          [ 0.02733224 -0.03262209 -0.05941698 ...  0.01475807 -0.00267223                  \n",
      "                           -0.0497399 ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.02580904 -0.01580251  0.01429209 ... -0.04213502  0.01697733                  \n",
      "                            0.03264038]                                                                     \n",
      "                          [-0.02206247 -0.00739094 -0.10420967 ...  0.1335604  -0.05284399                  \n",
      "                           -0.0911142 ]                                                                     \n",
      "                          [-0.02024172  0.00773413 -0.02720681 ...  0.04369025 -0.03607033                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.01898229]]                                                                    \n",
      "soft max = [[1.16586765e-05 1.05544888e-05 1.11308927e-05 ... 1.10664349e-05\n",
      "             1.17480977e-05 1.17698486e-05]                                 \n",
      "            [1.04378428e-05 1.02908862e-05 1.01921394e-05 ... 1.20142024e-05\n",
      "             1.06750807e-05 1.06790234e-05]                                 \n",
      "            [1.14147178e-05 1.07504674e-05 1.04662348e-05 ... 1.12720858e-05\n",
      "             1.10773124e-05 1.05680090e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.13973442e-05 1.09328149e-05 1.12668344e-05 ... 1.06486838e-05\n",
      "             1.12971293e-05 1.14754698e-05]                                 \n",
      "            [1.08645897e-05 1.10251649e-05 1.00077685e-05 ... 1.26940289e-05\n",
      "             1.05352559e-05 1.01396869e-05]                                 \n",
      "            [1.08843894e-05 1.11931888e-05 1.08088421e-05 ... 1.16029754e-05\n",
      "             1.07134610e-05 1.08981062e-05]]                                \n",
      "log =  102732.9036510219\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.414767072335767 [[ 1.16586765e-05  1.05544888e-05  1.11308927e-05 ...  1.10664349e-05 \n",
      "                                                   1.17480977e-05 -9.93412625e-05]                                    \n",
      "                                                 [ 1.04378428e-05  1.02908862e-05  1.01921394e-05 ...  1.20142024e-05 \n",
      "                                                   1.06750807e-05  1.06790234e-05]                                    \n",
      "                                                 [ 1.14147178e-05  1.07504674e-05  1.04662348e-05 ...  1.12720858e-05 \n",
      "                                                   1.10773124e-05  1.05680090e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.13973442e-05 -1.00178296e-04  1.12668344e-05 ...  1.06486838e-05 \n",
      "                                                   1.12971293e-05  1.14754698e-05]                                    \n",
      "                                                 [ 1.08645897e-05 -1.00085946e-04  1.00077685e-05 ...  1.26940289e-05 \n",
      "                                                   1.05352559e-05  1.01396869e-05]                                    \n",
      "                                                 [ 1.08843894e-05 -9.99179223e-05  1.08088421e-05 ...  1.16029754e-05 \n",
      "                                                   1.07134610e-05  1.08981062e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.414767072335767 [[ 1.16586765e-05  1.05544888e-05  1.11308927e-05 ...  1.10664349e-05 [[-0.00052222 -0.00023577  0.00017348 ...  0.00079419 -0.00137788 \n",
      "                                                              1.17480977e-05 -9.93412625e-05]                                      -0.00021697]                                                    \n",
      "                                                            [ 1.04378428e-05  1.02908862e-05  1.01921394e-05 ...  1.20142024e-05  [-0.00107342  0.00049327  0.00069645 ...  0.00048698 -0.00165782 \n",
      "                                                              1.06750807e-05  1.06790234e-05]                                      -0.00025761]                                                    \n",
      "                                                            [ 1.14147178e-05  1.07504674e-05  1.04662348e-05 ...  1.12720858e-05  [-0.00139518  0.00107864  0.0012711  ...  0.00036972 -0.00173693 \n",
      "                                                              1.10773124e-05  1.05680090e-05]                                      -0.00084491]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.13973442e-05 -1.00178296e-04  1.12668344e-05 ...  1.06486838e-05  [-0.00034745  0.00066925 -0.0005594  ...  0.00140976 -0.00112524 \n",
      "                                                              1.12971293e-05  1.14754698e-05]                                      -0.00096563]                                                    \n",
      "                                                            [ 1.08645897e-05 -1.00085946e-04  1.00077685e-05 ...  1.26940289e-05  [ 0.00027992 -0.00048561 -0.00078785 ...  0.00162237 -0.00059711 \n",
      "                                                              1.05352559e-05  1.01396869e-05]                                      -0.0009112 ]                                                    \n",
      "                                                            [ 1.08843894e-05 -9.99179223e-05  1.08088421e-05 ...  1.16029754e-05  [ 0.03402716 -0.09811824 -0.04910837 ...  0.0234631   0.0336217  \n",
      "                                                              1.07134610e-05  1.08981062e-05]]                                      0.03808276]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.22126121982047758 [[-0.00896162 -0.0172134  -0.02727418 ... -0.01006678 -0.01343773\n",
      "                                        0.00382478]                                                   \n",
      "                                      [ 0.00575126 -0.00887939 -0.03683294 ...  0.05948597  0.00112615\n",
      "                                        0.02978083]                                                   \n",
      "                                      [-0.00098968  0.04846574 -0.03519493 ...  0.01282764 -0.05304259\n",
      "                                       -0.02616354]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.03579314  0.01356508 -0.02027903 ...  0.00457607 -0.00670831\n",
      "                                       -0.00685642]                                                   \n",
      "                                      [ 0.01657381  0.03035526  0.03613121 ... -0.00523326 -0.00806624\n",
      "                                        0.02179205]                                                   \n",
      "                                      [ 0.03606178 -0.14779934 -0.09184727 ...  0.02283707  0.03482657\n",
      "                                        0.03379337]]                                                  \n",
      "Epoch 8, loss: 11.636028\n",
      "== W == -0.8740313770220391\n",
      "enter of the function =  [[ 0.00534238 -0.01248436  0.0281913  ... -0.03483554  0.03907639 [7 4 9 ... 1 3 1]\n",
      "                            0.05653647]                                                                     \n",
      "                          [ 0.01171246 -0.01525299 -0.02761917 ...  0.03667037  0.00595223                  \n",
      "                           -0.01752381]                                                                     \n",
      "                          [-0.01241941 -0.00348468 -0.01879688 ...  0.04422519 -0.06162081                  \n",
      "                           -0.0397771 ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.00560784  0.05691886  0.01021527 ...  0.0101974   0.01007723                  \n",
      "                            0.00758796]                                                                     \n",
      "                          [ 0.00477164 -0.0446146   0.00458427 ... -0.04986953  0.00466773                  \n",
      "                            0.01990973]                                                                     \n",
      "                          [ 0.02431646 -0.02761223 -0.02694195 ... -0.02272423  0.01621252                  \n",
      "                            0.01691962]]                                                                    \n",
      "soft max = [[1.11659712e-05 1.09686820e-05 1.14240386e-05 ... 1.07262386e-05\n",
      "             1.15490695e-05 1.17524880e-05]                                 \n",
      "            [1.12373264e-05 1.09383558e-05 1.08039230e-05 ... 1.15213157e-05\n",
      "             1.11727828e-05 1.09135450e-05]                                 \n",
      "            [1.09693945e-05 1.10678422e-05 1.08996601e-05 ... 1.16086868e-05\n",
      "             1.04427470e-05 1.06733650e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.11689357e-05 1.17569828e-05 1.12205144e-05 ... 1.12203140e-05\n",
      "             1.12189657e-05 1.11910734e-05]                                 \n",
      "            [1.11596001e-05 1.06218573e-05 1.11575093e-05 ... 1.05661866e-05\n",
      "             1.11584406e-05 1.13298203e-05]                                 \n",
      "            [1.13798580e-05 1.08039980e-05 1.08112422e-05 ... 1.08569373e-05\n",
      "             1.12880089e-05 1.12959935e-05]]                                \n",
      "log =  102734.29505866564\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.41492167318507 [[ 1.11659712e-05  1.09686820e-05  1.14240386e-05 ... -1.00384873e-04 \n",
      "                                                  1.15490695e-05  1.17524880e-05]                                    \n",
      "                                                [ 1.12373264e-05  1.09383558e-05  1.08039230e-05 ...  1.15213157e-05 \n",
      "                                                  1.11727828e-05  1.09135450e-05]                                    \n",
      "                                                [ 1.09693945e-05  1.10678422e-05  1.08996601e-05 ...  1.16086868e-05 \n",
      "                                                  1.04427470e-05 -1.00437746e-04]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [ 1.11689357e-05 -9.93541283e-05  1.12205144e-05 ...  1.12203140e-05 \n",
      "                                                  1.12189657e-05  1.11910734e-05]                                    \n",
      "                                                [ 1.11596001e-05  1.06218573e-05  1.11575093e-05 ...  1.05661866e-05 \n",
      "                                                  1.11584406e-05  1.13298203e-05]                                    \n",
      "                                                [ 1.13798580e-05 -1.00307113e-04  1.08112422e-05 ...  1.08569373e-05 \n",
      "                                                  1.12880089e-05  1.12959935e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.41492167318507 [[ 1.11659712e-05  1.09686820e-05  1.14240386e-05 ... -1.00384873e-04 [[-0.00052946 -0.00023668  0.000169   ...  0.0008093  -0.00138954 \n",
      "                                                             1.15490695e-05  1.17524880e-05]                                      -0.00022812]                                                    \n",
      "                                                           [ 1.12373264e-05  1.09383558e-05  1.08039230e-05 ...  1.15213157e-05  [-0.00108072  0.00049249  0.00069202 ...  0.00050276 -0.00167004 \n",
      "                                                             1.11727828e-05  1.09135450e-05]                                      -0.00026937]                                                    \n",
      "                                                           [ 1.09693945e-05  1.10678422e-05  1.08996601e-05 ...  1.16086868e-05  [-0.00140229  0.0010777   0.00126657 ...  0.00038653 -0.00174926 \n",
      "                                                             1.04427470e-05 -1.00437746e-04]                                      -0.00085748]                                                    \n",
      "                                                           ...                                                                   ...                                                              \n",
      "                                                           [ 1.11689357e-05 -9.93541283e-05  1.12205144e-05 ...  1.12203140e-05  [-0.00035366  0.00067052 -0.00056498 ...  0.00142671 -0.00113566 \n",
      "                                                             1.12189657e-05  1.11910734e-05]                                      -0.00097726]                                                    \n",
      "                                                           [ 1.11596001e-05  1.06218573e-05  1.11575093e-05 ...  1.05661866e-05  [ 0.00027399 -0.00048473 -0.00079334 ...  0.00164015 -0.00060752 \n",
      "                                                             1.11584406e-05  1.13298203e-05]                                      -0.00092351]                                                    \n",
      "                                                           [ 1.13798580e-05 -1.00307113e-04  1.08112422e-05 ...  1.08569373e-05  [ 0.03403128 -0.09814563 -0.04912481 ...  0.02346988  0.03362747 \n",
      "                                                             1.12880089e-05  1.12959935e-05]]                                      0.03808969]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.22572616450379276 [[-0.00905646 -0.01738789 -0.02754519 ... -0.01015951 -0.01358588\n",
      "                                        0.00386086]                                                   \n",
      "                                      [ 0.00579804 -0.00896326 -0.03719431 ...  0.0600857   0.00112083\n",
      "                                        0.03007606]                                                   \n",
      "                                      [-0.00101353  0.04896119 -0.03553417 ...  0.01295961 -0.05359038\n",
      "                                       -0.02643362]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.03615454  0.01370742 -0.02048741 ...  0.00463593 -0.00678665\n",
      "                                       -0.00693464]                                                   \n",
      "                                      [ 0.01674234  0.03065396  0.03648464 ... -0.00526937 -0.00815287\n",
      "                                        0.02200086]                                                   \n",
      "                                      [ 0.03676267 -0.15025852 -0.09325683 ...  0.02330007  0.03551106\n",
      "                                        0.03451213]]                                                  \n",
      "Epoch 9, loss: 11.640648\n",
      "== W == -0.8889675281884365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[-0.00869133  0.04563043 -0.01601259 ...  0.05160961 -0.01284288 [2 6 6 ... 2 7 3]\n",
      "                           -0.01064963]                                                                     \n",
      "                          [-0.0050781   0.05269926  0.03052996 ...  0.01582102 -0.01122144                  \n",
      "                           -0.01172351]                                                                     \n",
      "                          [ 0.04271696 -0.01610603  0.01666659 ... -0.09489274  0.04510488                  \n",
      "                            0.06686923]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.03389755 -0.03060615  0.01078164 ... -0.06844595  0.04619472                  \n",
      "                            0.05578093]                                                                     \n",
      "                          [ 0.00757573  0.00531678 -0.00372191 ... -0.03875431  0.01357111                  \n",
      "                            0.03396146]                                                                     \n",
      "                          [-0.00046387 -0.02088556  0.02878564 ...  0.00137115  0.00548156                  \n",
      "                            0.00390115]]                                                                    \n",
      "soft max = [[1.10098738e-05 1.16244920e-05 1.09295620e-05 ... 1.16942051e-05\n",
      "             1.09642605e-05 1.09883343e-05]                                 \n",
      "            [1.10497271e-05 1.17069547e-05 1.14502754e-05 ... 1.12830866e-05\n",
      "             1.09820528e-05 1.09765405e-05]                                 \n",
      "            [1.15906738e-05 1.09285408e-05 1.12926312e-05 ... 1.01005622e-05\n",
      "             1.16183844e-05 1.18740228e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.14889002e-05 1.07712190e-05 1.12263698e-05 ... 1.03712533e-05\n",
      "             1.16310535e-05 1.17430873e-05]                                 \n",
      "            [1.11904368e-05 1.11651866e-05 1.10647227e-05 ... 1.06838100e-05\n",
      "             1.12577292e-05 1.14896345e-05]                                 \n",
      "            [1.11008308e-05 1.08764322e-05 1.14303199e-05 ... 1.11212198e-05\n",
      "             1.11670266e-05 1.11493921e-05]]                                \n",
      "log =  102735.71439298263\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.41507937699807 [[ 1.10098738e-05  1.16244920e-05 -1.00181549e-04 ...  1.16942051e-05 \n",
      "                                                  1.09642605e-05  1.09883343e-05]                                    \n",
      "                                                [ 1.10497271e-05  1.17069547e-05  1.14502754e-05 ...  1.12830866e-05 \n",
      "                                                  1.09820528e-05  1.09765405e-05]                                    \n",
      "                                                [ 1.15906738e-05  1.09285408e-05  1.12926312e-05 ...  1.01005622e-05 \n",
      "                                                  1.16183844e-05  1.18740228e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [ 1.14889002e-05  1.07712190e-05 -9.98847413e-05 ...  1.03712533e-05 \n",
      "                                                  1.16310535e-05  1.17430873e-05]                                    \n",
      "                                                [ 1.11904368e-05  1.11651866e-05  1.10647227e-05 ... -1.00427301e-04 \n",
      "                                                  1.12577292e-05  1.14896345e-05]                                    \n",
      "                                                [ 1.11008308e-05  1.08764322e-05  1.14303199e-05 ...  1.11212198e-05 \n",
      "                                                  1.11670266e-05  1.11493921e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.41507937699807 [[ 1.10098738e-05  1.16244920e-05 -1.00181549e-04 ...  1.16942051e-05 [[-0.00053681 -0.0002376   0.00016444 ...  0.0008247  -0.00140139 \n",
      "                                                             1.09642605e-05  1.09883343e-05]                                      -0.00023945]                                                    \n",
      "                                                           [ 1.10497271e-05  1.17069547e-05  1.14502754e-05 ...  1.12830866e-05  [-0.00108815  0.0004917   0.0006875  ...  0.00051884 -0.00168245 \n",
      "                                                             1.09820528e-05  1.09765405e-05]                                      -0.00028134]                                                    \n",
      "                                                           [ 1.15906738e-05  1.09285408e-05  1.12926312e-05 ...  1.01005622e-05  [-0.00140952  0.00107675  0.00126196 ...  0.00040364 -0.00176178 \n",
      "                                                             1.16183844e-05  1.18740228e-05]                                      -0.00087025]                                                    \n",
      "                                                           ...                                                                   ...                                                              \n",
      "                                                           [ 1.14889002e-05  1.07712190e-05 -9.98847413e-05 ...  1.03712533e-05  [-0.00035998  0.0006718  -0.00057064 ...  0.00144396 -0.00114627 \n",
      "                                                             1.16310535e-05  1.17430873e-05]                                      -0.00098908]                                                    \n",
      "                                                           [ 1.11904368e-05  1.11651866e-05  1.10647227e-05 ... -1.00427301e-04  [ 0.00026795 -0.00048385 -0.00079893 ...  0.00165824 -0.00061812 \n",
      "                                                             1.12577292e-05  1.14896345e-05]                                      -0.00093603]                                                    \n",
      "                                                           [ 1.11008308e-05  1.08764322e-05  1.14303199e-05 ...  1.11212198e-05  [ 0.03403533 -0.09817341 -0.04914152 ...  0.02347676  0.03363325 \n",
      "                                                             1.11670266e-05  1.11493921e-05]]                                      0.03809664]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.23028127684974092 [[-0.00915232 -0.01756414 -0.02781895 ... -0.01025301 -0.01373564\n",
      "                                        0.00389718]                                                   \n",
      "                                      [ 0.00584521 -0.00904796 -0.03755933 ...  0.06069159  0.00111534\n",
      "                                        0.03037413]                                                   \n",
      "                                      [-0.00103769  0.04946158 -0.03587685 ...  0.01309308 -0.05414378\n",
      "                                       -0.02670653]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.03651962  0.0138512  -0.02069793 ...  0.00469656 -0.00686587\n",
      "                                       -0.00701376]                                                   \n",
      "                                      [ 0.01691251  0.03095565  0.03684156 ... -0.00530566 -0.00824048\n",
      "                                        0.02221163]                                                   \n",
      "                                      [ 0.03747061 -0.15274256 -0.09468065 ...  0.02376777  0.03620244\n",
      "                                        0.03523815]]                                                  \n",
      "Epoch 10, loss: 11.645361\n",
      "== W == -0.9041482602028463\n",
      "enter of the function =  [[-0.02650081 -0.02379445 -0.06541642 ...  0.1311548  -0.05952958 [1 0 1 ... 2 0 3]\n",
      "                           -0.08935475]                                                                     \n",
      "                          [-0.09178871 -0.08367271 -0.08126331 ...  0.11882514 -0.07722115                  \n",
      "                           -0.10819013]                                                                     \n",
      "                          [-0.03159944 -0.00622917 -0.00029702 ...  0.0389028  -0.02885158                  \n",
      "                           -0.0320577 ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.02136278 -0.02877351  0.01250291 ... -0.10390247  0.04316006                  \n",
      "                            0.0319749 ]                                                                     \n",
      "                          [ 0.02546505 -0.00675481  0.00731353 ... -0.03572406  0.03525379                  \n",
      "                            0.04349475]                                                                     \n",
      "                          [-0.0030261  -0.01672553 -0.04284915 ... -0.00416158 -0.00303119                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.02046953]]                                                                    \n",
      "soft max = [[1.08150259e-05 1.08443350e-05 1.04022367e-05 ... 1.26618315e-05\n",
      "             1.04636536e-05 1.01561813e-05]                                 \n",
      "            [1.01314916e-05 1.02140534e-05 1.02386928e-05 ... 1.25066738e-05\n",
      "             1.02801630e-05 9.96667609e-06]                                 \n",
      "            [1.07600245e-05 1.10365015e-05 1.11021663e-05 ... 1.15460116e-05\n",
      "             1.07896321e-05 1.07550947e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.13452602e-05 1.07904746e-05 1.12451866e-05 ... 1.00095016e-05\n",
      "             1.15952708e-05 1.14662985e-05]                                 \n",
      "            [1.13918971e-05 1.10307018e-05 1.11869822e-05 ... 1.07157348e-05\n",
      "             1.15039570e-05 1.15991523e-05]                                 \n",
      "            [1.10719089e-05 1.09212643e-05 1.06396557e-05 ... 1.10593441e-05\n",
      "             1.10718526e-05 1.13351305e-05]]                                \n",
      "log =  102737.16240017368\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.415240266685965 [[ 1.08150259e-05 -1.00266776e-04  1.04022367e-05 ...  1.26618315e-05 \n",
      "                                                   1.04636536e-05  1.01561813e-05]                                    \n",
      "                                                 [-1.00979619e-04  1.02140534e-05  1.02386928e-05 ...  1.25066738e-05 \n",
      "                                                   1.02801630e-05  9.96667609e-06]                                    \n",
      "                                                 [ 1.07600245e-05 -1.00074610e-04  1.11021663e-05 ...  1.15460116e-05 \n",
      "                                                   1.07896321e-05  1.07550947e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.13452602e-05  1.07904746e-05 -9.98659245e-05 ...  1.00095016e-05 \n",
      "                                                   1.15952708e-05  1.14662985e-05]                                    \n",
      "                                                 [-9.97192140e-05  1.10307018e-05  1.11869822e-05 ...  1.07157348e-05 \n",
      "                                                   1.15039570e-05  1.15991523e-05]                                    \n",
      "                                                 [ 1.10719089e-05  1.09212643e-05  1.06396557e-05 ...  1.10593441e-05 \n",
      "                                                   1.10718526e-05  1.13351305e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.415240266685965 [[ 1.08150259e-05 -1.00266776e-04  1.04022367e-05 ...  1.26618315e-05 [[-0.00054429 -0.00023854  0.00015979 ...  0.00084038 -0.00141343 \n",
      "                                                              1.04636536e-05  1.01561813e-05]                                      -0.00025098]                                                    \n",
      "                                                            [-1.00979619e-04  1.02140534e-05  1.02386928e-05 ...  1.25066738e-05  [-0.0010957   0.0004909   0.00068291 ...  0.00053521 -0.00169506 \n",
      "                                                              1.02801630e-05  9.96667609e-06]                                      -0.00029351]                                                    \n",
      "                                                            [ 1.07600245e-05 -1.00074610e-04  1.11021663e-05 ...  1.15460116e-05  [-0.00141687  0.00107578  0.00125726 ...  0.00042106 -0.00177451 \n",
      "                                                              1.07896321e-05  1.07550947e-05]                                      -0.00088324]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.13452602e-05  1.07904746e-05 -9.98659245e-05 ...  1.00095016e-05  [-0.00036641  0.00067308 -0.0005764  ...  0.00146152 -0.00115705 \n",
      "                                                              1.15952708e-05  1.14662985e-05]                                      -0.00100111]                                                    \n",
      "                                                            [-9.97192140e-05  1.10307018e-05  1.11869822e-05 ...  1.07157348e-05  [ 0.0002618  -0.00048297 -0.00080461 ...  0.00167667 -0.0006289  \n",
      "                                                              1.15039570e-05  1.15991523e-05]                                      -0.00094877]                                                    \n",
      "                                                            [ 1.10719089e-05  1.09212643e-05  1.06396557e-05 ...  1.10593441e-05  [ 0.03403931 -0.09820159 -0.04915851 ...  0.02348375  0.03363902 \n",
      "                                                              1.10718526e-05  1.13351305e-05]]                                      0.03810362]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.23492837948607265 [[-0.00924921 -0.01774216 -0.02809549 ... -0.01034729 -0.01388701\n",
      "                                        0.00393376]                                                   \n",
      "                                      [ 0.00589278 -0.00913353 -0.03792805 ...  0.06130369  0.00110967\n",
      "                                        0.03067506]                                                   \n",
      "                                      [-0.00106216  0.04996696 -0.03622299 ...  0.01322804 -0.05470283\n",
      "                                       -0.0269823 ]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.03688842  0.01399643 -0.02091062 ...  0.00475796 -0.00694599\n",
      "                                       -0.00709378]                                                   \n",
      "                                      [ 0.01708431  0.03126037  0.03720198 ... -0.00534213 -0.00832906\n",
      "                                        0.02242439]                                                   \n",
      "                                      [ 0.03818567 -0.15525172 -0.09611887 ...  0.02424021  0.0369008 \n",
      "                                        0.0359715 ]]                                                  \n",
      "Epoch 11, loss: 11.650169\n",
      "== W == -0.919577232246435\n",
      "enter of the function =  [[-0.00079939 -0.04944736 -0.04844433 ... -0.00513956  0.00067938 [7 1 3 ... 2 2 2]\n",
      "                           -0.00155147]                                                                     \n",
      "                          [-0.01856008 -0.08720509 -0.05676713 ...  0.06262904 -0.05934661                  \n",
      "                           -0.01549096]                                                                     \n",
      "                          [ 0.01509344 -0.00959375  0.00185245 ...  0.002417    0.00066663                  \n",
      "                            0.02377216]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.06932126  0.03803333 -0.04615992 ...  0.06145042 -0.0645263                   \n",
      "                           -0.02893993]                                                                     \n",
      "                          [-0.03993769 -0.01381721 -0.03074313 ...  0.03801604 -0.07146839                  \n",
      "                           -0.04795781]                                                                     \n",
      "                          [-0.02098347  0.00315324 -0.02889129 ...  0.02023098 -0.01700692                  \n",
      "                            0.02141829]]                                                                    \n",
      "soft max = [[1.10960519e-05 1.05691712e-05 1.05797777e-05 ... 1.10479975e-05\n",
      "             1.11124725e-05 1.10877099e-05]                                 \n",
      "            [1.09007181e-05 1.01775433e-05 1.04920898e-05 ... 1.18226572e-05\n",
      "             1.04650605e-05 1.09342251e-05]                                 \n",
      "            [1.12738083e-05 1.09988970e-05 1.11255159e-05 ... 1.11317985e-05\n",
      "             1.11123309e-05 1.13720764e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.03611940e-05 1.15354174e-05 1.06039739e-05 ... 1.18087310e-05\n",
      "             1.04109949e-05 1.07881554e-05]                                 \n",
      "            [1.06701600e-05 1.09525416e-05 1.07687198e-05 ... 1.15352180e-05\n",
      "             1.03389711e-05 1.05849263e-05]                                 \n",
      "            [1.08743334e-05 1.11399972e-05 1.07886802e-05 ... 1.13318770e-05\n",
      "             1.09176618e-05 1.13453394e-05]]                                \n",
      "log =  102738.63985017546\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.415404427797274 [[ 1.10960519e-05  1.05691712e-05  1.05797777e-05 ... -1.00063114e-04 \n",
      "                                                   1.11124725e-05  1.10877099e-05]                                    \n",
      "                                                 [ 1.09007181e-05 -1.00933568e-04  1.04920898e-05 ...  1.18226572e-05 \n",
      "                                                   1.04650605e-05  1.09342251e-05]                                    \n",
      "                                                 [ 1.12738083e-05  1.09988970e-05  1.11255159e-05 ...  1.11317985e-05 \n",
      "                                                   1.11123309e-05  1.13720764e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.03611940e-05  1.15354174e-05 -1.00507137e-04 ...  1.18087310e-05 \n",
      "                                                   1.04109949e-05  1.07881554e-05]                                    \n",
      "                                                 [ 1.06701600e-05  1.09525416e-05 -1.00342391e-04 ...  1.15352180e-05 \n",
      "                                                   1.03389711e-05  1.05849263e-05]                                    \n",
      "                                                 [ 1.08743334e-05  1.11399972e-05 -1.00322431e-04 ...  1.13318770e-05 \n",
      "                                                   1.09176618e-05  1.13453394e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.415404427797274 [[ 1.10960519e-05  1.05691712e-05  1.05797777e-05 ... -1.00063114e-04 [[-0.00055189 -0.00023949  0.00015507 ...  0.00085635 -0.00142567 \n",
      "                                                              1.11124725e-05  1.10877099e-05]                                      -0.0002627 ]                                                    \n",
      "                                                            [ 1.09007181e-05 -1.00933568e-04  1.04920898e-05 ...  1.18226572e-05  [-0.00110337  0.00049008  0.00067824 ...  0.00055189 -0.00170787 \n",
      "                                                              1.04650605e-05  1.09342251e-05]                                      -0.00030588]                                                    \n",
      "                                                            [ 1.12738083e-05  1.09988970e-05  1.11255159e-05 ...  1.11317985e-05  [-0.00142434  0.0010748   0.00125248 ...  0.00043881 -0.00178744 \n",
      "                                                              1.11123309e-05  1.13720764e-05]                                      -0.00089644]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.03611940e-05  1.15354174e-05 -1.00507137e-04 ...  1.18087310e-05  [-0.00037295  0.00067437 -0.00058225 ...  0.00147941 -0.00116802 \n",
      "                                                              1.04109949e-05  1.07881554e-05]                                      -0.00101334]                                                    \n",
      "                                                            [ 1.06701600e-05  1.09525416e-05 -1.00342391e-04 ...  1.15352180e-05  [ 0.00025554 -0.00048209 -0.00081039 ...  0.00169542 -0.00063986 \n",
      "                                                              1.03389711e-05  1.05849263e-05]                                      -0.00096171]                                                    \n",
      "                                                            [ 1.08743334e-05  1.11399972e-05 -1.00322431e-04 ...  1.13318770e-05  [ 0.03404322 -0.09823018 -0.04917578 ...  0.02349085  0.03364478 \n",
      "                                                              1.09176618e-05  1.13453394e-05]]                                      0.03811062]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.2396693319859268 [[-0.00934714 -0.01792196 -0.02837485 ... -0.01044236 -0.01404001\n",
      "                                       0.00397059]                                                   \n",
      "                                     [ 0.00594075 -0.00921995 -0.0383005  ...  0.06192208  0.00110382\n",
      "                                       0.03097888]                                                   \n",
      "                                     [-0.00108695  0.05047739 -0.03657265 ...  0.01336453 -0.05526761\n",
      "                                      -0.02726096]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.03726097  0.01414313 -0.02112549 ...  0.00482016 -0.00702702\n",
      "                                      -0.00717473]                                                   \n",
      "                                     [ 0.01725777  0.03156814  0.03756596 ... -0.00537879 -0.00841864\n",
      "                                       0.02263914]                                                   \n",
      "                                     [ 0.03890792 -0.15778625 -0.09757164 ...  0.02471745  0.0376062 \n",
      "                                       0.03671225]]                                                  \n",
      "Epoch 12, loss: 11.655074\n",
      "== W == -0.9352581447877418\n",
      "enter of the function =  [[-0.01725314 -0.03912313 -0.00888232 ...  0.05054556 -0.02371137 [6 5 1 ... 6 1 2]\n",
      "                           -0.0156838 ]                                                                     \n",
      "                          [ 0.02684205  0.03459049  0.01720944 ...  0.03314221  0.0648574                   \n",
      "                            0.02211082]                                                                     \n",
      "                          [ 0.0468772  -0.00825079 -0.00483765 ... -0.03632505  0.06264102                  \n",
      "                            0.05762869]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.02575362  0.01507124  0.05091173 ... -0.12071558 -0.01060648                  \n",
      "                            0.04481387]                                                                     \n",
      "                          [ 0.02218814 -0.01408124 -0.00373798 ... -0.07863444  0.07485375                  \n",
      "                            0.06864381]                                                                     \n",
      "                          [-0.04634067 -0.05749702 -0.05802662 ...  0.07414468 -0.04666992                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.02445072]]                                                                    \n",
      "soft max = [[1.09144222e-05 1.06783151e-05 1.10061683e-05 ... 1.16800674e-05\n",
      "             1.08441614e-05 1.09315641e-05]                                 \n",
      "            [1.14064644e-05 1.14951899e-05 1.12971178e-05 ... 1.14785537e-05\n",
      "             1.18484326e-05 1.13526252e-05]                                 \n",
      "            [1.16372992e-05 1.10131213e-05 1.10507747e-05 ... 1.07082358e-05\n",
      "             1.18222010e-05 1.17630925e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.13940559e-05 1.12729881e-05 1.16843451e-05 ... 9.84164247e-06\n",
      "             1.09872082e-05 1.16133124e-05]                                 \n",
      "            [1.13535030e-05 1.09490966e-05 1.10629337e-05 ... 1.02646274e-05\n",
      "             1.19674677e-05 1.18933807e-05]                                 \n",
      "            [1.06015214e-05 1.04839044e-05 1.04783537e-05 ... 1.19589850e-05\n",
      "             1.05980314e-05 1.08361467e-05]]                                \n",
      "log =  102740.14753749265\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.415571948610294 [[ 1.09144222e-05  1.06783151e-05  1.10061683e-05 ...  1.16800674e-05 \n",
      "                                                   1.08441614e-05  1.09315641e-05]                                    \n",
      "                                                 [ 1.14064644e-05  1.14951899e-05  1.12971178e-05 ...  1.14785537e-05 \n",
      "                                                   1.18484326e-05  1.13526252e-05]                                    \n",
      "                                                 [ 1.16372992e-05 -1.00097990e-04  1.10507747e-05 ...  1.07082358e-05 \n",
      "                                                   1.18222010e-05  1.17630925e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.13940559e-05  1.12729881e-05  1.16843451e-05 ...  9.84164247e-06 \n",
      "                                                   1.09872082e-05  1.16133124e-05]                                    \n",
      "                                                 [ 1.13535030e-05 -1.00162015e-04  1.10629337e-05 ...  1.02646274e-05 \n",
      "                                                   1.19674677e-05  1.18933807e-05]                                    \n",
      "                                                 [ 1.06015214e-05  1.04839044e-05 -1.00632757e-04 ...  1.19589850e-05 \n",
      "                                                   1.05980314e-05  1.08361467e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.415571948610294 [[ 1.09144222e-05  1.06783151e-05  1.10061683e-05 ...  1.16800674e-05 [[-0.00055961 -0.00024046  0.00015026 ...  0.00087263 -0.0014381  \n",
      "                                                              1.08441614e-05  1.09315641e-05]                                      -0.00027462]                                                    \n",
      "                                                            [ 1.14064644e-05  1.14951899e-05  1.12971178e-05 ...  1.14785537e-05  [-0.00111116  0.00048926  0.00067347 ...  0.00056887 -0.00172089 \n",
      "                                                              1.18484326e-05  1.13526252e-05]                                      -0.00031846]                                                    \n",
      "                                                            [ 1.16372992e-05 -1.00097990e-04  1.10507747e-05 ...  1.07082358e-05  [-0.00143194  0.00107381  0.00124761 ...  0.00045688 -0.00180058 \n",
      "                                                              1.18222010e-05  1.17630925e-05]                                      -0.00090986]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.13940559e-05  1.12729881e-05  1.16843451e-05 ...  9.84164247e-06  [-0.00037961  0.00067567 -0.00058821 ...  0.00149762 -0.00117917 \n",
      "                                                              1.09872082e-05  1.16133124e-05]                                      -0.00102578]                                                    \n",
      "                                                            [ 1.13535030e-05 -1.00162015e-04  1.10629337e-05 ...  1.02646274e-05  [ 0.00024916 -0.0004812  -0.00081626 ...  0.00171452 -0.00065101 \n",
      "                                                              1.19674677e-05  1.18933807e-05]                                      -0.00097487]                                                    \n",
      "                                                            [ 1.06015214e-05  1.04839044e-05 -1.00632757e-04 ...  1.19589850e-05  [ 0.03404705 -0.09825919 -0.04919334 ...  0.02349807  0.03365054 \n",
      "                                                              1.05980314e-05  1.08361467e-05]]                                      0.03811765]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.24450603162098963 [[-0.00944613 -0.01810358 -0.02865705 ... -0.01053822 -0.01419467\n",
      "                                        0.00400767]                                                   \n",
      "                                      [ 0.00598912 -0.00930725 -0.03867672 ...  0.06254682  0.00109778\n",
      "                                        0.0312856 ]                                                   \n",
      "                                      [-0.00111206  0.05099291 -0.03692585 ...  0.01350257 -0.05583816\n",
      "                                       -0.02754253]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.03763731  0.0142913  -0.02134257 ...  0.00488315 -0.00710897\n",
      "                                       -0.00725661]                                                   \n",
      "                                      [ 0.01743291  0.031879    0.03793351 ... -0.00541562 -0.00850923\n",
      "                                        0.02285592]                                                   \n",
      "                                      [ 0.03963743 -0.16034642 -0.09903912 ...  0.02519954  0.03831871\n",
      "                                        0.03746048]]                                                  \n",
      "Epoch 13, loss: 11.660078\n",
      "== W == -0.951194739422413\n",
      "enter of the function =  [[ 0.03313866 -0.10401085 -0.00170712 ... -0.03118404  0.01063076 [1 3 6 ... 0 2 2]\n",
      "                            0.02888768]                                                                     \n",
      "                          [-0.03404632 -0.01387937 -0.00681213 ...  0.01702479 -0.01009755                  \n",
      "                           -0.01384237]                                                                     \n",
      "                          [-0.00829967 -0.02194089 -0.02418506 ...  0.07321211 -0.02548724                  \n",
      "                           -0.02587006]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.01325566  0.00764875  0.06411052 ... -0.01448591  0.03308626                  \n",
      "                           -0.01768965]                                                                     \n",
      "                          [ 0.0246419  -0.01985837 -0.01229722 ...  0.00836765  0.01842104                  \n",
      "                            0.04178327]                                                                     \n",
      "                          [-0.0293193  -0.02819936 -0.05074747 ...  0.07513303 -0.04234288                  \n",
      "                           -0.05407401]]                                                                    \n",
      "soft max = [[1.14779085e-05 1.00068982e-05 1.10848399e-05 ... 1.07628618e-05\n",
      "             1.12224505e-05 1.14292196e-05]                                 \n",
      "            [1.07320995e-05 1.09507303e-05 1.10283959e-05 ... 1.12944371e-05\n",
      "             1.09922225e-05 1.09511355e-05]                                 \n",
      "            [1.10120029e-05 1.08628057e-05 1.08384551e-05 ... 1.19472082e-05\n",
      "             1.08243506e-05 1.08202077e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.12519471e-05 1.11890349e-05 1.18389631e-05 ... 1.09440903e-05\n",
      "             1.14773070e-05 1.09090844e-05]                                 \n",
      "            [1.13807966e-05 1.08854512e-05 1.09680698e-05 ... 1.11970816e-05\n",
      "             1.13102180e-05 1.15775605e-05]                                 \n",
      "            [1.07829504e-05 1.07950334e-05 1.05543495e-05 ... 1.19701799e-05\n",
      "             1.06434283e-05 1.05192984e-05]]                                \n",
      "log =  102741.68628206171\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.415742920229079 [[ 1.14779085e-05 -1.01104213e-04  1.10848399e-05 ...  1.07628618e-05 \n",
      "                                                   1.12224505e-05  1.14292196e-05]                                    \n",
      "                                                 [ 1.07320995e-05  1.09507303e-05  1.10283959e-05 ...  1.12944371e-05 \n",
      "                                                   1.09922225e-05  1.09511355e-05]                                    \n",
      "                                                 [ 1.10120029e-05  1.08628057e-05  1.08384551e-05 ...  1.19472082e-05 \n",
      "                                                   1.08243506e-05  1.08202077e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [-9.98591640e-05  1.11890349e-05  1.18389631e-05 ...  1.09440903e-05 \n",
      "                                                   1.14773070e-05  1.09090844e-05]                                    \n",
      "                                                 [ 1.13807966e-05  1.08854512e-05 -1.00143041e-04 ...  1.11970816e-05 \n",
      "                                                   1.13102180e-05  1.15775605e-05]                                    \n",
      "                                                 [ 1.07829504e-05  1.07950334e-05 -1.00556762e-04 ...  1.19701799e-05 \n",
      "                                                   1.06434283e-05  1.05192984e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.415742920229079 [[ 1.14779085e-05 -1.01104213e-04  1.10848399e-05 ...  1.07628618e-05 [[-0.00056745 -0.00024144  0.00014536 ...  0.0008892  -0.00145073 \n",
      "                                                              1.12224505e-05  1.14292196e-05]                                      -0.00028675]                                                    \n",
      "                                                            [ 1.07320995e-05  1.09507303e-05  1.10283959e-05 ...  1.12944371e-05  [-0.00111909  0.00048841  0.00066863 ...  0.00058617 -0.00173412 \n",
      "                                                              1.09922225e-05  1.09511355e-05]                                      -0.00033125]                                                    \n",
      "                                                            [ 1.10120029e-05  1.08628057e-05  1.08384551e-05 ...  1.19472082e-05  [-0.00143966  0.00107279  0.00124266 ...  0.00047529 -0.00181393 \n",
      "                                                              1.08243506e-05  1.08202077e-05]                                      -0.0009235 ]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [-9.98591640e-05  1.11890349e-05  1.18389631e-05 ...  1.09440903e-05  [-0.00038638  0.00067697 -0.00059425 ...  0.00151617 -0.00119052 \n",
      "                                                              1.14773070e-05  1.09090844e-05]                                      -0.00103843]                                                    \n",
      "                                                            [ 1.13807966e-05  1.08854512e-05 -1.00143041e-04 ...  1.11970816e-05  [ 0.00024268 -0.00048031 -0.00082222 ...  0.00173396 -0.00066235 \n",
      "                                                              1.13102180e-05  1.15775605e-05]                                      -0.00098825]                                                    \n",
      "                                                            [ 1.07829504e-05  1.07950334e-05 -1.00556762e-04 ...  1.19701799e-05  [ 0.0340508  -0.09828863 -0.0492112  ...  0.0235054   0.03365628 \n",
      "                                                              1.06434283e-05  1.05192984e-05]]                                      0.03812469]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.2494404141301793 [[-0.00954619 -0.01828702 -0.02894211 ... -0.01063488 -0.01435099\n",
      "                                       0.004045  ]                                                   \n",
      "                                     [ 0.0060379  -0.00939543 -0.03905676 ...  0.06317798  0.00109154\n",
      "                                       0.03159528]                                                   \n",
      "                                     [-0.0011375   0.05151358 -0.03728264 ...  0.01364216 -0.05641455\n",
      "                                      -0.02782705]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.03801748  0.01444097 -0.02156188 ...  0.00494696 -0.00719186\n",
      "                                      -0.00733944]                                                   \n",
      "                                     [ 0.01760973  0.03219298  0.03830468 ... -0.00545263 -0.00860083\n",
      "                                       0.02307473]                                                   \n",
      "                                     [ 0.04037427 -0.16293247 -0.10052144 ...  0.02568651  0.0390384 \n",
      "                                       0.03821626]]                                                  \n",
      "Epoch 14, loss: 11.665183\n",
      "== W == -0.9673907986729091\n",
      "enter of the function =  [[ 0.01499805 -0.02523092  0.00576186 ... -0.08938561  0.02013781 [5 0 3 ... 2 5 2]\n",
      "                            0.05945629]                                                                     \n",
      "                          [-0.02940238  0.00485267 -0.00811365 ...  0.04225958 -0.02735156                  \n",
      "                           -0.02214576]                                                                     \n",
      "                          [-0.01144261 -0.01427419  0.00681278 ...  0.01732425  0.05175456                  \n",
      "                           -0.04096114]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.01265135 -0.01965045 -0.04783541 ...  0.0419041  -0.01868383                  \n",
      "                           -0.01785883]                                                                     \n",
      "                          [ 0.06549688  0.01113186  0.02228663 ... -0.07962935  0.06074669                  \n",
      "                            0.06523396]                                                                     \n",
      "                          [-0.05761819 -0.0537487  -0.0495745  ...  0.16468954 -0.09865656                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.11329908]]                                                                    \n",
      "soft max = [[1.12709511e-05 1.08265315e-05 1.11673297e-05 ... 1.01537699e-05\n",
      "             1.13290302e-05 1.17833434e-05]                                 \n",
      "            [1.07814631e-05 1.11571811e-05 1.10134473e-05 ... 1.15824410e-05\n",
      "             1.08035967e-05 1.08599847e-05]                                 \n",
      "            [1.09768450e-05 1.09458072e-05 1.11790719e-05 ... 1.12972001e-05\n",
      "             1.16929398e-05 1.06575603e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.09635848e-05 1.08871176e-05 1.05845486e-05 ... 1.15783244e-05\n",
      "             1.08976464e-05 1.09066406e-05]                                 \n",
      "            [1.18547371e-05 1.12274596e-05 1.13534004e-05 ... 1.02533176e-05\n",
      "             1.17985584e-05 1.18516207e-05]                                 \n",
      "            [1.04815071e-05 1.05221438e-05 1.05661571e-05 ... 1.30909376e-05\n",
      "             1.00600698e-05 9.91383827e-06]]                                \n",
      "log =  102743.25693014734\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.415917436683038 [[ 1.12709511e-05  1.08265315e-05  1.11673297e-05 ...  1.01537699e-05 \n",
      "                                                   1.13290302e-05  1.17833434e-05]                                    \n",
      "                                                 [-1.00329648e-04  1.11571811e-05  1.10134473e-05 ...  1.15824410e-05 \n",
      "                                                   1.08035967e-05  1.08599847e-05]                                    \n",
      "                                                 [ 1.09768450e-05  1.09458072e-05  1.11790719e-05 ...  1.12972001e-05 \n",
      "                                                   1.16929398e-05  1.06575603e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.09635848e-05  1.08871176e-05 -1.00526563e-04 ...  1.15783244e-05 \n",
      "                                                   1.08976464e-05  1.09066406e-05]                                    \n",
      "                                                 [ 1.18547371e-05  1.12274596e-05  1.13534004e-05 ...  1.02533176e-05 \n",
      "                                                   1.17985584e-05  1.18516207e-05]                                    \n",
      "                                                 [ 1.04815071e-05  1.05221438e-05 -1.00544954e-04 ...  1.30909376e-05 \n",
      "                                                   1.00600698e-05  9.91383827e-06]]                                   \n",
      "loss , grand (prediction), grad by W =  11.415917436683038 [[ 1.12709511e-05  1.08265315e-05  1.11673297e-05 ...  1.01537699e-05 [[-0.00057542 -0.00024244  0.00014038 ...  0.00090609 -0.00146356 \n",
      "                                                              1.13290302e-05  1.17833434e-05]                                      -0.00029908]                                                    \n",
      "                                                            [-1.00329648e-04  1.11571811e-05  1.10134473e-05 ...  1.15824410e-05  [-0.00112714  0.00048756  0.00066369 ...  0.0006038  -0.00174755 \n",
      "                                                              1.08035967e-05  1.08599847e-05]                                      -0.00034426]                                                    \n",
      "                                                            [ 1.09768450e-05  1.09458072e-05  1.11790719e-05 ...  1.12972001e-05  [-0.00144751  0.00107177  0.00123762 ...  0.00049403 -0.00182749 \n",
      "                                                              1.16929398e-05  1.06575603e-05]                                      -0.00093737]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.09635848e-05  1.08871176e-05 -1.00526563e-04 ...  1.15783244e-05  [-0.00039327  0.00067828 -0.0006004  ...  0.00153505 -0.00120206 \n",
      "                                                              1.08976464e-05  1.09066406e-05]                                      -0.00105129]                                                    \n",
      "                                                            [ 1.18547371e-05  1.12274596e-05  1.13534004e-05 ...  1.02533176e-05  [ 0.00023608 -0.00047942 -0.00082829 ...  0.00175375 -0.00067389 \n",
      "                                                              1.17985584e-05  1.18516207e-05]                                      -0.00100185]                                                    \n",
      "                                                            [ 1.04815071e-05  1.05221438e-05 -1.00544954e-04 ...  1.30909376e-05  [ 0.03405445 -0.09831851 -0.04922936 ...  0.02351285  0.03366201 \n",
      "                                                              1.00600698e-05  9.91383827e-06]]                                      0.03813176]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.2544744545041841 [[-0.00964733 -0.0184723  -0.02923008 ... -0.01073233 -0.01450901\n",
      "                                       0.00408258]                                                   \n",
      "                                     [ 0.00608709 -0.0094845  -0.03944064 ...  0.06381562  0.00108512\n",
      "                                       0.03190792]                                                   \n",
      "                                     [-0.00116328  0.05203944 -0.03764304 ...  0.01378334 -0.05699683\n",
      "                                      -0.02811456]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.03840151  0.01459215 -0.02178344 ...  0.00501159 -0.00727568\n",
      "                                      -0.00742322]                                                   \n",
      "                                     [ 0.01778825  0.03251011  0.03867951 ... -0.00548982 -0.00869346\n",
      "                                       0.02329559]                                                   \n",
      "                                     [ 0.04111852 -0.16554468 -0.10201877 ...  0.02617843  0.03976535\n",
      "                                       0.03897967]]                                                  \n",
      "Epoch 15, loss: 11.670392\n",
      "== W == -0.9838501457457489\n",
      "enter of the function =  [[-0.05174824 -0.07917801 -0.02786849 ...  0.08658529 -0.04683819 [6 1 3 ... 1 4 2]\n",
      "                           -0.05263053]                                                                     \n",
      "                          [-0.06288532  0.01045069 -0.06149176 ...  0.1572209  -0.09594701                  \n",
      "                           -0.10364284]                                                                     \n",
      "                          [-0.02766853 -0.01720424 -0.0441422  ...  0.02445108 -0.03147162                  \n",
      "                           -0.02986666]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.01323459 -0.05439092  0.02472373 ... -0.04483441  0.01042731                  \n",
      "                            0.03813518]                                                                     \n",
      "                          [-0.0170059   0.03136527  0.0135837  ...  0.0043074  -0.01298295                  \n",
      "                            0.03243497]                                                                     \n",
      "                          [ 0.0139735   0.00056408  0.02188694 ... -0.09202211  0.0246626                   \n",
      "                            0.05334424]]                                                                    \n",
      "soft max = [[1.05426117e-05 1.02573603e-05 1.07973965e-05 ... 1.21066977e-05\n",
      "             1.05945037e-05 1.05333141e-05]                                 \n",
      "            [1.04258492e-05 1.12191735e-05 1.04403884e-05 ... 1.29927881e-05\n",
      "             1.00867887e-05 1.00094605e-05]                                 \n",
      "            [1.07995558e-05 1.09131589e-05 1.06231049e-05 ... 1.13773509e-05\n",
      "             1.07585621e-05 1.07758430e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.12504500e-05 1.05147877e-05 1.13804534e-05 ... 1.06157541e-05\n",
      "             1.12189111e-05 1.15341099e-05]                                 \n",
      "            [1.09153236e-05 1.14562886e-05 1.12543783e-05 ... 1.11504620e-05\n",
      "             1.09593239e-05 1.14685500e-05]                                 \n",
      "            [1.12587662e-05 1.11088004e-05 1.13482152e-05 ... 1.01264562e-05\n",
      "             1.13797577e-05 1.17108736e-05]]                                \n",
      "log =  102744.8603552733\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.416095595030367 [[ 1.05426117e-05  1.02573603e-05  1.07973965e-05 ...  1.21066977e-05 \n",
      "                                                   1.05945037e-05  1.05333141e-05]                                    \n",
      "                                                 [ 1.04258492e-05 -9.98919377e-05  1.04403884e-05 ...  1.29927881e-05 \n",
      "                                                   1.00867887e-05  1.00094605e-05]                                    \n",
      "                                                 [ 1.07995558e-05  1.09131589e-05  1.06231049e-05 ...  1.13773509e-05 \n",
      "                                                   1.07585621e-05  1.07758430e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.12504500e-05 -1.00596323e-04  1.13804534e-05 ...  1.06157541e-05 \n",
      "                                                   1.12189111e-05  1.15341099e-05]                                    \n",
      "                                                 [ 1.09153236e-05  1.14562886e-05  1.12543783e-05 ...  1.11504620e-05 \n",
      "                                                   1.09593239e-05  1.14685500e-05]                                    \n",
      "                                                 [ 1.12587662e-05  1.11088004e-05 -9.97628959e-05 ...  1.01264562e-05 \n",
      "                                                   1.13797577e-05  1.17108736e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.416095595030367 [[ 1.05426117e-05  1.02573603e-05  1.07973965e-05 ...  1.21066977e-05 [[-0.00058352 -0.00024345  0.00013531 ...  0.00092329 -0.0014766  \n",
      "                                                              1.05945037e-05  1.05333141e-05]                                      -0.00031161]                                                    \n",
      "                                                            [ 1.04258492e-05 -9.98919377e-05  1.04403884e-05 ...  1.29927881e-05  [-0.00113532  0.00048668  0.00065867 ...  0.00062175 -0.0017612  \n",
      "                                                              1.00867887e-05  1.00094605e-05]                                      -0.00035748]                                                    \n",
      "                                                            [ 1.07995558e-05  1.09131589e-05  1.06231049e-05 ...  1.13773509e-05  [-0.00145549  0.00107072  0.00123248 ...  0.00051312 -0.00184127 \n",
      "                                                              1.07585621e-05  1.07758430e-05]                                      -0.00095147]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.12504500e-05 -1.00596323e-04  1.13804534e-05 ...  1.06157541e-05  [-0.00040028  0.00067959 -0.00060665 ...  0.00155428 -0.00121379 \n",
      "                                                              1.12189111e-05  1.15341099e-05]                                      -0.00106437]                                                    \n",
      "                                                            [ 1.09153236e-05  1.14562886e-05  1.12543783e-05 ...  1.11504620e-05  [ 0.00022936 -0.00047852 -0.00083446 ...  0.0017739  -0.00068562 \n",
      "                                                              1.09593239e-05  1.14685500e-05]                                      -0.00101568]                                                    \n",
      "                                                            [ 1.12587662e-05  1.11088004e-05 -9.97628959e-05 ...  1.01264562e-05  [ 0.03405801 -0.09834884 -0.04924783 ...  0.02352043  0.03366773 \n",
      "                                                              1.13797577e-05  1.17108736e-05]]                                      0.03813885]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.25961016778618695 [[-0.00974955 -0.01865945 -0.02952098 ... -0.0108306  -0.01466874\n",
      "                                        0.00412042]                                                   \n",
      "                                      [ 0.00613669 -0.00957447 -0.03982841 ...  0.06445981  0.00107849\n",
      "                                        0.03222355]                                                   \n",
      "                                      [-0.00118938  0.05257055 -0.03800709 ...  0.01392611 -0.05758507\n",
      "                                       -0.02840508]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.03878946  0.01474486 -0.02200728 ...  0.00507706 -0.00736046\n",
      "                                       -0.00750796]                                                   \n",
      "                                      [ 0.01796849  0.03283042  0.03905802 ... -0.00552718 -0.00878713\n",
      "                                        0.02351853]                                                   \n",
      "                                      [ 0.04187025 -0.16818332 -0.10353125 ...  0.02667534  0.04049962\n",
      "                                        0.03975078]]                                                  \n",
      "Epoch 16, loss: 11.675706\n",
      "== W == -1.0005766442437354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[ 0.02649722  0.0140835   0.02761074 ... -0.09093362  0.01979403 [5 6 2 ... 1 8 6]\n",
      "                            0.04231853]                                                                     \n",
      "                          [ 0.04556736  0.01233435  0.03358863 ... -0.05576326  0.07189737                  \n",
      "                            0.05004455]                                                                     \n",
      "                          [-0.02000163  0.03218705 -0.08266883 ...  0.04551313 -0.02759283                  \n",
      "                           -0.07567412]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.03309909 -0.01905779 -0.05704074 ...  0.1328201  -0.04490901                  \n",
      "                           -0.07240236]                                                                     \n",
      "                          [-0.0134261  -0.01905041  0.00542358 ...  0.01623322 -0.05160305                  \n",
      "                           -0.05982934]                                                                     \n",
      "                          [ 0.00756177 -0.01852804 -0.00322038 ... -0.01919322  0.01633724                  \n",
      "                            0.01480857]]                                                                    \n",
      "soft max = [[1.13999763e-05 1.12593349e-05 1.14126775e-05 ... 1.01368819e-05\n",
      "             1.13238157e-05 1.15817732e-05]                                 \n",
      "            [1.16194617e-05 1.12396579e-05 1.14811055e-05 ... 1.04997431e-05\n",
      "             1.19294654e-05 1.16716008e-05]                                 \n",
      "            [1.08820259e-05 1.14650251e-05 1.02210081e-05 ... 1.16188315e-05\n",
      "             1.07997310e-05 1.02927518e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.07404283e-05 1.08923017e-05 1.04863386e-05 ... 1.26788361e-05\n",
      "             1.06143308e-05 1.03264823e-05]                                 \n",
      "            [1.09538167e-05 1.08923820e-05 1.11622510e-05 ... 1.12835654e-05\n",
      "             1.05435152e-05 1.04571371e-05]                                 \n",
      "            [1.11861436e-05 1.08980734e-05 1.10661807e-05 ... 1.08908266e-05\n",
      "             1.12847392e-05 1.12675017e-05]]                                \n",
      "log =  102746.49745918898\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.416277495465442 [[ 1.13999763e-05  1.12593349e-05  1.14126775e-05 ...  1.01368819e-05 \n",
      "                                                   1.13238157e-05  1.15817732e-05]                                    \n",
      "                                                 [ 1.16194617e-05  1.12396579e-05  1.14811055e-05 ...  1.04997431e-05 \n",
      "                                                   1.19294654e-05  1.16716008e-05]                                    \n",
      "                                                 [ 1.08820259e-05  1.14650251e-05 -1.00890103e-04 ...  1.16188315e-05 \n",
      "                                                   1.07997310e-05  1.02927518e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.07404283e-05 -1.00218809e-04  1.04863386e-05 ...  1.26788361e-05 \n",
      "                                                   1.06143308e-05  1.03264823e-05]                                    \n",
      "                                                 [ 1.09538167e-05  1.08923820e-05  1.11622510e-05 ...  1.12835654e-05 \n",
      "                                                  -1.00567596e-04  1.04571371e-05]                                    \n",
      "                                                 [ 1.11861436e-05  1.08980734e-05  1.10661807e-05 ...  1.08908266e-05 \n",
      "                                                   1.12847392e-05  1.12675017e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.416277495465442 [[ 1.13999763e-05  1.12593349e-05  1.14126775e-05 ...  1.01368819e-05 [[-0.00059175 -0.00024448  0.00013016 ...  0.00094081 -0.00148985 \n",
      "                                                              1.13238157e-05  1.15817732e-05]                                      -0.00032436]                                                    \n",
      "                                                            [ 1.16194617e-05  1.12396579e-05  1.14811055e-05 ...  1.04997431e-05  [-0.00114363  0.0004858   0.00065356 ...  0.00064004 -0.00177507 \n",
      "                                                              1.19294654e-05  1.16716008e-05]                                      -0.00037093]                                                    \n",
      "                                                            [ 1.08820259e-05  1.14650251e-05 -1.00890103e-04 ...  1.16188315e-05  [-0.0014636   0.00106966  0.00122725 ...  0.00053256 -0.00185527 \n",
      "                                                              1.07997310e-05  1.02927518e-05]                                      -0.00096581]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.07404283e-05 -1.00218809e-04  1.04863386e-05 ...  1.26788361e-05  [-0.00040741  0.00068091 -0.000613   ...  0.00157387 -0.00122573 \n",
      "                                                              1.06143308e-05  1.03264823e-05]                                      -0.00107767]                                                    \n",
      "                                                            [ 1.09538167e-05  1.08923820e-05  1.11622510e-05 ...  1.12835654e-05  [ 0.00022252 -0.00047763 -0.00084073 ...  0.00179442 -0.00069755 \n",
      "                                                             -1.00567596e-04  1.04571371e-05]                                      -0.00102973]                                                    \n",
      "                                                            [ 1.11861436e-05  1.08980734e-05  1.10661807e-05 ...  1.08908266e-05  [ 0.03406147 -0.09837963 -0.04926663 ...  0.02352813  0.03367342 \n",
      "                                                              1.12847392e-05  1.12675017e-05]]                                      0.03814595]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.26484960988912065 [[-0.00985288 -0.01884848 -0.02981484 ... -0.01092967 -0.01483019\n",
      "                                        0.0041585 ]                                                   \n",
      "                                      [ 0.00618671 -0.00966535 -0.0402201  ...  0.06511063  0.00107167\n",
      "                                        0.03254221]                                                   \n",
      "                                      [-0.00121583  0.05310696 -0.03837484 ...  0.0140705  -0.05817934\n",
      "                                       -0.02869865]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.03918136  0.0148991  -0.02223341 ...  0.00514337 -0.0074462 \n",
      "                                       -0.00759368]                                                   \n",
      "                                      [ 0.01815047  0.03315394  0.03944026 ... -0.00556471 -0.00888186\n",
      "                                        0.02374356]                                                   \n",
      "                                      [ 0.04262954 -0.17084864 -0.10505904 ...  0.0271773   0.04124129\n",
      "                                        0.04052968]]                                                  \n",
      "Epoch 17, loss: 11.681127\n",
      "== W == -1.0175741978304382\n",
      "enter of the function =  [[-0.01877313 -0.02240963  0.00834581 ... -0.05656726  0.02071695 [9 3 7 ... 1 2 7]\n",
      "                            0.04477283]                                                                     \n",
      "                          [ 0.01764141 -0.03644168  0.02457551 ... -0.04864091  0.06557524                  \n",
      "                            0.06889409]                                                                     \n",
      "                          [ 0.01855639  0.00857744  0.04717155 ... -0.00580311  0.0672453                   \n",
      "                           -0.0175714 ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.02827603 -0.0143789  -0.01021469 ...  0.05879723 -0.06159484                  \n",
      "                           -0.05354404]                                                                     \n",
      "                          [-0.06522085  0.01066855 -0.07625238 ...  0.09513419 -0.1258386                   \n",
      "                           -0.12587975]                                                                     \n",
      "                          [ 0.00395644 -0.0279153   0.02469162 ... -0.00437394  0.03332777                  \n",
      "                            0.03344846]]                                                                    \n",
      "soft max = [[1.08947281e-05 1.08551814e-05 1.11942242e-05 ... 1.04906552e-05\n",
      "             1.13335697e-05 1.16095144e-05]                                 \n",
      "            [1.12987663e-05 1.07039246e-05 1.13773854e-05 ... 1.05741382e-05\n",
      "             1.18535497e-05 1.18929552e-05]                                 \n",
      "            [1.13091093e-05 1.11968174e-05 1.16373958e-05 ... 1.10369532e-05\n",
      "             1.18733624e-05 1.09078285e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.07916870e-05 1.09427073e-05 1.09883701e-05 ... 1.17734779e-05\n",
      "             1.04380449e-05 1.05224188e-05]                                 \n",
      "            [1.04002650e-05 1.12202557e-05 1.02861647e-05 ... 1.22091580e-05\n",
      "             9.78855201e-06 9.78814917e-06]                                 \n",
      "            [1.11451963e-05 1.07955805e-05 1.13787065e-05 ... 1.10527381e-05\n",
      "             1.14774003e-05 1.14787855e-05]]                                \n",
      "log =  102748.1691728729\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.416463241430323 [[ 1.08947281e-05  1.08551814e-05  1.11942242e-05 ...  1.04906552e-05 \n",
      "                                                   1.13335697e-05 -9.95015967e-05]                                    \n",
      "                                                 [ 1.12987663e-05  1.07039246e-05  1.13773854e-05 ...  1.05741382e-05 \n",
      "                                                   1.18535497e-05  1.18929552e-05]                                    \n",
      "                                                 [ 1.13091093e-05  1.11968174e-05  1.16373958e-05 ... -1.00074158e-04 \n",
      "                                                   1.18733624e-05  1.09078285e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.07916870e-05 -1.00168404e-04  1.09883701e-05 ...  1.17734779e-05 \n",
      "                                                   1.04380449e-05  1.05224188e-05]                                    \n",
      "                                                 [ 1.04002650e-05  1.12202557e-05 -1.00824946e-04 ...  1.22091580e-05 \n",
      "                                                   9.78855201e-06  9.78814917e-06]                                    \n",
      "                                                 [ 1.11451963e-05  1.07955805e-05  1.13787065e-05 ... -1.00058373e-04 \n",
      "                                                   1.14774003e-05  1.14787855e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.416463241430323 [[ 1.08947281e-05  1.08551814e-05  1.11942242e-05 ...  1.04906552e-05 [[-0.00060011 -0.00024552  0.00012491 ...  0.00095866 -0.00150331 \n",
      "                                                              1.13335697e-05 -9.95015967e-05]                                      -0.00033733]                                                    \n",
      "                                                            [ 1.12987663e-05  1.07039246e-05  1.13773854e-05 ...  1.05741382e-05  [-0.00115208  0.0004849   0.00064836 ...  0.00065867 -0.00178916 \n",
      "                                                              1.18535497e-05  1.18929552e-05]                                      -0.0003846 ]                                                    \n",
      "                                                            [ 1.13091093e-05  1.11968174e-05  1.16373958e-05 ... -1.00074158e-04  [-0.00147185  0.00106858  0.00122193 ...  0.00055236 -0.0018695  \n",
      "                                                              1.18733624e-05  1.09078285e-05]                                      -0.00098038]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.07916870e-05 -1.00168404e-04  1.09883701e-05 ...  1.17734779e-05  [-0.00041467  0.00068224 -0.00061945 ...  0.00159381 -0.00123786 \n",
      "                                                              1.04380449e-05  1.05224188e-05]                                      -0.00109119]                                                    \n",
      "                                                            [ 1.04002650e-05  1.12202557e-05 -1.00824946e-04 ...  1.22091580e-05  [ 0.00021557 -0.00047673 -0.0008471  ...  0.00181531 -0.00070969 \n",
      "                                                              9.78855201e-06  9.78814917e-06]                                      -0.00104403]                                                    \n",
      "                                                            [ 1.11451963e-05  1.07955805e-05  1.13787065e-05 ... -1.00058373e-04  [ 0.03406483 -0.09841088 -0.04928575 ...  0.02353596  0.03367909 \n",
      "                                                              1.14774003e-05  1.14787855e-05]]                                      0.03815306]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.2701948784298029 [[-0.00995733 -0.01903941 -0.03011168 ... -0.01102956 -0.01499339\n",
      "                                       0.00419684]                                                   \n",
      "                                     [ 0.00623714 -0.00975714 -0.04061577 ...  0.06576813  0.00106463\n",
      "                                       0.03286393]                                                   \n",
      "                                     [-0.00124263  0.05364873 -0.03874631 ...  0.01421653 -0.05877968\n",
      "                                      -0.02899529]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.03957725  0.0150549  -0.02246188 ...  0.00521054 -0.00753292\n",
      "                                      -0.0076804 ]                                                   \n",
      "                                     [ 0.0183342   0.0334807   0.03982625 ... -0.00560242 -0.00897766\n",
      "                                       0.0239707 ]                                                   \n",
      "                                     [ 0.04339645 -0.17354092 -0.1066023  ...  0.02768436  0.04199044\n",
      "                                       0.04131643]]                                                  \n",
      "Epoch 18, loss: 11.686658\n",
      "== W == -1.0348467498440992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[-3.22520571e-02 -2.06084775e-02 -1.15214783e-02 ...  2.78151822e-02 [5 7 5 ... 4 6 6]\n",
      "                           -2.12417878e-02 -3.73090265e-02]                                                     \n",
      "                          [-6.99719623e-02 -6.54367425e-02 -1.39679501e-01 ...  2.16549857e-01                  \n",
      "                           -1.12634541e-01 -1.60289281e-01]                                                     \n",
      "                          [-7.41613366e-02  4.38953312e-02 -1.88878185e-04 ...  5.01299607e-02                  \n",
      "                           -6.27053025e-02 -6.05218022e-03]                                                     \n",
      "                          ...                                                                                   \n",
      "                          [ 2.86661814e-02 -5.86342241e-03  1.74037412e-02 ... -9.09186671e-02                  \n",
      "                            4.62238474e-02  7.20511419e-02]                                                     \n",
      "                          [-2.60033470e-02  1.06396106e-03 -5.34399454e-03 ... -4.45629927e-03                  \n",
      "                           -3.93820923e-03  1.41174108e-02]                                                     \n",
      "                          [-1.78145988e-02 -5.25727586e-02 -4.31667694e-02 ...  5.46107166e-02                  \n",
      "                           -3.53581361e-02 -3.86182809e-02]]                                                    \n",
      "soft max = [[1.07481713e-05 1.08740499e-05 1.09733127e-05 ... 1.14135685e-05\n",
      "             1.08671655e-05 1.06939554e-05]                                 \n",
      "            [1.03503023e-05 1.03973498e-05 9.65338072e-06 ... 1.37844004e-05\n",
      "             9.91801845e-06 9.45646286e-06]                                 \n",
      "            [1.03070317e-05 1.15985840e-05 1.10983762e-05 ... 1.16711227e-05\n",
      "             1.04257884e-05 1.10334935e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.14232856e-05 1.10355763e-05 1.12953533e-05 ... 1.01357525e-05\n",
      "             1.16256229e-05 1.19297924e-05]                                 \n",
      "            [1.08155438e-05 1.11122894e-05 1.10413100e-05 ... 1.10511157e-05\n",
      "             1.10568426e-05 1.12582940e-05]                                 \n",
      "            [1.09044732e-05 1.05319651e-05 1.06314960e-05 ... 1.17235355e-05\n",
      "             1.07148385e-05 1.06799634e-05]]                                \n",
      "log =  102749.87645757553\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.416652939730614 [[ 1.07481713e-05  1.08740499e-05  1.09733127e-05 ...  1.14135685e-05 \n",
      "                                                   1.08671655e-05  1.06939554e-05]                                    \n",
      "                                                 [ 1.03503023e-05  1.03973498e-05  9.65338072e-06 ... -9.73267107e-05 \n",
      "                                                   9.91801845e-06  9.45646286e-06]                                    \n",
      "                                                 [ 1.03070317e-05  1.15985840e-05  1.10983762e-05 ...  1.16711227e-05 \n",
      "                                                   1.04257884e-05  1.10334935e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.14232856e-05  1.10355763e-05  1.12953533e-05 ...  1.01357525e-05 \n",
      "                                                   1.16256229e-05  1.19297924e-05]                                    \n",
      "                                                 [ 1.08155438e-05  1.11122894e-05  1.10413100e-05 ...  1.10511157e-05 \n",
      "                                                   1.10568426e-05  1.12582940e-05]                                    \n",
      "                                                 [ 1.09044732e-05  1.05319651e-05  1.06314960e-05 ...  1.17235355e-05 \n",
      "                                                   1.07148385e-05  1.06799634e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.416652939730614 [[ 1.07481713e-05  1.08740499e-05  1.09733127e-05 ...  1.14135685e-05 [[-0.00060861 -0.00024658  0.00011957 ...  0.00097685 -0.00151698 \n",
      "                                                              1.08671655e-05  1.06939554e-05]                                      -0.00035051]                                                    \n",
      "                                                            [ 1.03503023e-05  1.03973498e-05  9.65338072e-06 ... -9.73267107e-05  [-0.00116067  0.00048398  0.00064306 ...  0.00067765 -0.00180347 \n",
      "                                                              9.91801845e-06  9.45646286e-06]                                      -0.0003985 ]                                                    \n",
      "                                                            [ 1.03070317e-05  1.15985840e-05  1.10983762e-05 ...  1.16711227e-05  [-0.00148023  0.00106749  0.00121652 ...  0.00057253 -0.00188396 \n",
      "                                                              1.04257884e-05  1.10334935e-05]                                      -0.00099519]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.14232856e-05  1.10355763e-05  1.12953533e-05 ...  1.01357525e-05  [-0.00042205  0.00068357 -0.00062601 ...  0.00161412 -0.0012502  \n",
      "                                                              1.16256229e-05  1.19297924e-05]                                      -0.00110494]                                                    \n",
      "                                                            [ 1.08155438e-05  1.11122894e-05  1.10413100e-05 ...  1.10511157e-05  [ 0.00020848 -0.00047583 -0.00085358 ...  0.00183658 -0.00072203 \n",
      "                                                              1.10568426e-05  1.12582940e-05]                                      -0.00105855]                                                    \n",
      "                                                            [ 1.09044732e-05  1.05319651e-05  1.06314960e-05 ...  1.17235355e-05  [ 0.03406807 -0.09844261 -0.0493052  ...  0.02354393  0.03368474 \n",
      "                                                              1.07148385e-05  1.06799634e-05]]                                      0.03816019]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.27564811358030744 [[-0.0100629  -0.01923226 -0.03041155 ... -0.01113027 -0.01515836\n",
      "                                        0.00423544]                                                   \n",
      "                                      [ 0.00628799 -0.00984987 -0.04101544 ...  0.0664324   0.00105739\n",
      "                                        0.03318872]                                                   \n",
      "                                      [-0.00126977  0.0541959  -0.03912156 ...  0.01436422 -0.05938618\n",
      "                                       -0.02929505]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.03997717  0.01521227 -0.02269269 ...  0.00527859 -0.00762063\n",
      "                                       -0.00776811]                                                   \n",
      "                                      [ 0.0185197   0.03381074  0.04021604 ... -0.00564029 -0.00907453\n",
      "                                        0.02419996]                                                   \n",
      "                                      [ 0.04417106 -0.17626044 -0.10816118 ...  0.02819656  0.04274714\n",
      "                                        0.04211113]]                                                  \n",
      "Epoch 19, loss: 11.692301\n",
      "== W == -1.0523982828579226\n",
      "enter of the function =  [[-0.00789442 -0.02789281 -0.03256301 ... -0.00242094 -0.0177934  [4 5 4 ... 1 1 7]\n",
      "                           -0.0143784 ]                                                                     \n",
      "                          [-0.0038486   0.00200891 -0.026312   ...  0.02555584 -0.02605763                  \n",
      "                           -0.00954399]                                                                     \n",
      "                          [-0.04574175 -0.0002704  -0.05390222 ...  0.11010003 -0.05592954                  \n",
      "                           -0.04728976]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.01024261 -0.04021353 -0.01860029 ...  0.00069624  0.0332317                   \n",
      "                            0.00731086]                                                                     \n",
      "                          [ 0.04129946 -0.03867779  0.01780571 ... -0.06741346  0.05160548                  \n",
      "                            0.0672661 ]                                                                     \n",
      "                          [-0.01298127 -0.0409001  -0.13420388 ...  0.073113   -0.02665139                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.05626773]]                                                                    \n",
      "soft max = [[1.10124473e-05 1.07944035e-05 1.07441090e-05 ... 1.10728889e-05\n",
      "             1.09039730e-05 1.09412737e-05]                                 \n",
      "            [1.10570918e-05 1.11220489e-05 1.08114810e-05 ... 1.13870467e-05\n",
      "             1.08142314e-05 1.09942965e-05]                                 \n",
      "            [1.06034441e-05 1.10967272e-05 1.05172672e-05 ... 1.23916226e-05\n",
      "             1.04959669e-05 1.05870426e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.09866182e-05 1.06622246e-05 1.08951783e-05 ... 1.11074589e-05\n",
      "             1.14747885e-05 1.11811741e-05]                                 \n",
      "            [1.15677387e-05 1.06786116e-05 1.12991367e-05 ... 1.03761215e-05\n",
      "             1.16875725e-05 1.18720478e-05]                                 \n",
      "            [1.09565708e-05 1.06549068e-05 9.70573296e-06 ... 1.19416658e-05\n",
      "             1.08078123e-05 1.04924179e-05]]                                \n",
      "log =  102751.62030590195\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.416846700655771 [[ 1.10124473e-05  1.07944035e-05  1.07441090e-05 ...  1.10728889e-05 \n",
      "                                                   1.09039730e-05  1.09412737e-05]                                    \n",
      "                                                 [ 1.10570918e-05  1.11220489e-05  1.08114810e-05 ...  1.13870467e-05 \n",
      "                                                   1.08142314e-05  1.09942965e-05]                                    \n",
      "                                                 [ 1.06034441e-05  1.10967272e-05  1.05172672e-05 ...  1.23916226e-05 \n",
      "                                                   1.04959669e-05  1.05870426e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.09866182e-05 -1.00448887e-04  1.08951783e-05 ...  1.11074589e-05 \n",
      "                                                   1.14747885e-05  1.11811741e-05]                                    \n",
      "                                                 [ 1.15677387e-05 -1.00432499e-04  1.12991367e-05 ...  1.03761215e-05 \n",
      "                                                   1.16875725e-05  1.18720478e-05]                                    \n",
      "                                                 [ 1.09565708e-05  1.06549068e-05  9.70573296e-06 ... -9.91694453e-05 \n",
      "                                                   1.08078123e-05  1.04924179e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.416846700655771 [[ 1.10124473e-05  1.07944035e-05  1.07441090e-05 ...  1.10728889e-05 [[-0.00061724 -0.00024766  0.00011413 ...  0.00099538 -0.00153088 \n",
      "                                                              1.09039730e-05  1.09412737e-05]                                      -0.00036391]                                                    \n",
      "                                                            [ 1.10570918e-05  1.11220489e-05  1.08114810e-05 ...  1.13870467e-05  [-0.00116939  0.00048305  0.00063767 ...  0.00069698 -0.00181801 \n",
      "                                                              1.08142314e-05  1.09942965e-05]                                      -0.00041263]                                                    \n",
      "                                                            [ 1.06034441e-05  1.10967272e-05  1.05172672e-05 ...  1.23916226e-05  [-0.00148875  0.00106638  0.001211   ...  0.00059307 -0.00189865 \n",
      "                                                              1.04959669e-05  1.05870426e-05]                                      -0.00101025]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.09866182e-05 -1.00448887e-04  1.08951783e-05 ...  1.11074589e-05  [-0.00042955  0.00068491 -0.00063268 ...  0.0016348  -0.00126275 \n",
      "                                                              1.14747885e-05  1.11811741e-05]                                      -0.00111892]                                                    \n",
      "                                                            [ 1.15677387e-05 -1.00432499e-04  1.12991367e-05 ...  1.03761215e-05  [ 0.00020128 -0.00047493 -0.00086017 ...  0.00185823 -0.00073459 \n",
      "                                                              1.16875725e-05  1.18720478e-05]                                      -0.00107332]                                                    \n",
      "                                                            [ 1.09565708e-05  1.06549068e-05  9.70573296e-06 ... -9.91694453e-05  [ 0.03407119 -0.09847483 -0.049325   ...  0.02355203  0.03369035 \n",
      "                                                              1.08078123e-05  1.04924179e-05]]                                      0.03816733]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.28121149893693914 [[-0.01016962 -0.01942705 -0.03071447 ... -0.0112318  -0.01532511\n",
      "                                        0.00427429]                                                   \n",
      "                                      [ 0.00633926 -0.00994353 -0.04141917 ...  0.0671035   0.00104993\n",
      "                                        0.03351662]                                                   \n",
      "                                      [-0.00129727  0.05474854 -0.03950061 ...  0.01451359 -0.05999888\n",
      "                                       -0.02959795]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.04038116  0.01537123 -0.02292588 ...  0.00534751 -0.00770933\n",
      "                                       -0.00785684]                                                   \n",
      "                                      [ 0.01870698  0.03414409  0.04060967 ... -0.00567832 -0.00917249\n",
      "                                        0.02443138]                                                   \n",
      "                                      [ 0.04495345 -0.17900747 -0.10973584 ...  0.02871396  0.04351145\n",
      "                                        0.04291384]]                                                  \n",
      "Epoch 20, loss: 11.698058\n",
      "== W == -1.0702328181835987\n",
      "enter of the function =  [[ 0.06400979 -0.00693333  0.05235209 ... -0.07794524  0.0761764  [9 6 9 ... 6 8 5]\n",
      "                            0.07248683]                                                                     \n",
      "                          [-0.01469443  0.02198232  0.02722091 ... -0.09914109  0.06221188                  \n",
      "                            0.09169765]                                                                     \n",
      "                          [ 0.02256713 -0.00484677  0.00437311 ... -0.04953545  0.01721035                  \n",
      "                            0.03738803]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.0176857  -0.00651004 -0.09827365 ...  0.07642022  0.04758709                  \n",
      "                           -0.02034264]                                                                     \n",
      "                          [-0.01084872 -0.03586244 -0.01188197 ... -0.02873629  0.02037033                  \n",
      "                            0.03722317]                                                                     \n",
      "                          [ 0.04275413 -0.04342606  0.00928427 ... -0.00330708  0.03087359                  \n",
      "                            0.01520823]]                                                                    \n",
      "soft max = [[1.18326258e-05 1.10222670e-05 1.16954856e-05 ... 1.02666994e-05\n",
      "             1.19774681e-05 1.19333579e-05]                                 \n",
      "            [1.09370532e-05 1.13456357e-05 1.14052268e-05 ... 1.00513780e-05\n",
      "             1.18113710e-05 1.21648237e-05]                                 \n",
      "            [1.13522728e-05 1.10452897e-05 1.11475968e-05 ... 1.05625569e-05\n",
      "             1.12916237e-05 1.15217766e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.12969925e-05 1.10269337e-05 1.00601007e-05 ... 1.19803888e-05\n",
      "             1.16398892e-05 1.08754526e-05]                                 \n",
      "            [1.09791949e-05 1.07079707e-05 1.09678565e-05 ... 1.07845499e-05\n",
      "             1.13273614e-05 1.15198773e-05]                                 \n",
      "            [1.15837698e-05 1.06272852e-05 1.12024790e-05 ... 1.10623091e-05\n",
      "             1.14469626e-05 1.12690390e-05]]                                \n",
      "log =  102753.40174293725\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.417044638104139 [[ 1.18326258e-05  1.10222670e-05  1.16954856e-05 ...  1.02666994e-05 \n",
      "                                                   1.19774681e-05 -9.91777533e-05]                                    \n",
      "                                                 [ 1.09370532e-05  1.13456357e-05  1.14052268e-05 ...  1.00513780e-05 \n",
      "                                                   1.18113710e-05  1.21648237e-05]                                    \n",
      "                                                 [ 1.13522728e-05  1.10452897e-05  1.11475968e-05 ...  1.05625569e-05 \n",
      "                                                   1.12916237e-05 -9.95893345e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.12969925e-05  1.10269337e-05  1.00601007e-05 ...  1.19803888e-05 \n",
      "                                                   1.16398892e-05  1.08754526e-05]                                    \n",
      "                                                 [ 1.09791949e-05  1.07079707e-05  1.09678565e-05 ...  1.07845499e-05 \n",
      "                                                  -9.97837497e-05  1.15198773e-05]                                    \n",
      "                                                 [ 1.15837698e-05  1.06272852e-05  1.12024790e-05 ...  1.10623091e-05 \n",
      "                                                   1.14469626e-05  1.12690390e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.417044638104139 [[ 1.18326258e-05  1.10222670e-05  1.16954856e-05 ...  1.02666994e-05 [[-0.00062602 -0.00024875  0.0001086  ...  0.00101425 -0.00154499 \n",
      "                                                              1.19774681e-05 -9.91777533e-05]                                      -0.00037754]                                                    \n",
      "                                                            [ 1.09370532e-05  1.13456357e-05  1.14052268e-05 ...  1.00513780e-05  [-0.00117826  0.00048211  0.00063218 ...  0.00071668 -0.00183279 \n",
      "                                                              1.18113710e-05  1.21648237e-05]                                      -0.00042699]                                                    \n",
      "                                                            [ 1.13522728e-05  1.10452897e-05  1.11475968e-05 ...  1.05625569e-05  [-0.00149742  0.00106525  0.00120539 ...  0.00061399 -0.00191357 \n",
      "                                                              1.12916237e-05 -9.95893345e-05]                                      -0.00102555]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.12969925e-05  1.10269337e-05  1.00601007e-05 ...  1.19803888e-05  [-0.00043719  0.00068626 -0.00063945 ...  0.00165587 -0.00127552 \n",
      "                                                              1.16398892e-05  1.08754526e-05]                                      -0.00113314]                                                    \n",
      "                                                            [ 1.09791949e-05  1.07079707e-05  1.09678565e-05 ...  1.07845499e-05  [ 0.00019394 -0.00047402 -0.00086686 ...  0.00188028 -0.00074736 \n",
      "                                                             -9.97837497e-05  1.15198773e-05]                                      -0.00108834]                                                    \n",
      "                                                            [ 1.15837698e-05  1.06272852e-05  1.12024790e-05 ...  1.10623091e-05  [ 0.03407418 -0.09850755 -0.04934516 ...  0.02356028  0.03369594 \n",
      "                                                              1.14469626e-05  1.12690390e-05]]                                      0.03817448]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.2868872624071856 [[-0.01027749 -0.01962379 -0.03102047 ... -0.01133417 -0.01549367\n",
      "                                       0.00431339]                                                   \n",
      "                                     [ 0.00639096 -0.01003813 -0.04182698 ...  0.06778151  0.00104225\n",
      "                                       0.03384766]                                                   \n",
      "                                     [-0.00132513  0.05530669 -0.0398835  ...  0.01466466 -0.06061785\n",
      "                                      -0.02990403]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.04078927  0.01553179 -0.02316146 ...  0.00541734 -0.00779906\n",
      "                                      -0.0079466 ]                                                   \n",
      "                                     [ 0.01889606  0.03448078  0.04100716 ... -0.00571652 -0.00927157\n",
      "                                       0.02466496]                                                   \n",
      "                                     [ 0.0457437  -0.18178229 -0.11132645 ...  0.02923662  0.04428347\n",
      "                                       0.04372465]]                                                  \n",
      "Epoch 21, loss: 11.703932\n",
      "== W == -1.08835441531468\n",
      "enter of the function =  [[ 0.02766229 -0.04316378 -0.01835564 ... -0.01126427  0.03983458 [2 3 3 ... 1 5 9]\n",
      "                            0.01175229]                                                                     \n",
      "                          [-0.01192907 -0.01110192 -0.03077026 ...  0.07341578 -0.06317466                  \n",
      "                           -0.03823598]                                                                     \n",
      "                          [ 0.04371428 -0.02289406 -0.0014544  ... -0.08436144  0.0675952                   \n",
      "                            0.10566954]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.00894579 -0.01785315 -0.00847814 ... -0.01079795 -0.02860333                  \n",
      "                            0.00277859]                                                                     \n",
      "                          [-0.003408   -0.02583681  0.02855292 ... -0.07338414  0.01059813                  \n",
      "                            0.04350095]                                                                     \n",
      "                          [ 0.02251563  0.02324081  0.01369174 ... -0.07267552  0.03764178                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.04624392]]                                                                    \n",
      "soft max = [[1.14094335e-05 1.06293012e-05 1.08962925e-05 ... 1.09738367e-05\n",
      "             1.15491611e-05 1.12293459e-05]                                 \n",
      "            [1.09665438e-05 1.09756185e-05 1.07618553e-05 ... 1.19435813e-05\n",
      "             1.04187135e-05 1.06818095e-05]                                 \n",
      "            [1.15940554e-05 1.08469526e-05 1.10820183e-05 ... 1.02002965e-05\n",
      "             1.18742647e-05 1.23350865e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.09993088e-05 1.09017691e-05 1.10044539e-05 ... 1.09789552e-05\n",
      "             1.07852008e-05 1.11290278e-05]                                 \n",
      "            [1.10603897e-05 1.08150796e-05 1.14195996e-05 ... 1.03128850e-05\n",
      "             1.12163928e-05 1.15915824e-05]                                 \n",
      "            [1.13508639e-05 1.13590983e-05 1.12511458e-05 ... 1.03201956e-05\n",
      "             1.15238639e-05 1.16234214e-05]]                                \n",
      "log =  102755.22182741576\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.417246869712862 [[ 1.14094335e-05  1.06293012e-05 -1.00214819e-04 ...  1.09738367e-05 \n",
      "                                                   1.15491611e-05  1.12293459e-05]                                    \n",
      "                                                 [ 1.09665438e-05  1.09756185e-05  1.07618553e-05 ...  1.19435813e-05 \n",
      "                                                   1.04187135e-05  1.06818095e-05]                                    \n",
      "                                                 [ 1.15940554e-05  1.08469526e-05  1.10820183e-05 ...  1.02002965e-05 \n",
      "                                                   1.18742647e-05  1.23350865e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.09993088e-05 -1.00209342e-04  1.10044539e-05 ...  1.09789552e-05 \n",
      "                                                   1.07852008e-05  1.11290278e-05]                                    \n",
      "                                                 [ 1.10603897e-05  1.08150796e-05  1.14195996e-05 ...  1.03128850e-05 \n",
      "                                                   1.12163928e-05  1.15915824e-05]                                    \n",
      "                                                 [ 1.13508639e-05  1.13590983e-05  1.12511458e-05 ...  1.03201956e-05 \n",
      "                                                   1.15238639e-05 -9.94876897e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.417246869712862 [[ 1.14094335e-05  1.06293012e-05 -1.00214819e-04 ...  1.09738367e-05 [[-0.00063493 -0.00024986  0.00010298 ...  0.00103348 -0.00155933 \n",
      "                                                              1.15491611e-05  1.12293459e-05]                                      -0.0003914 ]                                                    \n",
      "                                                            [ 1.09665438e-05  1.09756185e-05  1.07618553e-05 ...  1.19435813e-05  [-0.00118727  0.00048115  0.00062659 ...  0.00073674 -0.00184779 \n",
      "                                                              1.04187135e-05  1.06818095e-05]                                      -0.0004416 ]                                                    \n",
      "                                                            [ 1.15940554e-05  1.08469526e-05  1.10820183e-05 ...  1.02002965e-05  [-0.00150622  0.00106411  0.00119968 ...  0.0006353  -0.00192873 \n",
      "                                                              1.18742647e-05  1.23350865e-05]                                      -0.00104111]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.09993088e-05 -1.00209342e-04  1.10044539e-05 ...  1.09789552e-05  [-0.00044496  0.00068761 -0.00064634 ...  0.00167732 -0.00128849 \n",
      "                                                              1.07852008e-05  1.11290278e-05]                                      -0.00114759]                                                    \n",
      "                                                            [ 1.10603897e-05  1.08150796e-05  1.14195996e-05 ...  1.03128850e-05  [ 0.00018648 -0.00047312 -0.00087367 ...  0.00190273 -0.00076034 \n",
      "                                                              1.12163928e-05  1.15915824e-05]                                      -0.0011036 ]                                                    \n",
      "                                                            [ 1.13508639e-05  1.13590983e-05  1.12511458e-05 ...  1.03201956e-05  [ 0.03407704 -0.09854078 -0.04936567 ...  0.02356866  0.03370148 \n",
      "                                                              1.15238639e-05 -9.94876897e-05]]                                      0.03818163]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.2926776771150277 [[-0.01038652 -0.01982252 -0.03132959 ... -0.01143736 -0.01566406\n",
      "                                       0.00435275]                                                   \n",
      "                                     [ 0.00644309 -0.01013369 -0.04223893 ...  0.06846649  0.00103434\n",
      "                                       0.03418187]                                                   \n",
      "                                     [-0.00135336  0.05587041 -0.04027028 ...  0.01481744 -0.06124317\n",
      "                                      -0.03021333]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.04120153  0.01569397 -0.02339947 ...  0.00548807 -0.0078898 \n",
      "                                      -0.0080374 ]                                                   \n",
      "                                     [ 0.01908696  0.03482085  0.04140857 ... -0.00575489 -0.00937176\n",
      "                                       0.02490072]                                                   \n",
      "                                     [ 0.04654187 -0.18458519 -0.11293317 ...  0.02976459  0.04506327\n",
      "                                       0.04454364]]                                                  \n",
      "Epoch 22, loss: 11.709925\n",
      "== W == -1.106767171306287\n",
      "enter of the function =  [[ 1.11084732e-02  1.01975631e-02 -1.79162689e-05 ... -5.59586440e-02 [8 5 9 ... 8 4 2]\n",
      "                            2.67315616e-02  3.39395788e-02]                                                     \n",
      "                          [ 3.30924694e-02 -9.07045511e-03  7.46794694e-02 ... -7.03765512e-02                  \n",
      "                            4.50036380e-02  6.41114261e-02]                                                     \n",
      "                          [-2.99541784e-02 -7.87043896e-03 -1.19557803e-02 ...  1.27276830e-01                  \n",
      "                           -7.33095220e-02 -7.03348033e-02]                                                     \n",
      "                          ...                                                                                   \n",
      "                          [ 1.34649607e-02  2.81976385e-02  2.41050931e-02 ...  2.82998233e-02                  \n",
      "                           -7.24245947e-04  1.17384974e-03]                                                     \n",
      "                          [ 2.90098110e-02 -6.83067976e-02  3.27657471e-02 ... -3.89480415e-02                  \n",
      "                            2.04832849e-02  5.29881871e-02]                                                     \n",
      "                          [-1.05777809e-02  2.28309751e-02 -2.22209520e-04 ...  1.62361745e-02                  \n",
      "                           -4.12010284e-02 -4.84795990e-03]]                                                    \n",
      "soft max = [[1.12212710e-05 1.12110541e-05 1.10971108e-05 ... 1.04933745e-05\n",
      "             1.13979585e-05 1.14804120e-05]                                 \n",
      "            [1.14706910e-05 1.09971071e-05 1.19577807e-05 ... 1.03431674e-05\n",
      "             1.16081373e-05 1.18320758e-05]                                 \n",
      "            [1.07698280e-05 1.10103117e-05 1.09654226e-05 ... 1.26035628e-05\n",
      "             1.03128757e-05 1.03435993e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.12477450e-05 1.14146811e-05 1.13680614e-05 ... 1.14158475e-05\n",
      "             1.10892754e-05 1.11103438e-05]                                 \n",
      "            [1.14239555e-05 1.03645974e-05 1.14669439e-05 ... 1.06734000e-05\n",
      "             1.13269630e-05 1.17011940e-05]                                 \n",
      "            [1.09805434e-05 1.13535864e-05 1.10948440e-05 ... 1.12789581e-05\n",
      "             1.06493800e-05 1.10436405e-05]]                                \n",
      "log =  102757.08165293642\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.417453516992936 [[ 1.12212710e-05  1.12110541e-05  1.10971108e-05 ...  1.04933745e-05 \n",
      "                                                  -9.97131526e-05  1.14804120e-05]                                    \n",
      "                                                 [ 1.14706910e-05  1.09971071e-05  1.19577807e-05 ...  1.03431674e-05 \n",
      "                                                   1.16081373e-05  1.18320758e-05]                                    \n",
      "                                                 [ 1.07698280e-05  1.10103117e-05  1.09654226e-05 ...  1.26035628e-05 \n",
      "                                                   1.03128757e-05 -1.00767512e-04]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.12477450e-05  1.14146811e-05  1.13680614e-05 ...  1.14158475e-05 \n",
      "                                                  -1.00021836e-04  1.11103438e-05]                                    \n",
      "                                                 [ 1.14239555e-05  1.03645974e-05  1.14669439e-05 ...  1.06734000e-05 \n",
      "                                                   1.13269630e-05  1.17011940e-05]                                    \n",
      "                                                 [ 1.09805434e-05  1.13535864e-05 -1.00016267e-04 ...  1.12789581e-05 \n",
      "                                                   1.06493800e-05  1.10436405e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.417453516992936 [[ 1.12212710e-05  1.12110541e-05  1.10971108e-05 ...  1.04933745e-05 [[-6.43984494e-04 -2.50985002e-04  9.72536372e-05 ...  1.05306911e-03 \n",
      "                                                             -9.97131526e-05  1.14804120e-05]                                      -1.57390441e-03 -4.05492218e-04]                                    \n",
      "                                                            [ 1.14706910e-05  1.09971071e-05  1.19577807e-05 ...  1.03431674e-05  [-1.19642525e-03  4.80170514e-04  6.20908742e-04 ...  7.57180930e-04 \n",
      "                                                              1.16081373e-05  1.18320758e-05]                                      -1.86303565e-03 -4.56450457e-04]                                    \n",
      "                                                            [ 1.07698280e-05  1.10103117e-05  1.09654226e-05 ...  1.26035628e-05  [-1.51517529e-03  1.06294171e-03  1.19386446e-03 ...  6.56998529e-04 \n",
      "                                                              1.03128757e-05 -1.00767512e-04]                                      -1.94413301e-03 -1.05691948e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.12477450e-05  1.14146811e-05  1.13680614e-05 ...  1.14158475e-05  [-4.52863769e-04  6.88964460e-04 -6.53337548e-04 ...  1.69916513e-03 \n",
      "                                                             -1.00021836e-04  1.11103438e-05]                                      -1.30169027e-03 -1.16228698e-03]                                    \n",
      "                                                            [ 1.14239555e-05  1.03645974e-05  1.14669439e-05 ...  1.06734000e-05  [ 1.78882716e-04 -4.72213622e-04 -8.80581703e-04 ...  1.92559759e-03 \n",
      "                                                              1.13269630e-05  1.17011940e-05]                                      -7.73545370e-04 -1.11911888e-03]                                    \n",
      "                                                            [ 1.09805434e-05  1.13535864e-05 -1.00016267e-04 ...  1.12789581e-05  [ 3.40797620e-02 -9.85745330e-02 -4.93865614e-02 ...  2.35771982e-02 \n",
      "                                                              1.06493800e-05  1.10436405e-05]]                                      3.37069849e-02  3.81887772e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.29858506232500065 [[-0.01049674 -0.02002324 -0.03164186 ... -0.0115414  -0.01583629\n",
      "                                        0.00439237]                                                   \n",
      "                                      [ 0.00649564 -0.01023022 -0.04265505 ...  0.06915852  0.00102621\n",
      "                                        0.03451927]                                                   \n",
      "                                      [-0.00138195  0.05643975 -0.04066099 ...  0.01497197 -0.06187489\n",
      "                                       -0.03052587]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.041618    0.01585779 -0.02363993 ...  0.00555972 -0.00798158\n",
      "                                       -0.00812925]                                                   \n",
      "                                      [ 0.0192797   0.03516432  0.04181391 ... -0.00579341 -0.00947308\n",
      "                                        0.0251387 ]                                                   \n",
      "                                      [ 0.04734806 -0.18741645 -0.11455615 ...  0.03029793  0.04585091\n",
      "                                        0.0453709 ]]                                                  \n",
      "Epoch 23, loss: 11.716039\n",
      "== W == -1.1254752200873939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[ 0.05152959  0.03920227  0.02200668 ... -0.11483039  0.06616109 [8 6 1 ... 0 2 2]\n",
      "                            0.10254498]                                                                     \n",
      "                          [ 0.06600929  0.01870529 -0.03011448 ... -0.05339793  0.0663075                   \n",
      "                            0.06496591]                                                                     \n",
      "                          [ 0.0111163  -0.00700682  0.00888881 ... -0.01794552  0.03304996                  \n",
      "                            0.04421203]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.00997271 -0.03839979  0.01028286 ... -0.02300865  0.06183407                  \n",
      "                            0.0424602 ]                                                                     \n",
      "                          [ 0.0393381  -0.03429489  0.02124165 ... -0.0405137   0.04448211                  \n",
      "                            0.06023401]                                                                     \n",
      "                          [ 0.01690343  0.00153534 -0.00180317 ... -0.12493541  0.05798879                  \n",
      "                            0.09735107]]                                                                    \n",
      "soft max = [[1.16832214e-05 1.15400826e-05 1.13433405e-05 ... 9.89266683e-06\n",
      "             1.18554212e-05 1.22947105e-05]                                 \n",
      "            [1.18536216e-05 1.13059534e-05 1.07672559e-05 ... 1.05194531e-05\n",
      "             1.18571570e-05 1.18412602e-05]                                 \n",
      "            [1.12204774e-05 1.10189589e-05 1.11955117e-05 ... 1.08990827e-05\n",
      "             1.14693025e-05 1.15980408e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.09863264e-05 1.06784144e-05 1.12111297e-05 ... 1.08440387e-05\n",
      "             1.18042333e-05 1.15777407e-05]                                 \n",
      "            [1.15416502e-05 1.07223383e-05 1.13346658e-05 ... 1.06558651e-05\n",
      "             1.16011735e-05 1.17853610e-05]                                 \n",
      "            [1.12856001e-05 1.11134878e-05 1.10764472e-05 ... 9.79320466e-06\n",
      "             1.17589299e-05 1.22310184e-05]]                                \n",
      "log =  102758.98234922605\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.41766470546956 [[ 1.16832214e-05  1.15400826e-05  1.13433405e-05 ...  9.89266683e-06 \n",
      "                                                 -9.92556899e-05  1.22947105e-05]                                    \n",
      "                                                [ 1.18536216e-05  1.13059534e-05  1.07672559e-05 ...  1.05194531e-05 \n",
      "                                                  1.18571570e-05  1.18412602e-05]                                    \n",
      "                                                [ 1.12204774e-05 -1.00092152e-04  1.11955117e-05 ...  1.08990827e-05 \n",
      "                                                  1.14693025e-05  1.15980408e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [-1.00124785e-04  1.06784144e-05  1.12111297e-05 ...  1.08440387e-05 \n",
      "                                                  1.18042333e-05  1.15777407e-05]                                    \n",
      "                                                [ 1.15416502e-05  1.07223383e-05 -9.97764453e-05 ...  1.06558651e-05 \n",
      "                                                  1.16011735e-05  1.17853610e-05]                                    \n",
      "                                                [ 1.12856001e-05  1.11134878e-05 -1.00034664e-04 ...  9.79320466e-06 \n",
      "                                                  1.17589299e-05  1.22310184e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.41766470546956 [[ 1.16832214e-05  1.15400826e-05  1.13433405e-05 ...  9.89266683e-06 [[-6.53186011e-04 -2.52127221e-04  9.14301519e-05 ...  1.07302850e-03 \n",
      "                                                            -9.92556899e-05  1.22947105e-05]                                      -1.58870737e-03 -4.19819674e-04]                                    \n",
      "                                                           [ 1.18536216e-05  1.13059534e-05  1.07672559e-05 ...  1.05194531e-05  [-1.20572900e-03  4.79177818e-04  6.15123445e-04 ...  7.78005531e-04 \n",
      "                                                             1.18571570e-05  1.18412602e-05]                                      -1.87852140e-03 -4.71547055e-04]                                    \n",
      "                                                           [ 1.12204774e-05 -1.00092152e-04  1.11955117e-05 ...  1.08990827e-05  [-1.52427435e-03  1.06176008e-03  1.18794758e-03 ...  6.79105103e-04 \n",
      "                                                             1.14693025e-05  1.15980408e-05]                                      -1.95978262e-03 -1.07299259e-03]                                    \n",
      "                                                           ...                                                                   ...                                                                  \n",
      "                                                           [-1.00124785e-04  1.06784144e-05  1.12111297e-05 ...  1.08440387e-05  [-4.60902857e-04  6.90327052e-04 -6.60448437e-04 ...  1.72141633e-03 \n",
      "                                                             1.18042333e-05  1.15777407e-05]                                      -1.31511000e-03 -1.17722994e-03]                                    \n",
      "                                                           [ 1.15416502e-05  1.07223383e-05 -9.97764453e-05 ...  1.06558651e-05  [ 1.71153023e-04 -4.71306224e-04 -8.87610780e-04 ...  1.94887865e-03 \n",
      "                                                             1.16011735e-05  1.17853610e-05]                                      -7.86974084e-04 -1.13489128e-03]                                    \n",
      "                                                           [ 1.12856001e-05  1.11134878e-05 -1.00034664e-04 ...  9.79320466e-06  [ 3.40823341e-02 -9.86088223e-02 -4.94078322e-02 ...  2.35858854e-02 \n",
      "                                                             1.17589299e-05  1.22310184e-05]]                                      3.37124444e-02  3.81959289e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.3046117843854053 [[-0.01060814 -0.02022599 -0.0319573  ... -0.01164629 -0.01601039\n",
      "                                       0.00443223]                                                   \n",
      "                                     [ 0.00654864 -0.01032772 -0.0430754  ...  0.06985768  0.00101784\n",
      "                                       0.0348599 ]                                                   \n",
      "                                     [-0.00141092  0.05701478 -0.04105566 ...  0.01512826 -0.06251308\n",
      "                                      -0.0308417 ]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.0420387   0.01602326 -0.02388286 ...  0.00563231 -0.00807442\n",
      "                                      -0.00822217]                                                   \n",
      "                                     [ 0.01947428  0.03551124  0.04222325 ... -0.00583209 -0.00957554\n",
      "                                       0.02537889]                                                   \n",
      "                                     [ 0.04816234 -0.19027636 -0.11619558 ...  0.03083668  0.04664649\n",
      "                                       0.04620649]]                                                  \n",
      "Epoch 24, loss: 11.722276\n",
      "== W == -1.1444827317017514\n",
      "enter of the function =  [[ 0.038251   -0.08082498 -0.01775256 ...  0.04335416  0.01246699 [1 7 0 ... 2 2 8]\n",
      "                            0.00189211]                                                                     \n",
      "                          [-0.00466085 -0.10239169 -0.03197567 ...  0.14370543  0.01422919                  \n",
      "                           -0.10157216]                                                                     \n",
      "                          [-0.02303542 -0.04224868  0.03988283 ... -0.03152797  0.00354152                  \n",
      "                            0.02944516]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.05231451 -0.02945786  0.00489258 ... -0.05272539  0.07687683                  \n",
      "                            0.05634594]                                                                     \n",
      "                          [ 0.0138968  -0.01661566 -0.08372399 ...  0.05003372  0.02968135                  \n",
      "                           -0.06755589]                                                                     \n",
      "                          [-0.02250357 -0.03601431 -0.06477977 ...  0.13270897 -0.06997481                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.02977064]]                                                                    \n",
      "soft max = [[1.15281684e-05 1.02340202e-05 1.09002955e-05 ... 1.15871488e-05\n",
      "             1.12347252e-05 1.11165453e-05]                                 \n",
      "            [1.10439372e-05 1.00156690e-05 1.07463568e-05 ... 1.28102789e-05\n",
      "             1.12545406e-05 1.00238805e-05]                                 \n",
      "            [1.08428627e-05 1.06365245e-05 1.15469957e-05 ... 1.07511690e-05\n",
      "             1.11348962e-05 1.14270987e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.16914403e-05 1.07734481e-05 1.11499503e-05 ... 1.05256704e-05\n",
      "             1.19821650e-05 1.17386686e-05]                                 \n",
      "            [1.12508002e-05 1.09126951e-05 1.02043946e-05 ... 1.16648049e-05\n",
      "             1.14297980e-05 1.03707213e-05]                                 \n",
      "            [1.08486310e-05 1.07030436e-05 1.03995516e-05 ... 1.26701829e-05\n",
      "             1.03456656e-05 1.07700789e-05]]                                \n",
      "log =  102760.92508345286\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.417880564828096 [[ 1.15281684e-05 -1.00877091e-04  1.09002955e-05 ...  1.15871488e-05 \n",
      "                                                   1.12347252e-05  1.11165453e-05]                                    \n",
      "                                                 [ 1.10439372e-05  1.00156690e-05  1.07463568e-05 ... -9.83008322e-05 \n",
      "                                                   1.12545406e-05  1.00238805e-05]                                    \n",
      "                                                 [-1.00268248e-04  1.06365245e-05  1.15469957e-05 ...  1.07511690e-05 \n",
      "                                                   1.11348962e-05  1.14270987e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.16914403e-05  1.07734481e-05 -9.99611608e-05 ...  1.05256704e-05 \n",
      "                                                   1.19821650e-05  1.17386686e-05]                                    \n",
      "                                                 [ 1.12508002e-05  1.09126951e-05 -1.00906716e-04 ...  1.16648049e-05 \n",
      "                                                   1.14297980e-05  1.03707213e-05]                                    \n",
      "                                                 [ 1.08486310e-05  1.07030436e-05  1.03995516e-05 ...  1.26701829e-05 \n",
      "                                                  -1.00765445e-04  1.07700789e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.417880564828096 [[ 1.15281684e-05 -1.00877091e-04  1.09002955e-05 ...  1.15871488e-05 [[-6.62535068e-04 -2.53286726e-04  8.55055054e-05 ...  1.09336397e-03 \n",
      "                                                              1.12347252e-05  1.11165453e-05]                                      -1.60374630e-03 -4.34387127e-04]                                    \n",
      "                                                            [ 1.10439372e-05  1.00156690e-05  1.07463568e-05 ... -9.83008322e-05  [-1.21518299e-03  4.78168894e-04  6.09235827e-04 ...  7.99221613e-04 \n",
      "                                                              1.12545406e-05  1.00238805e-05]                                      -1.89425253e-03 -4.86894587e-04]                                    \n",
      "                                                            [-1.00268248e-04  1.06365245e-05  1.15469957e-05 ...  1.07511690e-05  [-1.53352328e-03  1.06056013e-03  1.18192582e-03 ...  7.01622859e-04 \n",
      "                                                              1.11348962e-05  1.14270987e-05]                                      -1.97568201e-03 -1.08932994e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.16914403e-05  1.07734481e-05 -9.99611608e-05 ...  1.05256704e-05  [-4.69080255e-04  6.91695278e-04 -6.67674562e-04 ...  1.74407911e-03 \n",
      "                                                              1.19821650e-05  1.17386686e-05]                                      -1.32875581e-03 -1.19242282e-03]                                    \n",
      "                                                            [ 1.12508002e-05  1.09126951e-05 -1.00906716e-04 ...  1.16648049e-05  [ 1.63287296e-04 -4.70397689e-04 -8.94754917e-04 ...  1.97258603e-03 \n",
      "                                                              1.14297980e-05  1.03707213e-05]                                      -8.00630274e-04 -1.15092413e-03]                                    \n",
      "                                                            [ 1.08486310e-05  1.07030436e-05  1.03995516e-05 ...  1.26701829e-05  [ 3.40847500e-02 -9.86436593e-02 -4.94294953e-02 ...  2.35947287e-02 \n",
      "                                                             -1.00765445e-04  1.07700789e-05]]                                      3.37178545e-02  3.82030771e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.3107602576910783 [[-0.01072076 -0.02043077 -0.03227596 ... -0.01175202 -0.01618638\n",
      "                                       0.00447236]                                                   \n",
      "                                     [ 0.00660207 -0.0104262  -0.0435     ...  0.07056404  0.00100923\n",
      "                                       0.03520378]                                                   \n",
      "                                     [-0.00144028  0.05759554 -0.04145434 ...  0.01528633 -0.0631578 \n",
      "                                      -0.03116085]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.0424637   0.01619039 -0.0241283  ...  0.00570585 -0.00816831\n",
      "                                      -0.00831616]                                                   \n",
      "                                     [ 0.01967074  0.03586164  0.0426366  ... -0.00587092 -0.00967917\n",
      "                                       0.02562133]                                                   \n",
      "                                     [ 0.04898479 -0.19316521 -0.11785161 ...  0.0313809   0.04745008\n",
      "                                       0.04705052]]                                                  \n",
      "Epoch 25, loss: 11.728641\n",
      "== W == -1.1637939114732738\n",
      "enter of the function =  [[ 0.06144514 -0.03316593 -0.02706064 ...  0.00216093  0.1101988  [7 7 0 ... 3 1 6]\n",
      "                            0.02371262]                                                                     \n",
      "                          [-0.04433327 -0.10417994  0.08219595 ...  0.10151759 -0.04292525                  \n",
      "                           -0.045236  ]                                                                     \n",
      "                          [ 0.04776389 -0.01747839 -0.00647868 ... -0.1076478   0.0281213                   \n",
      "                            0.08665255]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.03511986 -0.06777972  0.07626902 ... -0.0708601   0.09112506                  \n",
      "                            0.0658818 ]                                                                     \n",
      "                          [-0.01601934 -0.05909094  0.06617859 ...  0.03536408 -0.00567307                  \n",
      "                           -0.03471129]                                                                     \n",
      "                          [ 0.00818619 -0.03637241 -0.0150821  ...  0.02070828 -0.03187492                  \n",
      "                           -0.00574131]]                                                                    \n",
      "soft max = [[1.17976771e-05 1.07326617e-05 1.07983881e-05 ... 1.11185897e-05\n",
      "             1.23871088e-05 1.13608148e-05]                                 \n",
      "            [1.06134732e-05 9.99692530e-06 1.20450462e-05 ... 1.22800391e-05\n",
      "             1.06284277e-05 1.06038964e-05]                                 \n",
      "            [1.16373692e-05 1.09023584e-05 1.10229432e-05 ... 9.96231739e-06\n",
      "             1.14110116e-05 1.20988459e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.14911524e-05 1.03675195e-05 1.19738672e-05 ... 1.03356328e-05\n",
      "             1.21530794e-05 1.18501357e-05]                                 \n",
      "            [1.09182771e-05 1.04579931e-05 1.18536532e-05 ... 1.14939590e-05\n",
      "             1.10318268e-05 1.07160887e-05]                                 \n",
      "            [1.11857843e-05 1.06983027e-05 1.09285149e-05 ... 1.13267343e-05\n",
      "             1.07465266e-05 1.10310741e-05]]                                \n",
      "log =  102762.91106159249\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.418101229065831 [[ 1.17976771e-05  1.07326617e-05  1.07983881e-05 ... -9.99925214e-05 \n",
      "                                                   1.23871088e-05  1.13608148e-05]                                    \n",
      "                                                 [ 1.06134732e-05  9.99692530e-06  1.20450462e-05 ... -9.88310720e-05 \n",
      "                                                   1.06284277e-05  1.06038964e-05]                                    \n",
      "                                                 [-9.94737419e-05  1.09023584e-05  1.10229432e-05 ...  9.96231739e-06 \n",
      "                                                   1.14110116e-05  1.20988459e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.14911524e-05  1.03675195e-05  1.19738672e-05 ...  1.03356328e-05 \n",
      "                                                   1.21530794e-05  1.18501357e-05]                                    \n",
      "                                                 [ 1.09182771e-05 -1.00653118e-04  1.18536532e-05 ...  1.14939590e-05 \n",
      "                                                   1.10318268e-05  1.07160887e-05]                                    \n",
      "                                                 [ 1.11857843e-05  1.06983027e-05  1.09285149e-05 ...  1.13267343e-05 \n",
      "                                                   1.07465266e-05  1.10310741e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.418101229065831 [[ 1.17976771e-05  1.07326617e-05  1.07983881e-05 ... -9.99925214e-05 [[-6.72033926e-04 -2.54463757e-04  7.94781213e-05 ...  1.11408297e-03 \n",
      "                                                              1.23871088e-05  1.13608148e-05]                                      -1.61902477e-03 -4.49198339e-04]                                    \n",
      "                                                            [ 1.06134732e-05  9.99692530e-06  1.20450462e-05 ... -9.88310720e-05  [-1.22478954e-03  4.77143505e-04  6.03244277e-04 ...  8.20836943e-04 \n",
      "                                                              1.06284277e-05  1.06038964e-05]                                      -1.91023274e-03 -5.02496960e-04]                                    \n",
      "                                                            [-9.94737419e-05  1.09023584e-05  1.10229432e-05 ...  9.96231739e-06  [-1.54292443e-03  1.05934160e-03  1.17579755e-03 ...  7.24559902e-04 \n",
      "                                                              1.14110116e-05  1.20988459e-05]                                      -1.99183495e-03 -1.10593563e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.14911524e-05  1.03675195e-05  1.19738672e-05 ...  1.03356328e-05  [-4.77398192e-04  6.93069045e-04 -6.75017627e-04 ...  1.76716159e-03 \n",
      "                                                              1.21530794e-05  1.18501357e-05]                                      -1.34263127e-03 -1.20786951e-03]                                    \n",
      "                                                            [ 1.09182771e-05 -1.00653118e-04  1.18536532e-05 ...  1.14939590e-05  [ 1.55283316e-04 -4.69488147e-04 -9.02015825e-04 ...  1.99672809e-03 \n",
      "                                                              1.10318268e-05  1.07160887e-05]                                      -8.14517538e-04 -1.16722145e-03]                                    \n",
      "                                                            [ 1.11857843e-05  1.06983027e-05  1.09285149e-05 ...  1.13267343e-05  [ 3.40870018e-02 -9.86790570e-02 -4.94515611e-02 ...  2.36037324e-02 \n",
      "                                                              1.07465266e-05  1.10310741e-05]]                                      3.37232109e-02  3.82102187e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.3170329456661404 [[-0.01083459 -0.02063761 -0.03259787 ... -0.01185861 -0.01636429\n",
      "                                       0.00451274]                                                   \n",
      "                                     [ 0.00665593 -0.01052568 -0.04392891 ...  0.07127767  0.00100038\n",
      "                                       0.03555095]                                                   \n",
      "                                     [-0.00147001  0.0581821  -0.04185706 ...  0.01544621 -0.06380914\n",
      "                                      -0.03148335]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.04289303  0.01635921 -0.02437626 ...  0.00578035 -0.00826328\n",
      "                                      -0.00841124]                                                   \n",
      "                                     [ 0.01986908  0.03621556  0.04305402 ... -0.0059099  -0.00978397\n",
      "                                       0.02586604]                                                   \n",
      "                                     [ 0.04981548 -0.1960833  -0.11952443 ...  0.03193066  0.04826176\n",
      "                                       0.04790305]]                                                  \n",
      "Epoch 26, loss: 11.735134\n",
      "== W == -1.1834129990914863\n",
      "enter of the function =  [[-0.02500045  0.05230324 -0.03254029 ...  0.06072872 -0.04825506 [9 7 1 ... 4 4 7]\n",
      "                           -0.04965209]                                                                     \n",
      "                          [ 0.03578892 -0.05400972  0.04114431 ... -0.04944809  0.04659994                  \n",
      "                            0.03859892]                                                                     \n",
      "                          [-0.0038081  -0.02187412  0.01213097 ... -0.08356603  0.00998736                  \n",
      "                            0.0467042 ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.00498096 -0.03387789  0.00968329 ... -0.02504406  0.05310893                  \n",
      "                            0.01410896]                                                                     \n",
      "                          [ 0.0142791  -0.06948478 -0.03084449 ...  0.03205519 -0.00678812                  \n",
      "                            0.00831735]                                                                     \n",
      "                          [ 0.00239761  0.02862888 -0.06719197 ...  0.00463247 -0.00573924                  \n",
      "                            0.02084479]]                                                                    \n",
      "soft max = [[1.08197022e-05 1.16892831e-05 1.07384302e-05 ... 1.17881869e-05\n",
      "             1.05709973e-05 1.05562396e-05]                                 \n",
      "            [1.14978277e-05 1.05103394e-05 1.15595683e-05 ... 1.05583933e-05\n",
      "             1.16228053e-05 1.15301820e-05]                                 \n",
      "            [1.10514441e-05 1.08535812e-05 1.12290052e-05 ... 1.02042385e-05\n",
      "             1.12049603e-05 1.16240171e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.11490040e-05 1.07240761e-05 1.12015537e-05 ... 1.08192304e-05\n",
      "             1.16987048e-05 1.12512380e-05]                                 \n",
      "            [1.12531524e-05 1.03489434e-05 1.07566559e-05 ... 1.14549780e-05\n",
      "             1.10185595e-05 1.11862635e-05]                                 \n",
      "            [1.11202394e-05 1.14157968e-05 1.03726987e-05 ... 1.11451194e-05\n",
      "             1.10301227e-05 1.13272802e-05]]                                \n",
      "log =  102764.94152984874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.418326836649861 [[ 1.08197022e-05  1.16892831e-05  1.07384302e-05 ...  1.17881869e-05 \n",
      "                                                   1.05709973e-05 -1.00554872e-04]                                    \n",
      "                                                 [ 1.14978277e-05  1.05103394e-05  1.15595683e-05 ... -1.00552718e-04 \n",
      "                                                   1.16228053e-05  1.15301820e-05]                                    \n",
      "                                                 [ 1.10514441e-05 -1.00257530e-04  1.12290052e-05 ...  1.02042385e-05 \n",
      "                                                   1.12049603e-05  1.16240171e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11490040e-05  1.07240761e-05  1.12015537e-05 ...  1.08192304e-05 \n",
      "                                                   1.16987048e-05  1.12512380e-05]                                    \n",
      "                                                 [ 1.12531524e-05  1.03489434e-05  1.07566559e-05 ...  1.14549780e-05 \n",
      "                                                   1.10185595e-05  1.11862635e-05]                                    \n",
      "                                                 [ 1.11202394e-05  1.14157968e-05  1.03726987e-05 ... -9.99659918e-05 \n",
      "                                                   1.10301227e-05  1.13272802e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.418326836649861 [[ 1.08197022e-05  1.16892831e-05  1.07384302e-05 ...  1.17881869e-05 [[-6.81684871e-04 -2.55658559e-04  7.33464049e-05 ...  1.13519314e-03 \n",
      "                                                              1.05709973e-05 -1.00554872e-04]                                      -1.63454643e-03 -4.64257121e-04]                                    \n",
      "                                                            [ 1.14978277e-05  1.05103394e-05  1.15595683e-05 ... -1.00552718e-04  [-1.23455098e-03  4.76101411e-04  5.97147164e-04 ...  8.42859459e-04 \n",
      "                                                              1.16228053e-05  1.15301820e-05]                                      -1.92646580e-03 -5.18358133e-04]                                    \n",
      "                                                            [ 1.10514441e-05 -1.00257530e-04  1.12290052e-05 ...  1.02042385e-05  [-1.55248015e-03  1.05810423e-03  1.16956108e-03 ...  7.47924519e-04 \n",
      "                                                              1.12049603e-05  1.16240171e-05]                                      -2.00824528e-03 -1.12281379e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.11490040e-05  1.07240761e-05  1.12015537e-05 ...  1.08192304e-05  [-4.85858926e-04  6.94448255e-04 -6.82479354e-04 ...  1.79067207e-03 \n",
      "                                                              1.16987048e-05  1.12512380e-05]                                      -1.35674000e-03 -1.22357398e-03]                                    \n",
      "                                                            [ 1.12531524e-05  1.03489434e-05  1.07566559e-05 ...  1.14549780e-05  [ 1.47138833e-04 -4.68577730e-04 -9.09395232e-04 ...  2.02131336e-03 \n",
      "                                                              1.10185595e-05  1.11862635e-05]                                      -8.28639519e-04 -1.18378733e-03]                                    \n",
      "                                                            [ 1.11202394e-05  1.14157968e-05  1.03726987e-05 ... -9.99659918e-05  [ 3.40890808e-02 -9.87150286e-02 -4.94740406e-02 ...  2.36129011e-02 \n",
      "                                                              1.10301227e-05  1.13272802e-05]]                                      3.37285087e-02  3.82173505e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.3234323617671505 [[-0.01094966 -0.02084653 -0.03292305 ... -0.01196605 -0.01654412\n",
      "                                       0.00455337]                                                   \n",
      "                                     [ 0.00671025 -0.01062617 -0.04436216 ...  0.07199865  0.00099128\n",
      "                                       0.03590144]                                                   \n",
      "                                     [-0.00150014  0.05877452 -0.04226387 ...  0.01560792 -0.06446715\n",
      "                                      -0.03180924]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.04332673  0.01652974 -0.02462677 ...  0.00585582 -0.00835934\n",
      "                                      -0.00850744]                                                   \n",
      "                                     [ 0.02006932  0.03657302  0.04347554 ... -0.00594903 -0.00988995\n",
      "                                       0.02611302]                                                   \n",
      "                                     [ 0.05065451 -0.19903092 -0.12121419 ...  0.032486    0.04908161\n",
      "                                       0.04876419]]                                                  \n",
      "Epoch 27, loss: 11.741759\n",
      "== W == -1.2033442676123751\n",
      "enter of the function =  [[ 0.04068334  0.01608321  0.0812878  ... -0.14677947  0.07783949 [7 4 5 ... 4 7 1]\n",
      "                            0.10619841]                                                                     \n",
      "                          [ 0.00199423  0.00397631 -0.01386561 ... -0.02907981 -0.00076951                  \n",
      "                            0.03032095]                                                                     \n",
      "                          [ 0.00519188 -0.02965912 -0.07659549 ...  0.00319643 -0.01366908                  \n",
      "                           -0.03731108]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.04485255 -0.038415   -0.01896266 ...  0.03588692  0.0204583                   \n",
      "                            0.04358794]                                                                     \n",
      "                          [ 0.01437769  0.04403529 -0.04578479 ...  0.03818118 -0.01103902                  \n",
      "                            0.02444603]                                                                     \n",
      "                          [-0.01329256 -0.02736831 -0.05401271 ...  0.06563798 -0.03364444                  \n",
      "                           -0.02749506]]                                                                    \n",
      "soft max = [[1.15531802e-05 1.12724377e-05 1.20319449e-05 ... 9.57827895e-06\n",
      "             1.19905265e-05 1.23354324e-05]                                 \n",
      "            [1.11147341e-05 1.11367862e-05 1.09398467e-05 ... 1.07746655e-05\n",
      "             1.10840583e-05 1.14340797e-05]                                 \n",
      "            [1.11503320e-05 1.07684253e-05 1.02746726e-05 ... 1.11281042e-05\n",
      "             1.09419969e-05 1.06863403e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.06060526e-05 1.06745499e-05 1.08842276e-05 ... 1.14978988e-05\n",
      "             1.13218636e-05 1.15867863e-05]                                 \n",
      "            [1.12532287e-05 1.15919708e-05 1.05961698e-05 ... 1.15243083e-05\n",
      "             1.09708129e-05 1.13671024e-05]                                 \n",
      "            [1.09461176e-05 1.07931221e-05 1.05093431e-05 ... 1.18451130e-05\n",
      "             1.07255951e-05 1.07917541e-05]]                                \n",
      "log =  102767.0177761318\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.418557530681312 [[ 1.15531802e-05  1.12724377e-05  1.20319449e-05 ... -1.01532832e-04 \n",
      "                                                   1.19905265e-05  1.23354324e-05]                                    \n",
      "                                                 [ 1.11147341e-05  1.11367862e-05  1.09398467e-05 ...  1.07746655e-05 \n",
      "                                                   1.10840583e-05  1.14340797e-05]                                    \n",
      "                                                 [ 1.11503320e-05  1.07684253e-05  1.02746726e-05 ...  1.11281042e-05 \n",
      "                                                   1.09419969e-05  1.06863403e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.06060526e-05  1.06745499e-05  1.08842276e-05 ...  1.14978988e-05 \n",
      "                                                   1.13218636e-05  1.15867863e-05]                                    \n",
      "                                                 [ 1.12532287e-05  1.15919708e-05  1.05961698e-05 ... -9.95868028e-05 \n",
      "                                                   1.09708129e-05  1.13671024e-05]                                    \n",
      "                                                 [ 1.09461176e-05 -1.00317989e-04  1.05093431e-05 ...  1.18451130e-05 \n",
      "                                                   1.07255951e-05  1.07917541e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.418557530681312 [[ 1.15531802e-05  1.12724377e-05  1.20319449e-05 ... -1.01532832e-04 [[-6.91490218e-04 -2.56871376e-04  6.71087430e-05 ...  1.15670227e-03 \n",
      "                                                              1.19905265e-05  1.23354324e-05]                                      -1.65031494e-03 -4.79567333e-04]                                    \n",
      "                                                            [ 1.11147341e-05  1.11367862e-05  1.09398467e-05 ...  1.07746655e-05  [-1.24446967e-03  4.75042368e-04  5.90942841e-04 ...  8.65297273e-04 \n",
      "                                                              1.10840583e-05  1.14340797e-05]                                      -1.94295547e-03 -5.34482115e-04]                                    \n",
      "                                                            [ 1.11503320e-05  1.07684253e-05  1.02746726e-05 ...  1.11281042e-05  [-1.56219283e-03  1.05684775e-03  1.16321473e-03 ...  7.71725174e-04 \n",
      "                                                              1.09419969e-05  1.06863403e-05]                                      -2.02491688e-03 -1.13996861e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.06060526e-05  1.06745499e-05  1.08842276e-05 ...  1.14978988e-05  [-4.94464745e-04  6.95832805e-04 -6.90061483e-04 ...  1.81461904e-03 \n",
      "                                                              1.13218636e-05  1.15867863e-05]                                      -1.37108566e-03 -1.23954022e-03]                                    \n",
      "                                                            [ 1.12532287e-05  1.15919708e-05  1.05961698e-05 ... -9.95868028e-05  [ 1.38851568e-04 -4.67666576e-04 -9.16894885e-04 ...  2.04635059e-03 \n",
      "                                                              1.09708129e-05  1.13671024e-05]                                      -8.42999909e-04 -1.20062591e-03]                                    \n",
      "                                                            [ 1.09461176e-05 -1.00317989e-04  1.05093431e-05 ...  1.18451130e-05  [ 3.40909781e-02 -9.87515877e-02 -4.94969449e-02 ...  2.36222392e-02 \n",
      "                                                              1.07255951e-05  1.07917541e-05]]                                      3.37337431e-02  3.82244691e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.3299610705071037 [[-0.01106597 -0.02105755 -0.03325155 ... -0.01207436 -0.01672591\n",
      "                                       0.00459426]                                                   \n",
      "                                     [ 0.006765   -0.01072767 -0.04479981 ...  0.07272707  0.00098193\n",
      "                                       0.03625527]                                                   \n",
      "                                     [-0.00153067  0.05937284 -0.04267482 ...  0.01577148 -0.0651319 \n",
      "                                      -0.03213856]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.04376486  0.01670198 -0.02487986 ...  0.00593229 -0.0084565 \n",
      "                                      -0.00860475]                                                   \n",
      "                                     [ 0.02027149  0.03693406  0.0439012  ... -0.00598831 -0.00999714\n",
      "                                       0.02636232]                                                   \n",
      "                                     [ 0.05150195 -0.20200838 -0.12292107 ...  0.03304699  0.04990971\n",
      "                                       0.049634  ]]                                                  \n",
      "Epoch 28, loss: 11.748519\n",
      "== W == -1.223592022369726\n",
      "enter of the function =  [[-0.04867028 -0.02014472 -0.08201519 ...  0.09193283 -0.10262109 [8 3 2 ... 4 4 9]\n",
      "                           -0.0528724 ]                                                                     \n",
      "                          [-0.003721    0.05253094 -0.01529595 ... -0.02981796 -0.01961092                  \n",
      "                            0.01342127]                                                                     \n",
      "                          [ 0.01731654  0.01152672 -0.01387892 ...  0.0060847   0.00950149                  \n",
      "                            0.03565885]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.08590857 -0.0031302  -0.07944345 ...  0.26219623 -0.16934089                  \n",
      "                           -0.17581611]                                                                     \n",
      "                          [-0.07876707 -0.02573001 -0.08080716 ...  0.24486182 -0.11785937                  \n",
      "                           -0.14438394]                                                                     \n",
      "                          [-0.00757806 -0.04022909 -0.03183884 ...  0.00790936 -0.00703049                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.03935819]]                                                                    \n",
      "soft max = [[1.05646305e-05 1.08703320e-05 1.02181625e-05 ... 1.21595490e-05\n",
      "             1.00097625e-05 1.05203298e-05]                                 \n",
      "            [1.10503373e-05 1.16897559e-05 1.09231677e-05 ... 1.07656875e-05\n",
      "             1.08761361e-05 1.12413982e-05]                                 \n",
      "            [1.12852718e-05 1.12201209e-05 1.09386571e-05 ... 1.11592267e-05\n",
      "             1.11974207e-05 1.14941799e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.01784566e-05 1.10568678e-05 1.02444747e-05 ... 1.44165693e-05\n",
      "             9.36370523e-06 9.30326905e-06]                                 \n",
      "            [1.02514063e-05 1.08097873e-05 1.02305137e-05 ... 1.41688200e-05\n",
      "             9.85838724e-06 9.60033521e-06]                                 \n",
      "            [1.10077976e-05 1.06541860e-05 1.07439534e-05 ... 1.11796070e-05\n",
      "             1.10138268e-05 1.06634688e-05]]                                \n",
      "log =  102769.14113159609\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.418793459066231 [[ 1.05646305e-05  1.08703320e-05  1.02181625e-05 ...  1.21595490e-05 \n",
      "                                                  -1.01101349e-04  1.05203298e-05]                                    \n",
      "                                                 [ 1.10503373e-05  1.16897559e-05  1.09231677e-05 ...  1.07656875e-05 \n",
      "                                                   1.08761361e-05  1.12413982e-05]                                    \n",
      "                                                 [ 1.12852718e-05  1.12201209e-05 -1.00172454e-04 ...  1.11592267e-05 \n",
      "                                                   1.11974207e-05  1.14941799e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.01784566e-05  1.10568678e-05  1.02444747e-05 ...  1.44165693e-05 \n",
      "                                                   9.36370523e-06  9.30326905e-06]                                    \n",
      "                                                 [ 1.02514063e-05  1.08097873e-05  1.02305137e-05 ...  1.41688200e-05 \n",
      "                                                   9.85838724e-06  9.60033521e-06]                                    \n",
      "                                                 [ 1.10077976e-05  1.06541860e-05  1.07439534e-05 ...  1.11796070e-05 \n",
      "                                                   1.10138268e-05 -1.00447642e-04]]                                   \n",
      "loss , grand (prediction), grad by W =  11.418793459066231 [[ 1.05646305e-05  1.08703320e-05  1.02181625e-05 ...  1.21595490e-05 [[-7.01452312e-04 -2.58102457e-04  6.07635044e-05 ...  1.17861832e-03 \n",
      "                                                             -1.01101349e-04  1.05203298e-05]                                      -1.66633401e-03 -4.95132884e-04]                                    \n",
      "                                                            [ 1.10503373e-05  1.16897559e-05  1.09231677e-05 ...  1.07656875e-05  [-1.25454802e-03  4.73966131e-04  5.84629639e-04 ...  8.88158678e-04 \n",
      "                                                              1.08761361e-05  1.12413982e-05]                                      -1.95970562e-03 -5.50872967e-04]                                    \n",
      "                                                            [ 1.12852718e-05  1.12201209e-05 -1.00172454e-04 ...  1.11592267e-05  [-1.57206490e-03  1.05557191e-03  1.15675680e-03 ...  7.95970523e-04 \n",
      "                                                              1.11974207e-05  1.14941799e-05]                                      -2.04185366e-03 -1.15740435e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.01784566e-05  1.10568678e-05  1.02444747e-05 ...  1.44165693e-05  [-5.03217968e-04  6.97222590e-04 -6.97765772e-04 ...  1.83901116e-03 \n",
      "                                                              9.36370523e-06  9.30326905e-06]                                      -1.38567197e-03 -1.25577230e-03]                                    \n",
      "                                                            [ 1.02514063e-05  1.08097873e-05  1.02305137e-05 ...  1.41688200e-05  [ 1.30419213e-04 -4.66754825e-04 -9.24516548e-04 ...  2.07184869e-03 \n",
      "                                                              9.85838724e-06  9.60033521e-06]                                      -8.57602447e-04 -1.21774137e-03]                                    \n",
      "                                                            [ 1.10077976e-05  1.06541860e-05  1.07439534e-05 ...  1.11796070e-05  [ 3.40926846e-02 -9.87887486e-02 -4.95202857e-02 ...  2.36317516e-02 \n",
      "                                                              1.10138268e-05 -1.00447642e-04]]                                      3.37389089e-02  3.82315708e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.33662168850072083 [[-0.01118355 -0.02127069 -0.03358339 ... -0.01218354 -0.01690967\n",
      "                                        0.00463541]                                                   \n",
      "                                      [ 0.00682021 -0.01083019 -0.0452419  ...  0.07346299  0.00097232\n",
      "                                        0.03661248]                                                   \n",
      "                                      [-0.0015616   0.05997714 -0.04308993 ...  0.01593691 -0.06580347\n",
      "                                       -0.03247135]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.04420745  0.01687596 -0.02513556 ...  0.00600976 -0.00855478\n",
      "                                       -0.00870319]                                                   \n",
      "                                      [ 0.02047559  0.03729873  0.04433105 ... -0.00602773 -0.01010554\n",
      "                                        0.02661393]                                                   \n",
      "                                      [ 0.05235787 -0.20501598 -0.12464525 ...  0.03361368  0.05074615\n",
      "                                        0.05051259]]                                                  \n",
      "Epoch 29, loss: 11.755415\n",
      "== W == -1.2441605997917446\n",
      "enter of the function =  [[ 0.08414622  0.00045398  0.07130383 ... -0.16805346  0.11737904 [1 1 2 ... 0 9 1]\n",
      "                            0.12496779]                                                                     \n",
      "                          [-0.01352952 -0.04287394 -0.06696082 ...  0.05098132 -0.00250619                  \n",
      "                           -0.00389228]                                                                     \n",
      "                          [-0.00497682 -0.0123668  -0.12257419 ...  0.17241057  0.05411003                  \n",
      "                           -0.06940111]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.06392451 -0.01669991  0.04008128 ... -0.13387002  0.08520647                  \n",
      "                            0.10971784]                                                                     \n",
      "                          [ 0.05772651 -0.03094037  0.0408024  ... -0.17064588  0.10916279                  \n",
      "                            0.13115551]                                                                     \n",
      "                          [-0.05882363 -0.06689424 -0.10184171 ...  0.07681919 -0.18243106                  \n",
      "                           -0.12484669]]                                                                    \n",
      "soft max = [[1.20640383e-05 1.10954682e-05 1.19100978e-05 ... 9.37483815e-06\n",
      "             1.24716967e-05 1.25667013e-05]                                 \n",
      "            [1.09413945e-05 1.06249906e-05 1.03721253e-05 ... 1.16704976e-05\n",
      "             1.10626722e-05 1.10473490e-05]                                 \n",
      "            [1.10353742e-05 1.09541236e-05 9.81104290e-06 ... 1.31772695e-05\n",
      "             1.17070685e-05 1.03468452e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.18225329e-05 1.09067609e-05 1.15439796e-05 ... 9.70084264e-06\n",
      "             1.20768360e-05 1.23765136e-05]                                 \n",
      "            [1.17494835e-05 1.07525443e-05 1.15523071e-05 ... 9.35056613e-06\n",
      "             1.23696459e-05 1.26447015e-05]                                 \n",
      "            [1.04568696e-05 1.03728159e-05 1.00165734e-05 ... 1.19759678e-05\n",
      "             9.24101482e-06 9.78877263e-06]]                                \n",
      "log =  102771.31297224114\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.41903477469346 [[ 1.20640383e-05 -1.00015643e-04  1.19100978e-05 ...  9.37483815e-06 \n",
      "                                                  1.24716967e-05  1.25667013e-05]                                    \n",
      "                                                [ 1.09413945e-05 -1.00486120e-04  1.03721253e-05 ...  1.16704976e-05 \n",
      "                                                  1.10626722e-05  1.10473490e-05]                                    \n",
      "                                                [ 1.10353742e-05  1.09541236e-05 -1.01300068e-04 ...  1.31772695e-05 \n",
      "                                                  1.17070685e-05  1.03468452e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [-9.92885782e-05  1.09067609e-05  1.15439796e-05 ...  9.70084264e-06 \n",
      "                                                  1.20768360e-05  1.23765136e-05]                                    \n",
      "                                                [ 1.17494835e-05  1.07525443e-05  1.15523071e-05 ...  9.35056613e-06 \n",
      "                                                  1.23696459e-05 -9.84664096e-05]                                    \n",
      "                                                [ 1.04568696e-05 -1.00738295e-04  1.00165734e-05 ...  1.19759678e-05 \n",
      "                                                  9.24101482e-06  9.78877263e-06]]                                   \n",
      "loss , grand (prediction), grad by W =  11.41903477469346 [[ 1.20640383e-05 -1.00015643e-04  1.19100978e-05 ...  9.37483815e-06 [[-7.11573525e-04 -2.59352052e-04  5.43090393e-05 ...  1.20094944e-03 \n",
      "                                                             1.24716967e-05  1.25667013e-05]                                      -1.68260740e-03 -5.10957732e-04]                                    \n",
      "                                                           [ 1.09413945e-05 -1.00486120e-04  1.03721253e-05 ...  1.16704976e-05  [-1.26478844e-03  4.72872452e-04  5.78205872e-04 ...  9.11452152e-04 \n",
      "                                                             1.10626722e-05  1.10473490e-05]                                      -1.97672011e-03 -5.67534797e-04]                                    \n",
      "                                                           [ 1.10353742e-05  1.09541236e-05 -1.01300068e-04 ...  1.31772695e-05  [-1.58209881e-03  1.05427642e-03  1.15018554e-03 ...  8.20669411e-04 \n",
      "                                                             1.17070685e-05  1.03468452e-05]                                      -2.05905961e-03 -1.17512528e-03]                                    \n",
      "                                                           ...                                                                   ...                                                                  \n",
      "                                                           [-9.92885782e-05  1.09067609e-05  1.15439796e-05 ...  9.70084264e-06  [-5.12120941e-04  6.98617500e-04 -7.05593997e-04 ...  1.86385731e-03 \n",
      "                                                             1.20768360e-05  1.23765136e-05]                                      -1.40050267e-03 -1.27227431e-03]                                    \n",
      "                                                           [ 1.17494835e-05  1.07525443e-05  1.15523071e-05 ...  9.35056613e-06  [ 1.21839430e-04 -4.65842622e-04 -9.32262005e-04 ...  2.09781678e-03 \n",
      "                                                             1.23696459e-05 -9.84664096e-05]                                      -8.72450917e-04 -1.23513795e-03]                                    \n",
      "                                                           [ 1.04568696e-05 -1.00738295e-04  1.00165734e-05 ...  1.19759678e-05  [ 3.40941905e-02 -9.88265258e-02 -4.95440748e-02 ...  2.36414433e-02 \n",
      "                                                             9.24101482e-06  9.78877263e-06]]                                      3.37440006e-02  3.82386519e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.34341688553148997 [[-0.0113024  -0.02148598 -0.03391862 ... -0.01229359 -0.01709543\n",
      "                                        0.00467681]                                                   \n",
      "                                      [ 0.00687586 -0.01093376 -0.04568847 ...  0.0742065   0.00096245\n",
      "                                        0.03697309]                                                   \n",
      "                                      [-0.00159294  0.06058747 -0.04350927 ...  0.01610424 -0.06648192\n",
      "                                       -0.03280763]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.04465456  0.01705169 -0.0253939  ...  0.00608825 -0.00865418\n",
      "                                       -0.00880278]                                                   \n",
      "                                      [ 0.02068165  0.03766705  0.04476511 ... -0.00606729 -0.01021517\n",
      "                                        0.02686789]                                                   \n",
      "                                      [ 0.05322238 -0.20805403 -0.1263869  ...  0.03418614  0.051591  \n",
      "                                        0.05140003]]                                                  \n",
      "Epoch 30, loss: 11.762452\n",
      "== W == -1.2650543661174605\n",
      "enter of the function =  [[-0.00224878 -0.0778733  -0.10590874 ...  0.14229652 -0.09490769 [1 8 3 ... 2 1 5]\n",
      "                           -0.07895632]                                                                     \n",
      "                          [ 0.01918082 -0.00192784 -0.00303732 ... -0.05145884  0.02731246                  \n",
      "                            0.05589441]                                                                     \n",
      "                          [ 0.03915079 -0.02400547 -0.05943603 ...  0.09973577  0.03275062                  \n",
      "                           -0.05057025]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.02438893 -0.03935603 -0.0095459  ... -0.10685477  0.04907256                  \n",
      "                            0.0979193 ]                                                                     \n",
      "                          [ 0.03796365 -0.0482673   0.04503643 ... -0.13884185  0.09085288                  \n",
      "                            0.10839804]                                                                     \n",
      "                          [-0.03015768 -0.01137434 -0.05684485 ...  0.04312117 -0.03436575                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.02402701]]                                                                    \n",
      "soft max = [[1.10643796e-05 1.02584975e-05 9.97489010e-06 ... 1.27850460e-05\n",
      "             1.00852302e-05 1.02473934e-05]                                 \n",
      "            [1.13040436e-05 1.10679312e-05 1.10556583e-05 ... 1.05330807e-05\n",
      "             1.13963388e-05 1.17267680e-05]                                 \n",
      "            [1.15320542e-05 1.08262551e-05 1.04493906e-05 ... 1.22523218e-05\n",
      "             1.14584828e-05 1.05424444e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.13630699e-05 1.06613351e-05 1.09839354e-05 ... 9.96545799e-06\n",
      "             1.16470420e-05 1.22300861e-05]                                 \n",
      "            [1.15183722e-05 1.05667512e-05 1.16001279e-05 ... 9.65173643e-06\n",
      "             1.21439678e-05 1.23589158e-05]                                 \n",
      "            [1.07598542e-05 1.09638703e-05 1.04765019e-05 ... 1.15779318e-05\n",
      "             1.07146712e-05 1.08260219e-05]]                                \n",
      "log =  102773.534720578\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.419281635619777 [[ 1.10643796e-05 -1.00852614e-04  9.97489010e-06 ...  1.27850460e-05 \n",
      "                                                   1.00852302e-05  1.02473934e-05]                                    \n",
      "                                                 [ 1.13040436e-05  1.10679312e-05  1.10556583e-05 ...  1.05330807e-05 \n",
      "                                                  -9.97147723e-05  1.17267680e-05]                                    \n",
      "                                                 [ 1.15320542e-05  1.08262551e-05  1.04493906e-05 ...  1.22523218e-05 \n",
      "                                                   1.14584828e-05  1.05424444e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.13630699e-05  1.06613351e-05 -1.00127176e-04 ...  9.96545799e-06 \n",
      "                                                   1.16470420e-05  1.22300861e-05]                                    \n",
      "                                                 [ 1.15183722e-05 -1.00544360e-04  1.16001279e-05 ...  9.65173643e-06 \n",
      "                                                   1.21439678e-05  1.23589158e-05]                                    \n",
      "                                                 [ 1.07598542e-05  1.09638703e-05  1.04765019e-05 ...  1.15779318e-05 \n",
      "                                                   1.07146712e-05  1.08260219e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.419281635619777 [[ 1.10643796e-05 -1.00852614e-04  9.97489010e-06 ...  1.27850460e-05 [[-7.21856255e-04 -2.60620414e-04  4.77436804e-05 ...  1.22370395e-03 \n",
      "                                                              1.00852302e-05  1.02473934e-05]                                      -1.69913893e-03 -5.27045884e-04]                                    \n",
      "                                                            [ 1.13040436e-05  1.10679312e-05  1.10556583e-05 ...  1.05330807e-05  [-1.27519338e-03  4.71761079e-04  5.71669834e-04 ...  9.35186359e-04 \n",
      "                                                             -9.97147723e-05  1.17267680e-05]                                      -1.99400287e-03 -5.84471768e-04]                                    \n",
      "                                                            [ 1.15320542e-05  1.08262551e-05  1.04493906e-05 ...  1.22523218e-05  [-1.59229703e-03  1.05296103e-03  1.14349923e-03 ...  8.45830880e-04 \n",
      "                                                              1.14584828e-05  1.05424444e-05]                                      -2.07653874e-03 -1.19313578e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.13630699e-05  1.06613351e-05 -1.00127176e-04 ...  9.96545799e-06  [-5.21176042e-04  7.00017419e-04 -7.13547952e-04 ...  1.88916654e-03 \n",
      "                                                              1.16470420e-05  1.22300861e-05]                                      -1.41558158e-03 -1.28905040e-03]                                    \n",
      "                                                            [ 1.15183722e-05 -1.00544360e-04  1.16001279e-05 ...  9.65173643e-06  [ 1.13109849e-04 -4.64930117e-04 -9.40133057e-04 ...  2.12426419e-03 \n",
      "                                                              1.21439678e-05  1.23589158e-05]                                      -8.87549152e-04 -1.25281995e-03]                                    \n",
      "                                                            [ 1.07598542e-05  1.09638703e-05  1.04765019e-05 ...  1.15779318e-05  [ 3.40954859e-02 -9.88649342e-02 -4.95683248e-02 ...  2.36513193e-02 \n",
      "                                                              1.07146712e-05  1.08260219e-05]]                                      3.37490127e-02  3.82457084e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.3503493856409269 [[-0.01142254 -0.02170343 -0.03425726 ... -0.01240451 -0.01728321\n",
      "                                       0.00471847]                                                   \n",
      "                                     [ 0.00693197 -0.01103837 -0.04613958 ...  0.07495768  0.0009523 \n",
      "                                       0.03733715]                                                   \n",
      "                                     [-0.00162469  0.06120389 -0.04393286 ...  0.01627349 -0.06716733\n",
      "                                      -0.03314746]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.04510622  0.01722919 -0.02565489 ...  0.00616777 -0.00875473\n",
      "                                      -0.00890353]                                                   \n",
      "                                     [ 0.02088969  0.03803906  0.04520344 ... -0.00610698 -0.01032605\n",
      "                                       0.02712422]                                                   \n",
      "                                     [ 0.05409555 -0.21112284 -0.12814621 ...  0.03476441  0.05244435\n",
      "                                       0.05229642]]                                                  \n",
      "Epoch 31, loss: 11.769631\n",
      "== W == -1.2862777160071026\n",
      "enter of the function =  [[-0.01922452 -0.01915499 -0.0243805  ...  0.02270855 -0.00968959 [9 5 5 ... 9 2 9]\n",
      "                           -0.08297671]                                                                     \n",
      "                          [-0.08406734  0.02633468 -0.06748541 ...  0.15968022 -0.04473188                  \n",
      "                           -0.10241641]                                                                     \n",
      "                          [-0.00575921  0.0218317  -0.02742928 ...  0.07214129 -0.06386266                  \n",
      "                           -0.09467658]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.0017164  -0.00984689 -0.03171266 ...  0.03659649 -0.03127275                  \n",
      "                           -0.02384296]                                                                     \n",
      "                          [-0.00981242 -0.00939424 -0.01635668 ... -0.01267705  0.01126694                  \n",
      "                            0.02689262]                                                                     \n",
      "                          [ 0.1006891  -0.05487169  0.0378985  ... -0.14494551  0.12296291                  \n",
      "                            0.14750659]]                                                                    \n",
      "soft max = [[1.08769737e-05 1.08777299e-05 1.08210366e-05 ... 1.13427766e-05\n",
      "             1.09811809e-05 1.02051843e-05]                                 \n",
      "            [1.01940603e-05 1.13839817e-05 1.03645067e-05 ... 1.30078469e-05\n",
      "             1.06030393e-05 1.00087144e-05]                                 \n",
      "            [1.10244260e-05 1.13328350e-05 1.07880959e-05 ... 1.19175708e-05\n",
      "             1.04021228e-05 1.00864808e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.10690858e-05 1.09794537e-05 1.07419851e-05 ... 1.15014033e-05\n",
      "             1.07467117e-05 1.08268548e-05]                                 \n",
      "            [1.09798322e-05 1.09844247e-05 1.09082119e-05 ... 1.09484240e-05\n",
      "             1.12137366e-05 1.13903349e-05]                                 \n",
      "            [1.22626942e-05 1.04960697e-05 1.15163881e-05 ... 9.59197733e-06\n",
      "             1.25388958e-05 1.28504542e-05]]                                \n",
      "log =  102775.8078473644\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.41953420526271 [[ 1.08769737e-05  1.08777299e-05  1.08210366e-05 ...  1.13427766e-05 \n",
      "                                                  1.09811809e-05 -1.00905927e-04]                                    \n",
      "                                                [ 1.01940603e-05  1.13839817e-05  1.03645067e-05 ...  1.30078469e-05 \n",
      "                                                  1.06030393e-05  1.00087144e-05]                                    \n",
      "                                                [ 1.10244260e-05  1.13328350e-05  1.07880959e-05 ...  1.19175708e-05 \n",
      "                                                  1.04021228e-05  1.00864808e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [ 1.10690858e-05  1.09794537e-05  1.07419851e-05 ...  1.15014033e-05 \n",
      "                                                  1.07467117e-05 -1.00284256e-04]                                    \n",
      "                                                [ 1.09798322e-05  1.09844247e-05 -1.00202899e-04 ...  1.09484240e-05 \n",
      "                                                  1.12137366e-05  1.13903349e-05]                                    \n",
      "                                                [ 1.22626942e-05  1.04960697e-05  1.15163881e-05 ...  9.59197733e-06 \n",
      "                                                  1.25388958e-05 -9.82606569e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.41953420526271 [[ 1.08769737e-05  1.08777299e-05  1.08210366e-05 ...  1.13427766e-05 [[-7.32302932e-04 -2.61907799e-04  4.10657421e-05 ...  1.24689037e-03 \n",
      "                                                             1.09811809e-05 -1.00905927e-04]                                      -1.71593244e-03 -5.43401397e-04]                                    \n",
      "                                                           [ 1.01940603e-05  1.13839817e-05  1.03645067e-05 ...  1.30078469e-05  [-1.28576533e-03  4.70631760e-04  5.65019804e-04 ...  9.59370161e-04 \n",
      "                                                             1.06030393e-05  1.00087144e-05]                                      -2.01155789e-03 -6.01688090e-04]                                    \n",
      "                                                           [ 1.10244260e-05  1.13328350e-05  1.07880959e-05 ...  1.19175708e-05  [-1.60266208e-03  1.05162545e-03  1.13669608e-03 ...  8.71464173e-04 \n",
      "                                                             1.04021228e-05  1.00864808e-05]                                      -2.09429511e-03 -1.21144023e-03]                                    \n",
      "                                                           ...                                                                   ...                                                                  \n",
      "                                                           [ 1.10690858e-05  1.09794537e-05  1.07419851e-05 ...  1.15014033e-05  [-5.30385679e-04  7.01422228e-04 -7.21629448e-04 ...  1.91494814e-03 \n",
      "                                                             1.07467117e-05 -1.00284256e-04]                                      -1.43091254e-03 -1.30610480e-03]                                    \n",
      "                                                           [ 1.09798322e-05  1.09844247e-05 -1.00202899e-04 ...  1.09484240e-05  [ 1.04228074e-04 -4.64017463e-04 -9.48131522e-04 ...  2.15120046e-03 \n",
      "                                                             1.12137366e-05  1.13903349e-05]                                      -9.02901033e-04 -1.27079171e-03]                                    \n",
      "                                                           [ 1.22626942e-05  1.04960697e-05  1.15163881e-05 ...  9.59197733e-06  [ 3.40965604e-02 -9.89039893e-02 -4.95930483e-02 ...  2.36613848e-02 \n",
      "                                                             1.25388958e-05 -9.82606569e-05]]                                      3.37539393e-02  3.82527360e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.3574219682405373 [[-0.01154398 -0.02192308 -0.03459936 ... -0.01251632 -0.01747303\n",
      "                                       0.00476039]                                                   \n",
      "                                     [ 0.00698854 -0.01114403 -0.04659526 ...  0.07571661  0.00094189\n",
      "                                       0.03770467]                                                   \n",
      "                                     [-0.00165686  0.06182645 -0.04436075 ...  0.01644468 -0.06785977\n",
      "                                      -0.03349087]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.0455625   0.01740848 -0.02591857 ...  0.00624834 -0.00885643\n",
      "                                      -0.00900545]                                                   \n",
      "                                     [ 0.02109971  0.0384148   0.04564607 ... -0.00614681 -0.01043818\n",
      "                                       0.02738294]                                                   \n",
      "                                     [ 0.05497746 -0.21422271 -0.12992336 ...  0.03534857  0.05330628\n",
      "                                       0.05320184]]                                                  \n",
      "Epoch 32, loss: 11.776956\n",
      "== W == -1.3078350710402857\n",
      "enter of the function =  [[-3.85047175e-02  1.02655397e-02 -6.02838412e-02 ...  1.58318838e-01 [6 7 2 ... 2 1 8]\n",
      "                           -2.34559379e-02 -8.04337528e-02]                                                     \n",
      "                          [-5.87698729e-03  1.02029308e-02 -7.85077383e-02 ...  4.96113552e-02                  \n",
      "                           -6.34713037e-02 -2.23054763e-02]                                                     \n",
      "                          [ 6.06655775e-03 -4.69304017e-02 -1.32693359e-02 ...  1.41807032e-04                  \n",
      "                           -1.41602061e-03 -2.28611187e-02]                                                     \n",
      "                          ...                                                                                   \n",
      "                          [-2.95585152e-03 -1.74190147e-02  3.47182233e-03 ... -5.30148757e-02                  \n",
      "                           -1.32187635e-02  3.12036370e-02]                                                     \n",
      "                          [ 1.81955633e-02 -6.27765188e-03 -2.62606792e-02 ...  9.37027255e-03                  \n",
      "                            5.11943806e-03 -1.15710100e-02]                                                     \n",
      "                          [ 3.66929244e-02 -9.36101286e-02 -1.71232560e-03 ... -6.51561811e-02                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            5.21218687e-02  3.03379169e-02]]                                                    \n",
      "soft max = [[1.06680847e-05 1.12012659e-05 1.04382550e-05 ... 1.29887046e-05\n",
      "             1.08298404e-05 1.02300300e-05]                                 \n",
      "            [1.10219008e-05 1.12005647e-05 1.02497521e-05 ... 1.16507740e-05\n",
      "             1.04050364e-05 1.08423069e-05]                                 \n",
      "            [1.11543306e-05 1.05785764e-05 1.09407235e-05 ... 1.10884394e-05\n",
      "             1.10711790e-05 1.08362841e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.10541443e-05 1.08954171e-05 1.11254256e-05 ... 1.05144067e-05\n",
      "             1.09412768e-05 1.14382717e-05]                                 \n",
      "            [1.12904454e-05 1.10174856e-05 1.07995081e-05 ... 1.11912423e-05\n",
      "             1.11437711e-05 1.09593202e-05]                                 \n",
      "            [1.15012323e-05 1.00961194e-05 1.10678990e-05 ... 1.03875200e-05\n",
      "             1.16800602e-05 1.14283736e-05]]                                \n",
      "log =  102778.13387341183\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.419792652601314 [[ 1.06680847e-05  1.12012659e-05  1.04382550e-05 ...  1.29887046e-05 \n",
      "                                                   1.08298404e-05  1.02300300e-05]                                    \n",
      "                                                 [ 1.10219008e-05  1.12005647e-05  1.02497521e-05 ... -9.94603371e-05 \n",
      "                                                   1.04050364e-05  1.08423069e-05]                                    \n",
      "                                                 [ 1.11543306e-05  1.05785764e-05 -1.00170388e-04 ...  1.10884394e-05 \n",
      "                                                   1.10711790e-05  1.08362841e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.10541443e-05  1.08954171e-05 -9.99856855e-05 ...  1.05144067e-05 \n",
      "                                                   1.09412768e-05  1.14382717e-05]                                    \n",
      "                                                 [ 1.12904454e-05 -1.00093626e-04  1.07995081e-05 ...  1.11912423e-05 \n",
      "                                                   1.11437711e-05  1.09593202e-05]                                    \n",
      "                                                 [ 1.15012323e-05  1.00961194e-05  1.10678990e-05 ...  1.03875200e-05 \n",
      "                                                  -9.94310509e-05  1.14283736e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.419792652601314 [[ 1.06680847e-05  1.12012659e-05  1.04382550e-05 ...  1.29887046e-05 [[-7.42916010e-04 -2.63214463e-04  3.42735216e-05 ...  1.27051739e-03 \n",
      "                                                              1.08298404e-05  1.02300300e-05]                                      -1.73299183e-03 -5.60028376e-04]                                    \n",
      "                                                            [ 1.10219008e-05  1.12005647e-05  1.02497521e-05 ... -9.94603371e-05  [-1.29650679e-03  4.69484240e-04  5.58254039e-04 ...  9.84012616e-04 \n",
      "                                                              1.04050364e-05  1.08423069e-05]                                      -2.02938919e-03 -6.19188026e-04]                                    \n",
      "                                                            [ 1.11543306e-05  1.05785764e-05 -1.00170388e-04 ...  1.10884394e-05  [-1.61319650e-03  1.05026941e-03  1.12977432e-03 ...  8.97578741e-04 \n",
      "                                                              1.10711790e-05  1.08362841e-05]                                      -2.11233284e-03 -1.23004308e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.10541443e-05  1.08954171e-05 -9.99856855e-05 ...  1.05144067e-05  [-5.39752288e-04  7.02831803e-04 -7.29840314e-04 ...  1.94121156e-03 \n",
      "                                                              1.09412768e-05  1.14382717e-05]                                      -1.44649946e-03 -1.32344174e-03]                                    \n",
      "                                                            [ 1.12904454e-05 -1.00093626e-04  1.07995081e-05 ...  1.11912423e-05  [ 9.51916757e-05 -4.63104819e-04 -9.56259235e-04 ...  2.17863533e-03 \n",
      "                                                              1.11437711e-05  1.09593202e-05]                                      -9.18510486e-04 -1.28905762e-03]                                    \n",
      "                                                            [ 1.15012323e-05  1.00961194e-05  1.10678990e-05 ...  1.03875200e-05  [ 3.40974032e-02 -9.89437072e-02 -4.96182588e-02 ...  2.36716455e-02 \n",
      "                                                             -9.94310509e-05  1.14283736e-05]]                                      3.37587740e-02  3.82597304e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.3646374692469711 [[-0.01166674 -0.02214493 -0.03494494 ... -0.01262901 -0.01766492\n",
      "                                       0.00480256]                                                   \n",
      "                                     [ 0.00704557 -0.01125077 -0.04705556 ...  0.07648337  0.00093119\n",
      "                                       0.0380757 ]                                                   \n",
      "                                     [-0.00168945  0.06245524 -0.04479299 ...  0.01661784 -0.06855931\n",
      "                                      -0.03383789]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.04602343  0.01758958 -0.02618498 ...  0.00632997 -0.00895931\n",
      "                                      -0.00910857]                                                   \n",
      "                                     [ 0.02131175  0.03879431  0.04609305 ... -0.00618677 -0.01055159\n",
      "                                       0.02764406]                                                   \n",
      "                                     [ 0.0558682  -0.21735398 -0.13171852 ...  0.03593867  0.05417688\n",
      "                                       0.05411638]]                                                  \n",
      "Epoch 33, loss: 11.784430\n",
      "== W == -1.3297308780954986\n",
      "enter of the function =  [[-0.02657484 -0.10594307 -0.08085227 ...  0.03889765 -0.00679814 [7 5 0 ... 3 1 6]\n",
      "                           -0.02675009]                                                                     \n",
      "                          [-0.03663442  0.01842689 -0.03253284 ...  0.1327443  -0.09629584                  \n",
      "                           -0.11068982]                                                                     \n",
      "                          [-0.06950237 -0.10395427 -0.04656796 ...  0.03677615 -0.02000814                  \n",
      "                           -0.01272301]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.01526385 -0.10012647 -0.0122226  ...  0.01523984 -0.03366295                  \n",
      "                            0.0238673 ]                                                                     \n",
      "                          [-0.03220404 -0.03422617  0.02701756 ...  0.03366144 -0.0674043                   \n",
      "                            0.00175461]                                                                     \n",
      "                          [ 0.02458547  0.02254216  0.02867354 ... -0.06233984  0.00151651                  \n",
      "                            0.01831181]]                                                                    \n",
      "soft max = [[1.07948674e-05 9.97121613e-06 1.02245670e-05 ... 1.15252844e-05\n",
      "             1.10104793e-05 1.07929758e-05]                                 \n",
      "            [1.06868199e-05 1.12917516e-05 1.07307428e-05 ... 1.26592721e-05\n",
      "             1.00678762e-05 9.92399738e-06]                                 \n",
      "            [1.03412758e-05 9.99106656e-06 1.05811875e-05 ... 1.15008595e-05\n",
      "             1.08659873e-05 1.09454365e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.09176612e-05 1.00293837e-05 1.09509151e-05 ... 1.12558215e-05\n",
      "             1.07186228e-05 1.13533507e-05]                                 \n",
      "            [1.07342716e-05 1.07125875e-05 1.13891732e-05 ... 1.14650934e-05\n",
      "             1.03629954e-05 1.11050531e-05]                                 \n",
      "            [1.13615074e-05 1.13383160e-05 1.14080490e-05 ... 1.04156115e-05\n",
      "             1.11024092e-05 1.12904522e-05]]                                \n",
      "log =  102780.5143714683\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.420057152385368 [[ 1.07948674e-05  9.97121613e-06  1.02245670e-05 ... -9.95858267e-05 \n",
      "                                                   1.10104793e-05  1.07929758e-05]                                    \n",
      "                                                 [ 1.06868199e-05  1.12917516e-05  1.07307428e-05 ...  1.26592721e-05 \n",
      "                                                   1.00678762e-05  9.92399738e-06]                                    \n",
      "                                                 [-1.00769835e-04  9.99106656e-06  1.05811875e-05 ...  1.15008595e-05 \n",
      "                                                   1.08659873e-05  1.09454365e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.09176612e-05  1.00293837e-05  1.09509151e-05 ...  1.12558215e-05 \n",
      "                                                   1.07186228e-05  1.13533507e-05]                                    \n",
      "                                                 [ 1.07342716e-05 -1.00398524e-04  1.13891732e-05 ...  1.14650934e-05 \n",
      "                                                   1.03629954e-05  1.11050531e-05]                                    \n",
      "                                                 [ 1.13615074e-05  1.13383160e-05  1.14080490e-05 ...  1.04156115e-05 \n",
      "                                                   1.11024092e-05  1.12904522e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.420057152385368 [[ 1.07948674e-05  9.97121613e-06  1.02245670e-05 ... -9.95858267e-05 [[-7.53697973e-04 -2.64540666e-04  2.73652984e-05 ...  1.29459392e-03 \n",
      "                                                              1.10104793e-05  1.07929758e-05]                                      -1.75032105e-03 -5.76930977e-04]                                    \n",
      "                                                            [ 1.06868199e-05  1.12917516e-05  1.07307428e-05 ...  1.26592721e-05  [-1.30742031e-03  4.68318261e-04  5.51370779e-04 ...  1.00912299e-03 \n",
      "                                                              1.00678762e-05  9.92399738e-06]                                      -2.04750082e-03 -6.36975887e-04]                                    \n",
      "                                                            [-1.00769835e-04  9.99106656e-06  1.05811875e-05 ...  1.15008595e-05  [-1.62390285e-03  1.04889264e-03  1.12273214e-03 ...  9.24184248e-04 \n",
      "                                                              1.08659873e-05  1.09454365e-05]                                      -2.13065609e-03 -1.24894886e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.09176612e-05  1.00293837e-05  1.09509151e-05 ...  1.12558215e-05  [-5.49278335e-04  7.04246015e-04 -7.38182396e-04 ...  1.96796651e-03 \n",
      "                                                              1.07186228e-05  1.13533507e-05]                                      -1.46234628e-03 -1.34106554e-03]                                    \n",
      "                                                            [ 1.07342716e-05 -1.00398524e-04  1.13891732e-05 ...  1.14650934e-05  [ 8.59981969e-05 -4.62192347e-04 -9.64518049e-04 ...  2.20657877e-03 \n",
      "                                                              1.03629954e-05  1.11050531e-05]                                      -9.34381486e-04 -1.30762212e-03]                                    \n",
      "                                                            [ 1.13615074e-05  1.13383160e-05  1.14080490e-05 ...  1.04156115e-05  [ 3.40980030e-02 -9.89841043e-02 -4.96439698e-02 ...  2.36821069e-02 \n",
      "                                                              1.11024092e-05  1.12904522e-05]]                                      3.37635106e-02  3.82666870e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.37199878224087335 [[-0.01179084 -0.02236901 -0.03529405 ... -0.0127426  -0.0178589 \n",
      "                                        0.00484498]                                                   \n",
      "                                      [ 0.00710306 -0.01135858 -0.04752053 ...  0.07725805  0.00092021\n",
      "                                        0.03845027]                                                   \n",
      "                                      [-0.00172248  0.06309029 -0.04522962 ...  0.016793   -0.06926603\n",
      "                                       -0.03418857]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.04648906  0.01777251 -0.02645412 ...  0.00641268 -0.00906336\n",
      "                                       -0.00921289]                                                   \n",
      "                                      [ 0.02152582  0.03917762  0.04654442 ... -0.00622685 -0.01066629\n",
      "                                        0.02790761]                                                   \n",
      "                                      [ 0.05676785 -0.22051696 -0.13353189 ...  0.03653477  0.05505624\n",
      "                                        0.05504014]]                                                  \n",
      "Epoch 34, loss: 11.792056\n",
      "== W == -1.3519696076040013\n",
      "enter of the function =  [[ 0.0494422   0.03344245  0.01889661 ... -0.03933157  0.04758777 [5 9 1 ... 5 7 1]\n",
      "                            0.02689976]                                                                     \n",
      "                          [ 0.01539731  0.01189371 -0.01628246 ...  0.03345154 -0.00285078                  \n",
      "                            0.01059773]                                                                     \n",
      "                          [ 0.014721   -0.04276893  0.01550324 ... -0.06551454 -0.00221476                  \n",
      "                            0.0324949 ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.04537347 -0.08539237 -0.02298419 ...  0.06603329 -0.05633753                  \n",
      "                           -0.04474527]                                                                     \n",
      "                          [ 0.03445421 -0.00453459 -0.03637644 ... -0.01235316 -0.0187212                   \n",
      "                           -0.02382034]                                                                     \n",
      "                          [-0.02946566 -0.00751983 -0.03508122 ... -0.12669464  0.06542459                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.1730667 ]]                                                                    \n",
      "soft max = [[1.16460572e-05 1.14612059e-05 1.12956997e-05 ... 1.06567545e-05\n",
      "             1.16244804e-05 1.13864636e-05]                                 \n",
      "            [1.12562417e-05 1.12168734e-05 1.09052358e-05 ... 1.14613101e-05\n",
      "             1.10526996e-05 1.12023459e-05]                                 \n",
      "            [1.12486316e-05 1.06201863e-05 1.12574341e-05 ... 1.03813501e-05\n",
      "             1.10597316e-05 1.14503509e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.05925615e-05 1.01770289e-05 1.08323962e-05 ... 1.18408897e-05\n",
      "             1.04770584e-05 1.05992179e-05]                                 \n",
      "            [1.14728078e-05 1.10341046e-05 1.06882931e-05 ... 1.09481701e-05\n",
      "             1.08786732e-05 1.08233425e-05]                                 \n",
      "            [1.07624134e-05 1.10012143e-05 1.07021458e-05 ... 9.76525657e-06\n",
      "             1.18336844e-05 1.31785721e-05]]                                \n",
      "log =  102782.95096817992\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.420327885353325 [[ 1.16460572e-05  1.14612059e-05  1.12956997e-05 ...  1.06567545e-05 \n",
      "                                                   1.16244804e-05  1.13864636e-05]                                    \n",
      "                                                 [ 1.12562417e-05  1.12168734e-05  1.09052358e-05 ...  1.14613101e-05 \n",
      "                                                   1.10526996e-05 -9.99087652e-05]                                    \n",
      "                                                 [ 1.12486316e-05 -1.00490925e-04  1.12574341e-05 ...  1.03813501e-05 \n",
      "                                                   1.10597316e-05  1.14503509e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.05925615e-05  1.01770289e-05  1.08323962e-05 ...  1.18408897e-05 \n",
      "                                                   1.04770584e-05  1.05992179e-05]                                    \n",
      "                                                 [ 1.14728078e-05  1.10341046e-05  1.06882931e-05 ... -1.00162941e-04 \n",
      "                                                   1.08786732e-05  1.08233425e-05]                                    \n",
      "                                                 [ 1.07624134e-05 -1.00109897e-04  1.07021458e-05 ...  9.76525657e-06 \n",
      "                                                   1.18336844e-05  1.31785721e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.420327885353325 [[ 1.16460572e-05  1.14612059e-05  1.12956997e-05 ...  1.06567545e-05 [[-7.64651331e-04 -2.65886670e-04  2.03393349e-05 ...  1.31912904e-03 \n",
      "                                                              1.16244804e-05  1.13864636e-05]                                      -1.76792406e-03 -5.94113404e-04]                                    \n",
      "                                                            [ 1.12562417e-05  1.12168734e-05  1.09052358e-05 ...  1.14613101e-05  [-1.31850843e-03  4.67133562e-04  5.44368248e-04 ...  1.03471075e-03 \n",
      "                                                              1.10526996e-05 -9.99087652e-05]                                      -2.06589690e-03 -6.55056036e-04]                                    \n",
      "                                                            [ 1.12486316e-05 -1.00490925e-04  1.12574341e-05 ...  1.03813501e-05  [-1.63478373e-03  1.04749484e-03  1.11556773e-03 ...  9.51290575e-04 \n",
      "                                                              1.10597316e-05  1.14503509e-05]                                      -2.14926905e-03 -1.26816211e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.05925615e-05  1.01770289e-05  1.08323962e-05 ...  1.18408897e-05  [-5.58966315e-04  7.05664730e-04 -7.46657554e-04 ...  1.99522289e-03 \n",
      "                                                              1.04770584e-05  1.05992179e-05]                                      -1.47845699e-03 -1.35898056e-03]                                    \n",
      "                                                            [ 1.14728078e-05  1.10341046e-05  1.06882931e-05 ... -1.00162941e-04  [ 7.66451503e-05 -4.61280214e-04 -9.72909834e-04 ...  2.23504098e-03 \n",
      "                                                              1.08786732e-05  1.08233425e-05]                                      -9.50518052e-04 -1.32648972e-03]                                    \n",
      "                                                            [ 1.07624134e-05 -1.00109897e-04  1.07021458e-05 ...  9.76525657e-06  [ 3.40983480e-02 -9.90251977e-02 -4.96701955e-02 ...  2.36927748e-02 \n",
      "                                                              1.18336844e-05  1.31785721e-05]]                                      3.37681422e-02  3.82736007e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.3795088596499474 [[-0.01191628 -0.02259534 -0.03564671 ... -0.01285708 -0.01805499\n",
      "                                       0.00488766]                                                   \n",
      "                                     [ 0.00716102 -0.01146748 -0.04799022 ...  0.07804072  0.00090893\n",
      "                                       0.0388284 ]                                                   \n",
      "                                     [-0.00175594  0.06373168 -0.04567069 ...  0.01697017 -0.06998   \n",
      "                                      -0.03454294]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.04695944  0.01795727 -0.02672605 ...  0.00649649 -0.00916862\n",
      "                                      -0.00931843]                                                   \n",
      "                                     [ 0.02174194  0.03956477  0.04700022 ... -0.00626705 -0.0107823 \n",
      "                                       0.02817361]                                                   \n",
      "                                     [ 0.05767651 -0.22371197 -0.13536365 ...  0.03713694  0.05594444\n",
      "                                       0.05597321]]                                                  \n",
      "Epoch 35, loss: 11.799837\n",
      "== W == -1.3745557516708267\n",
      "enter of the function =  [[ 0.02498471 -0.04117329 -0.04078592 ...  0.05491719 -0.00579455 [4 1 5 ... 6 2 6]\n",
      "                            0.0108257 ]                                                                     \n",
      "                          [-0.05846246 -0.02885179  0.01431688 ...  0.12954493 -0.0320606                   \n",
      "                           -0.06867305]                                                                     \n",
      "                          [ 0.01003159 -0.05200917  0.01429876 ... -0.01217058  0.04116237                  \n",
      "                           -0.02408014]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.00105033 -0.00897285  0.02357053 ... -0.10448095  0.03812122                  \n",
      "                            0.07671175]                                                                     \n",
      "                          [ 0.00018626 -0.08337302 -0.1129061  ...  0.11126799 -0.00251614                  \n",
      "                           -0.08097195]                                                                     \n",
      "                          [ 0.00408686  0.00426063 -0.01019124 ... -0.06429206  0.05467362                  \n",
      "                            0.05572664]]                                                                    \n",
      "soft max = [[1.13632606e-05 1.06358183e-05 1.06399391e-05 ... 1.17085328e-05\n",
      "             1.10188356e-05 1.12035017e-05]                                 \n",
      "            [1.04535144e-05 1.07676783e-05 1.12426836e-05 ... 1.26157447e-05\n",
      "             1.07331822e-05 1.03473208e-05]                                 \n",
      "            [1.11946085e-05 1.05211920e-05 1.12424798e-05 ... 1.09488027e-05\n",
      "             1.15485865e-05 1.08191806e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.10712356e-05 1.09838700e-05 1.13472022e-05 ... 9.98336025e-06\n",
      "             1.15135189e-05 1.19665162e-05]                                 \n",
      "            [1.10849346e-05 1.01963281e-05 9.89960226e-06 ... 1.23872619e-05\n",
      "             1.10550191e-05 1.02208396e-05]                                 \n",
      "            [1.11282569e-05 1.11301908e-05 1.09704956e-05 ... 1.03927518e-05\n",
      "             1.17056813e-05 1.17180141e-05]]                                \n",
      "log =  102785.44534613563\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.420605038459515 [[ 1.13632606e-05  1.06358183e-05  1.06399391e-05 ...  1.17085328e-05 \n",
      "                                                   1.10188356e-05  1.12035017e-05]                                    \n",
      "                                                 [ 1.04535144e-05 -1.00343433e-04  1.12426836e-05 ...  1.26157447e-05 \n",
      "                                                   1.07331822e-05  1.03473208e-05]                                    \n",
      "                                                 [ 1.11946085e-05  1.05211920e-05  1.12424798e-05 ...  1.09488027e-05 \n",
      "                                                   1.15485865e-05  1.08191806e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.10712356e-05  1.09838700e-05  1.13472022e-05 ...  9.98336025e-06 \n",
      "                                                   1.15135189e-05  1.19665162e-05]                                    \n",
      "                                                 [ 1.10849346e-05  1.01963281e-05 -1.01211509e-04 ...  1.23872619e-05 \n",
      "                                                   1.10550191e-05  1.02208396e-05]                                    \n",
      "                                                 [ 1.11282569e-05  1.11301908e-05  1.09704956e-05 ...  1.03927518e-05 \n",
      "                                                   1.17056813e-05  1.17180141e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.420605038459515 [[ 1.13632606e-05  1.06358183e-05  1.06399391e-05 ...  1.17085328e-05 [[-7.75778622e-04 -2.67252737e-04  1.31938766e-05 ...  1.34413208e-03 \n",
      "                                                              1.10188356e-05  1.12035017e-05]                                      -1.78580491e-03 -6.11579910e-04]                                    \n",
      "                                                            [ 1.04535144e-05 -1.00343433e-04  1.12426836e-05 ...  1.26157447e-05  [-1.32977377e-03  4.65929881e-04  5.37244649e-04 ...  1.06078559e-03 \n",
      "                                                              1.07331822e-05  1.03473208e-05]                                      -2.08458158e-03 -6.73432886e-04]                                    \n",
      "                                                            [ 1.11946085e-05  1.05211920e-05  1.12424798e-05 ...  1.09488027e-05  [-1.64584177e-03  1.04607574e-03  1.10827923e-03 ...  9.78907829e-04 \n",
      "                                                              1.15485865e-05  1.08191806e-05]                                      -2.16817598e-03 -1.28768744e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.10712356e-05  1.09838700e-05  1.13472022e-05 ...  9.98336025e-06  [-5.68818754e-04  7.07087810e-04 -7.55267668e-04 ...  2.02299084e-03 \n",
      "                                                              1.15135189e-05  1.19665162e-05]                                      -1.49483563e-03 -1.37719120e-03]                                    \n",
      "                                                            [ 1.10849346e-05  1.01963281e-05 -1.01211509e-04 ...  1.23872619e-05  [ 6.71300195e-05 -4.60368593e-04 -9.81436475e-04 ...  2.26403238e-03 \n",
      "                                                              1.10550191e-05  1.02208396e-05]                                      -9.66924252e-04 -1.34566496e-03]                                    \n",
      "                                                            [ 1.11282569e-05  1.11301908e-05  1.09704956e-05 ...  1.03927518e-05  [ 3.40984261e-02 -9.90670048e-02 -4.96969507e-02 ...  2.37036555e-02 \n",
      "                                                              1.17056813e-05  1.17180141e-05]]                                      3.37726617e-02  3.82804666e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.38717071395675795 [[-0.01204309 -0.02282395 -0.03600298 ... -0.01297246 -0.01825322\n",
      "                                        0.0049306 ]                                                   \n",
      "                                      [ 0.00721944 -0.01157748 -0.04846468 ...  0.07883147  0.00089736\n",
      "                                        0.03921014]                                                   \n",
      "                                      [-0.00178985  0.06437947 -0.04611624 ...  0.01714938 -0.07070129\n",
      "                                       -0.03490106]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.04743463  0.0181439  -0.02700077 ...  0.0065814  -0.00927509\n",
      "                                       -0.0094252 ]                                                   \n",
      "                                      [ 0.02196013  0.03995581  0.04746049 ... -0.00630737 -0.01089963\n",
      "                                        0.02844208]                                                   \n",
      "                                      [ 0.05859426 -0.22693934 -0.13721399 ...  0.03774524  0.05684156\n",
      "                                        0.05691568]]                                                  \n",
      "Epoch 36, loss: 11.807776\n",
      "== W == -1.3974938220551603\n",
      "enter of the function =  [[-0.04089784 -0.0049287  -0.03468659 ... -0.07368994  0.04472776 [0 0 1 ... 9 8 3]\n",
      "                            0.12645483]                                                                     \n",
      "                          [-0.03379103 -0.14984165 -0.0618857  ...  0.05669496 -0.03150017                  \n",
      "                           -0.01135087]                                                                     \n",
      "                          [ 0.02245383 -0.03638079 -0.04869133 ...  0.05263084  0.00876178                  \n",
      "                           -0.01408404]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.01340076 -0.05154904  0.01003425 ... -0.03714157  0.02037486                  \n",
      "                            0.06925483]                                                                     \n",
      "                          [ 0.05584552 -0.03350042  0.04441183 ... -0.04998347  0.09358307                  \n",
      "                            0.08172634]                                                                     \n",
      "                          [ 0.02438884 -0.04596833 -0.01922568 ... -0.07036524  0.02806639                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.02975977]]                                                                    \n",
      "soft max = [[1.06373694e-05 1.10269508e-05 1.07036463e-05 ... 1.02942049e-05\n",
      "             1.15883329e-05 1.25751907e-05]                                 \n",
      "            [1.07132364e-05 9.53938866e-06 1.04164403e-05 ... 1.17278460e-05\n",
      "             1.07378071e-05 1.09563608e-05]                                 \n",
      "            [1.13330687e-05 1.06855276e-05 1.05547893e-05 ... 1.16802794e-05\n",
      "             1.11789533e-05 1.09264561e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.12309326e-05 1.05246699e-05 1.11931871e-05 ... 1.06774013e-05\n",
      "             1.13095320e-05 1.18760752e-05]                                 \n",
      "            [1.17178881e-05 1.07163502e-05 1.15846724e-05 ... 1.05411599e-05\n",
      "             1.21685423e-05 1.20251151e-05]                                 \n",
      "            [1.13550195e-05 1.05835692e-05 1.08704203e-05 ... 1.03284871e-05\n",
      "             1.13968550e-05 1.14161706e-05]]                                \n",
      "log =  102787.99924599897\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.420888805110996 [[-1.00473742e-04  1.10269508e-05  1.07036463e-05 ...  1.02942049e-05 \n",
      "                                                   1.15883329e-05  1.25751907e-05]                                    \n",
      "                                                 [-1.00397875e-04  9.53938866e-06  1.04164403e-05 ...  1.17278460e-05 \n",
      "                                                   1.07378071e-05  1.09563608e-05]                                    \n",
      "                                                 [ 1.13330687e-05 -1.00425584e-04  1.05547893e-05 ...  1.16802794e-05 \n",
      "                                                   1.11789533e-05  1.09264561e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.12309326e-05  1.05246699e-05  1.11931871e-05 ...  1.06774013e-05 \n",
      "                                                   1.13095320e-05 -9.92350359e-05]                                    \n",
      "                                                 [ 1.17178881e-05  1.07163502e-05  1.15846724e-05 ...  1.05411599e-05 \n",
      "                                                  -9.89425688e-05  1.20251151e-05]                                    \n",
      "                                                 [ 1.13550195e-05  1.05835692e-05  1.08704203e-05 ...  1.03284871e-05 \n",
      "                                                   1.13968550e-05  1.14161706e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.420888805110996 [[-1.00473742e-04  1.10269508e-05  1.07036463e-05 ...  1.02942049e-05 [[-7.87082412e-04 -2.68639134e-04  5.92715226e-06 ...  1.36961253e-03 \n",
      "                                                              1.15883329e-05  1.25751907e-05]                                      -1.80396767e-03 -6.29334798e-04]                                    \n",
      "                                                            [-1.00397875e-04  9.53938866e-06  1.04164403e-05 ...  1.17278460e-05  [-1.34121893e-03  4.64706954e-04  5.29998173e-04 ...  1.08735743e-03 \n",
      "                                                              1.07378071e-05  1.09563608e-05]                                      -2.10355907e-03 -6.92110900e-04]                                    \n",
      "                                                            [ 1.13330687e-05 -1.00425584e-04  1.05547893e-05 ...  1.16802794e-05  [-1.65707961e-03  1.04463505e-03  1.10086481e-03 ...  1.00704634e-03 \n",
      "                                                              1.11789533e-05  1.09264561e-05]                                      -2.18738117e-03 -1.30752952e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.12309326e-05  1.05246699e-05  1.11931871e-05 ...  1.06774013e-05  [-5.78838206e-04  7.08515109e-04 -7.64014632e-04 ...  2.05128075e-03 \n",
      "                                                              1.13095320e-05 -9.92350359e-05]                                      -1.51148629e-03 -1.39570191e-03]                                    \n",
      "                                                            [ 1.17178881e-05  1.07163502e-05  1.15846724e-05 ...  1.05411599e-05  [ 5.74502585e-05 -4.59457660e-04 -9.90099874e-04 ...  2.29356363e-03 \n",
      "                                                             -9.89425688e-05  1.20251151e-05]                                      -9.83604201e-04 -1.36515245e-03]                                    \n",
      "                                                            [ 1.13550195e-05  1.05835692e-05  1.08704203e-05 ...  1.03284871e-05  [ 3.40982247e-02 -9.91095439e-02 -4.97242504e-02 ...  2.37147551e-02 \n",
      "                                                              1.13968550e-05  1.14161706e-05]]                                      3.37770617e-02  3.82872791e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.39498741893181627 [[-0.01217128 -0.02305487 -0.03636288 ... -0.01308874 -0.01845361\n",
      "                                        0.00497379]                                                   \n",
      "                                      [ 0.00727834 -0.0116886  -0.04894396 ...  0.07963039  0.00088549\n",
      "                                        0.0395955 ]                                                   \n",
      "                                      [-0.00182421  0.06503373 -0.04656632 ...  0.01733067 -0.07142998\n",
      "                                       -0.03526294]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.04791466  0.01833241 -0.02727834 ...  0.00666745 -0.00938279\n",
      "                                       -0.00953323]                                                   \n",
      "                                      [ 0.0221804   0.04035076  0.04792528 ... -0.0063478  -0.01101829\n",
      "                                        0.02871304]                                                   \n",
      "                                      [ 0.05952119 -0.2301994  -0.1390831  ...  0.03835973  0.05774771\n",
      "                                        0.05786764]]                                                  \n",
      "Epoch 37, loss: 11.815876\n",
      "== W == -1.4207883480018981\n",
      "enter of the function =  [[ 0.03998688 -0.06909614 -0.03133042 ...  0.08365043 -0.04193835 [4 5 2 ... 5 1 0]\n",
      "                           -0.05436577]                                                                     \n",
      "                          [ 0.02537984  0.00436059  0.02997678 ... -0.04607159  0.01456642                  \n",
      "                            0.03433108]                                                                     \n",
      "                          [ 0.01410138 -0.01097527  0.02790691 ... -0.07840069  0.07334752                  \n",
      "                            0.07972498]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.0179316  -0.0542996  -0.09530931 ...  0.10200403 -0.02640465                  \n",
      "                           -0.05628091]                                                                     \n",
      "                          [-0.01877502 -0.02771711 -0.08737604 ...  0.14947028 -0.02382801                  \n",
      "                           -0.07975291]                                                                     \n",
      "                          [-0.09512852  0.01652915 -0.07549109 ...  0.13215173 -0.04915122                  \n",
      "                           -0.07368673]]                                                                    \n",
      "soft max = [[1.15319713e-05 1.03402110e-05 1.07381840e-05 ... 1.20466527e-05\n",
      "             1.06248762e-05 1.04936535e-05]                                 \n",
      "            [1.13647476e-05 1.11283622e-05 1.14171109e-05 ... 1.05810517e-05\n",
      "             1.12425179e-05 1.14669328e-05]                                 \n",
      "            [1.12372909e-05 1.09590011e-05 1.13935035e-05 ... 1.02444462e-05\n",
      "             1.19231743e-05 1.19994569e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.08830313e-05 1.04943479e-05 1.00726830e-05 ... 1.22697937e-05\n",
      "             1.07912083e-05 1.04735759e-05]                                 \n",
      "            [1.08738561e-05 1.07770546e-05 1.01529101e-05 ... 1.28662382e-05\n",
      "             1.08190492e-05 1.02306028e-05]                                 \n",
      "            [1.00745041e-05 1.12646056e-05 1.02742968e-05 ... 1.26453321e-05\n",
      "             1.05485160e-05 1.02928521e-05]]                                \n",
      "log =  102790.61446873115\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.421179385414572 [[ 1.15319713e-05  1.03402110e-05  1.07381840e-05 ...  1.20466527e-05 \n",
      "                                                   1.06248762e-05  1.04936535e-05]                                    \n",
      "                                                 [ 1.13647476e-05  1.11283622e-05  1.14171109e-05 ...  1.05810517e-05 \n",
      "                                                   1.12425179e-05  1.14669328e-05]                                    \n",
      "                                                 [ 1.12372909e-05  1.09590011e-05 -9.97176076e-05 ...  1.02444462e-05 \n",
      "                                                   1.19231743e-05  1.19994569e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.08830313e-05  1.04943479e-05  1.00726830e-05 ...  1.22697937e-05 \n",
      "                                                   1.07912083e-05  1.04735759e-05]                                    \n",
      "                                                 [ 1.08738561e-05 -1.00334056e-04  1.01529101e-05 ...  1.28662382e-05 \n",
      "                                                   1.08190492e-05  1.02306028e-05]                                    \n",
      "                                                 [-1.01036607e-04  1.12646056e-05  1.02742968e-05 ...  1.26453321e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   1.05485160e-05  1.02928521e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.421179385414572 [[ 1.15319713e-05  1.03402110e-05  1.07381840e-05 ...  1.20466527e-05 [[-7.98565290e-04 -2.70046129e-04 -1.46262536e-06 ...  1.39558015e-03 \n",
      "                                                              1.06248762e-05  1.04936535e-05]                                      -1.82241644e-03 -6.47382417e-04]                                    \n",
      "                                                            [ 1.13647476e-05  1.11283622e-05  1.14171109e-05 ...  1.05810517e-05  [-1.35284656e-03  4.63464513e-04  5.22626988e-04 ...  1.11443639e-03 \n",
      "                                                              1.12425179e-05  1.14669328e-05]                                      -2.12283361e-03 -7.11094590e-04]                                    \n",
      "                                                            [ 1.12372909e-05  1.09590011e-05 -9.97176076e-05 ...  1.02444462e-05  [-1.66849994e-03  1.04317248e-03  1.09332258e-03 ...  1.03571669e-03 \n",
      "                                                              1.19231743e-05  1.19994569e-05]                                      -2.20688896e-03 -1.32769306e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.08830313e-05  1.04943479e-05  1.00726830e-05 ...  1.22697937e-05  [-5.89027253e-04  7.09946480e-04 -7.72900354e-04 ...  2.08010320e-03 \n",
      "                                                              1.07912083e-05  1.04735759e-05]                                      -1.52841309e-03 -1.41451721e-03]                                    \n",
      "                                                            [ 1.08738561e-05 -1.00334056e-04  1.01529101e-05 ...  1.28662382e-05  [ 4.76032925e-05 -4.58547597e-04 -9.98901947e-04 ...  2.32364564e-03 \n",
      "                                                              1.08190492e-05  1.02306028e-05]                                      -1.00056206e-03 -1.38495682e-03]                                    \n",
      "                                                            [-1.01036607e-04  1.12646056e-05  1.02742968e-05 ...  1.26453321e-05  [ 3.40977305e-02 -9.91528336e-02 -4.97521105e-02 ...  2.37260801e-02 \n",
      "                                                              1.05485160e-05  1.02928521e-05]]                                      3.37813346e-02  3.82940327e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.4029621108925 [[-0.01230087 -0.0232881  -0.03672645 ... -0.01320593 -0.01865619\n",
      "                                    0.00501723]                                                   \n",
      "                                  [ 0.00733771 -0.01180084 -0.04942809 ...  0.08043757  0.00087331\n",
      "                                    0.03998454]                                                   \n",
      "                                  [-0.00185902  0.06569451 -0.04702098 ...  0.01751404 -0.07216616\n",
      "                                   -0.03562865]                                                   \n",
      "                                  ...                                                             \n",
      "                                  [-0.0483996   0.01852282 -0.02755876 ...  0.00675463 -0.00949173\n",
      "                                   -0.00964252]                                                   \n",
      "                                  [ 0.02240278  0.04074967  0.04839464 ... -0.00638835 -0.01113831\n",
      "                                    0.02898652]                                                   \n",
      "                                  [ 0.06045738 -0.23349249 -0.14097117 ...  0.03898047  0.05866295\n",
      "                                    0.05882919]]                                                  \n",
      "Epoch 38, loss: 11.824141\n",
      "== W == -1.444443873915706\n",
      "enter of the function =  [[-0.1266546  -0.04948054 -0.13738475 ...  0.15489408 -0.13340339 [2 5 8 ... 5 2 4]\n",
      "                           -0.11980712]                                                                     \n",
      "                          [-0.04449873 -0.03553575 -0.058212   ...  0.13451231 -0.09668996                  \n",
      "                           -0.10046184]                                                                     \n",
      "                          [ 0.05569433 -0.0242892  -0.01950447 ...  0.00210197  0.05726865                  \n",
      "                            0.01554943]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.07231029 -0.01727214  0.0365458  ... -0.11806094  0.13184307                  \n",
      "                            0.11472026]                                                                     \n",
      "                          [ 0.02397434 -0.03324572  0.03745617 ... -0.03676926  0.06061812                  \n",
      "                            0.04933357]                                                                     \n",
      "                          [ 0.03327135  0.00362933  0.03465306 ... -0.00757517  0.06732406                  \n",
      "                            0.07189871]]                                                                    \n",
      "soft max = [[9.76048391e-06 1.05435683e-05 9.65631228e-06 ... 1.29344030e-05\n",
      "             9.69483420e-06 9.82754793e-06]                                 \n",
      "            [1.05962255e-05 1.06916261e-05 1.04519083e-05 ... 1.26734453e-05\n",
      "             1.00573792e-05 1.00195155e-05]                                 \n",
      "            [1.17129012e-05 1.08125487e-05 1.08644078e-05 ... 1.11017032e-05\n",
      "             1.17313557e-05 1.12520013e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.19091483e-05 1.08886878e-05 1.14907501e-05 ... 9.84472363e-06\n",
      "             1.26396620e-05 1.24250778e-05]                                 \n",
      "            [1.13471989e-05 1.07161383e-05 1.15012158e-05 ... 1.06784459e-05\n",
      "             1.17707153e-05 1.16386347e-05]                                 \n",
      "            [1.14531858e-05 1.11186725e-05 1.14690217e-05 ... 1.09947887e-05\n",
      "             1.18499144e-05 1.19042477e-05]]                                \n",
      "log =  102793.2928779101\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.421476986434456 [[ 9.76048391e-06  1.05435683e-05 -1.01454799e-04 ...  1.29344030e-05 \n",
      "                                                   9.69483420e-06  9.82754793e-06]                                    \n",
      "                                                 [ 1.05962255e-05  1.06916261e-05  1.04519083e-05 ...  1.26734453e-05 \n",
      "                                                   1.00573792e-05  1.00195155e-05]                                    \n",
      "                                                 [ 1.17129012e-05  1.08125487e-05  1.08644078e-05 ...  1.11017032e-05 \n",
      "                                                  -9.93797554e-05  1.12520013e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.19091483e-05  1.08886878e-05  1.14907501e-05 ...  9.84472363e-06 \n",
      "                                                   1.26396620e-05  1.24250778e-05]                                    \n",
      "                                                 [ 1.13471989e-05  1.07161383e-05 -9.96098954e-05 ...  1.06784459e-05 \n",
      "                                                   1.17707153e-05  1.16386347e-05]                                    \n",
      "                                                 [ 1.14531858e-05  1.11186725e-05  1.14690217e-05 ...  1.09947887e-05 \n",
      "                                                   1.18499144e-05  1.19042477e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.421476986434456 [[ 9.76048391e-06  1.05435683e-05 -1.01454799e-04 ...  1.29344030e-05 [[-8.10229874e-04 -2.71473990e-04 -8.97725953e-06 ...  1.42204488e-03 \n",
      "                                                              9.69483420e-06  9.82754793e-06]                                      -1.84115540e-03 -6.65727167e-04]                                    \n",
      "                                                            [ 1.05962255e-05  1.06916261e-05  1.04519083e-05 ...  1.26734453e-05  [-1.36465934e-03  4.62202291e-04  5.15129251e-04 ...  1.14203285e-03 \n",
      "                                                              1.00573792e-05  1.00195155e-05]                                      -2.14240948e-03 -7.30388518e-04]                                    \n",
      "                                                            [ 1.17129012e-05  1.08125487e-05  1.08644078e-05 ...  1.11017032e-05  [-1.68010548e-03  1.04168775e-03  1.08565065e-03 ...  1.06492969e-03 \n",
      "                                                             -9.93797554e-05  1.12520013e-05]                                      -2.22670374e-03 -1.34818282e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.19091483e-05  1.08886878e-05  1.14907501e-05 ...  9.84472363e-06  [-5.99388507e-04  7.11381765e-04 -7.81926760e-04 ...  2.10946907e-03 \n",
      "                                                              1.26396620e-05  1.24250778e-05]                                      -1.54562023e-03 -1.43364165e-03]                                    \n",
      "                                                            [ 1.13471989e-05  1.07161383e-05 -9.96098954e-05 ...  1.06784459e-05  [ 3.75865181e-05 -4.57638591e-04 -1.00784463e-03 ...  2.35428958e-03 \n",
      "                                                              1.17707153e-05  1.16386347e-05]                                      -1.01780203e-03 -1.40508279e-03]                                    \n",
      "                                                            [ 1.14531858e-05  1.11186725e-05  1.14690217e-05 ...  1.09947887e-05  [ 3.40969297e-02 -9.91968933e-02 -4.97805470e-02 ...  2.37376373e-02 \n",
      "                                                              1.18499144e-05  1.19042477e-05]]                                      3.37854722e-02  3.83007212e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.4110979899883779 [[-0.01243186 -0.02352368 -0.03709372 ... -0.01332404 -0.01886097\n",
      "                                       0.00506093]                                                   \n",
      "                                     [ 0.00739756 -0.01191421 -0.04991715 ...  0.08125309  0.00086082\n",
      "                                       0.04037727]                                                   \n",
      "                                     [-0.00189429  0.06636189 -0.04748025 ...  0.01769954 -0.07290989\n",
      "                                      -0.03599821]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.04888948  0.01871515 -0.02784208 ...  0.00684298 -0.00960194\n",
      "                                      -0.00975309]                                                   \n",
      "                                     [ 0.02262728  0.04115259  0.04886859 ... -0.00642899 -0.0112597 \n",
      "                                       0.02926254]                                                   \n",
      "                                     [ 0.06140293 -0.23681895 -0.1428784  ...  0.03960754  0.0595874 \n",
      "                                       0.05980042]]                                                  \n",
      "Epoch 39, loss: 11.832575\n",
      "== W == -1.468464956868365\n",
      "enter of the function =  [[ 0.01855487 -0.04521161  0.04825813 ... -0.1084702   0.01065334 [1 2 8 ... 7 0 2]\n",
      "                            0.08414824]                                                                     \n",
      "                          [ 0.00971708 -0.05487545 -0.06698235 ... -0.00246627  0.04213924                  \n",
      "                           -0.00544654]                                                                     \n",
      "                          [-0.06757752  0.09715539 -0.18174475 ...  0.2870173  -0.11292718                  \n",
      "                           -0.09417426]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.02333165  0.03958739 -0.03375062 ...  0.10179491 -0.09193532                  \n",
      "                           -0.07754119]                                                                     \n",
      "                          [ 0.01412342  0.00168752 -0.02172116 ... -0.01873003  0.02627995                  \n",
      "                            0.01373357]                                                                     \n",
      "                          [ 0.0468093   0.01067327 -0.04343527 ... -0.08012492  0.08998488                  \n",
      "                            0.07000619]]                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max = [[1.12842305e-05 1.05871367e-05 1.16244366e-05 ... 9.93815264e-06\n",
      "             1.11954192e-05 1.20492160e-05]                                 \n",
      "            [1.11849423e-05 1.04853171e-05 1.03591377e-05 ... 1.10494989e-05\n",
      "             1.15535251e-05 1.10166175e-05]                                 \n",
      "            [1.03529741e-05 1.22069656e-05 9.23597885e-06 ... 1.47592301e-05\n",
      "             9.89395714e-06 1.00812483e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.08213356e-05 1.15240799e-05 1.07091737e-05 ... 1.22637317e-05\n",
      "             1.01038449e-05 1.02503327e-05]                                 \n",
      "            [1.12343358e-05 1.10954918e-05 1.08387773e-05 ... 1.08712461e-05\n",
      "             1.13717397e-05 1.12299568e-05]                                 \n",
      "            [1.16076070e-05 1.11956423e-05 1.06059598e-05 ... 1.02238828e-05\n",
      "             1.21197486e-05 1.18800147e-05]]                                \n",
      "log =  102796.03640215046\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.421781822461163 [[ 1.12842305e-05 -1.00523974e-04  1.16244366e-05 ...  9.93815264e-06 \n",
      "                                                   1.11954192e-05  1.20492160e-05]                                    \n",
      "                                                 [ 1.11849423e-05  1.04853171e-05 -1.00751973e-04 ...  1.10494989e-05 \n",
      "                                                   1.15535251e-05  1.10166175e-05]                                    \n",
      "                                                 [ 1.03529741e-05  1.22069656e-05  9.23597885e-06 ...  1.47592301e-05 \n",
      "                                                  -1.01217154e-04  1.00812483e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.08213356e-05  1.15240799e-05  1.07091737e-05 ... -9.88473794e-05 \n",
      "                                                   1.01038449e-05  1.02503327e-05]                                    \n",
      "                                                 [-9.98767754e-05  1.10954918e-05  1.08387773e-05 ...  1.08712461e-05 \n",
      "                                                   1.13717397e-05  1.12299568e-05]                                    \n",
      "                                                 [ 1.16076070e-05  1.11956423e-05 -1.00505151e-04 ...  1.02238828e-05 \n",
      "                                                   1.21197486e-05  1.18800147e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.421781822461163 [[ 1.12842305e-05 -1.00523974e-04  1.16244366e-05 ...  9.93815264e-06 [[-8.22078808e-04 -2.72922988e-04 -1.66185689e-05 ...  1.44901692e-03 \n",
      "                                                              1.11954192e-05  1.20492160e-05]                                      -1.86018874e-03 -6.84373494e-04]                                    \n",
      "                                                            [ 1.11849423e-05  1.04853171e-05 -1.00751973e-04 ...  1.10494989e-05  [-1.37665997e-03  4.60920016e-04  5.07503100e-04 ...  1.17015741e-03 \n",
      "                                                              1.15535251e-05  1.10166175e-05]                                      -2.16229101e-03 -7.49997294e-04]                                    \n",
      "                                                            [ 1.03529741e-05  1.22069656e-05  9.23597885e-06 ...  1.47592301e-05  [-1.69189894e-03  1.04018057e-03  1.07784712e-03 ...  1.09469639e-03 \n",
      "                                                             -1.01217154e-04  1.00812483e-05]                                      -2.24682994e-03 -1.36900363e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.08213356e-05  1.15240799e-05  1.07091737e-05 ... -9.88473794e-05  [-6.09924606e-04  7.12820805e-04 -7.91095787e-04 ...  2.13938945e-03 \n",
      "                                                              1.01038449e-05  1.02503327e-05]                                      -1.56311191e-03 -1.45307983e-03]                                    \n",
      "                                                            [-9.98767754e-05  1.10954918e-05  1.08387773e-05 ...  1.08712461e-05  [ 2.73973032e-05 -4.56730834e-04 -1.01692986e-03 ...  2.38550686e-03 \n",
      "                                                              1.13717397e-05  1.12299568e-05]                                      -1.03532837e-03 -1.42553510e-03]                                    \n",
      "                                                            [ 1.16076070e-05  1.11956423e-05 -1.00505151e-04 ...  1.02238828e-05  [ 3.40958082e-02 -9.92417429e-02 -4.98095769e-02 ...  2.37494337e-02 \n",
      "                                                              1.21197486e-05  1.18800147e-05]]                                      3.37894660e-02  3.83073386e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.41939832151351863 [[-0.01256428 -0.02376163 -0.03746475 ... -0.01344306 -0.019068  \n",
      "                                        0.00510488]                                                   \n",
      "                                      [ 0.00745789 -0.01202873 -0.05041117 ...  0.08207704  0.000848  \n",
      "                                        0.04077374]                                                   \n",
      "                                      [-0.00193004  0.06703593 -0.0479442  ...  0.01788719 -0.07366125\n",
      "                                       -0.03637168]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.04938437  0.01890941 -0.02812832 ...  0.00693251 -0.00971341\n",
      "                                       -0.00986496]                                                   \n",
      "                                      [ 0.02285393  0.04155954  0.0493472  ... -0.00646974 -0.01138248\n",
      "                                        0.02954111]                                                   \n",
      "                                      [ 0.06235793 -0.24017911 -0.14480499 ...  0.04024099  0.06052112\n",
      "                                        0.06078143]]                                                  \n",
      "Epoch 40, loss: 11.841180\n",
      "== W == -1.4928561639296458\n",
      "enter of the function =  [[ 1.52159360e-02 -5.80764258e-02 -1.07700188e-01 ...  4.66482108e-03 [1 0 3 ... 4 1 1]\n",
      "                            1.45058377e-02  2.11040810e-02]                                                     \n",
      "                          [ 2.08096923e-02 -3.50488885e-03  1.07831096e-02 ... -5.12863958e-02                  \n",
      "                            4.97640828e-02  3.63073171e-02]                                                     \n",
      "                          [ 4.24774721e-05 -4.41819801e-02 -8.79829133e-02 ...  3.77541510e-02                  \n",
      "                           -4.89949178e-02  2.73866246e-03]                                                     \n",
      "                          ...                                                                                   \n",
      "                          [ 1.43737040e-02 -3.01560920e-03 -5.57387145e-02 ...  4.65096025e-02                  \n",
      "                           -6.85406846e-03 -3.54505038e-02]                                                     \n",
      "                          [ 2.19312320e-02 -2.66051666e-02  3.23346467e-02 ... -9.41288650e-02                  \n",
      "                            9.12516086e-03  5.00981154e-02]                                                     \n",
      "                          [ 4.86006299e-02  7.92726489e-03  2.38166004e-02 ... -1.35315708e-01                  \n",
      "                            3.76942922e-02  9.00090778e-02]]                                                    \n",
      "soft max = [[1.12449202e-05 1.04502315e-05 9.94430838e-06 ... 1.11268975e-05\n",
      "             1.12369380e-05 1.13113272e-05]                                 \n",
      "            [1.13079978e-05 1.10363643e-05 1.11951837e-05 ... 1.05214303e-05\n",
      "             1.16402001e-05 1.14846099e-05]                                 \n",
      "            [1.10755838e-05 1.05964451e-05 1.01423288e-05 ... 1.15012383e-05\n",
      "             1.05455676e-05 1.11054860e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.12354534e-05 1.10417655e-05 1.04746897e-05 ... 1.16023789e-05\n",
      "             1.09994634e-05 1.06893728e-05]                                 \n",
      "            [1.13206873e-05 1.07843433e-05 1.14390759e-05 ... 1.00801857e-05\n",
      "             1.11766381e-05 1.16440890e-05]                                 \n",
      "            [1.16266652e-05 1.11632577e-05 1.13420511e-05 ... 9.67344832e-06\n",
      "             1.15005498e-05 1.21182143e-05]]                                \n",
      "log =  102798.84703762936\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.42209411529215 [[ 1.12449202e-05 -1.00660880e-04  9.94430838e-06 ...  1.11268975e-05 \n",
      "                                                  1.12369380e-05  1.13113272e-05]                                    \n",
      "                                                [-9.98031133e-05  1.10363643e-05  1.11951837e-05 ...  1.05214303e-05 \n",
      "                                                  1.16402001e-05  1.14846099e-05]                                    \n",
      "                                                [ 1.10755838e-05  1.05964451e-05  1.01423288e-05 ...  1.15012383e-05 \n",
      "                                                  1.05455676e-05  1.11054860e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [ 1.12354534e-05  1.10417655e-05  1.04746897e-05 ...  1.16023789e-05 \n",
      "                                                  1.09994634e-05  1.06893728e-05]                                    \n",
      "                                                [ 1.13206873e-05 -1.00326768e-04  1.14390759e-05 ...  1.00801857e-05 \n",
      "                                                  1.11766381e-05  1.16440890e-05]                                    \n",
      "                                                [ 1.16266652e-05 -9.99478535e-05  1.13420511e-05 ...  9.67344832e-06 \n",
      "                                                  1.15005498e-05  1.21182143e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.42209411529215 [[ 1.12449202e-05 -1.00660880e-04  9.94430838e-06 ...  1.11268975e-05 [[-8.34114760e-04 -2.74393398e-04 -2.43883873e-05 ...  1.47650669e-03 \n",
      "                                                             1.12369380e-05  1.13113272e-05]                                      -1.87952071e-03 -7.03325892e-04]                                    \n",
      "                                                           [-9.98031133e-05  1.10363643e-05  1.11951837e-05 ...  1.05214303e-05  [-1.38885117e-03  4.59617415e-04  4.99746658e-04 ...  1.19882095e-03 \n",
      "                                                             1.16402001e-05  1.14846099e-05]                                      -2.18248258e-03 -7.69925578e-04]                                    \n",
      "                                                           [ 1.10755838e-05  1.05964451e-05  1.01423288e-05 ...  1.15012383e-05  [-1.70388310e-03  1.03865063e-03  1.06991007e-03 ...  1.12502813e-03 \n",
      "                                                             1.05455676e-05  1.11054860e-05]                                      -2.26727202e-03 -1.39016034e-03]                                    \n",
      "                                                           ...                                                                   ...                                                                  \n",
      "                                                           [ 1.12354534e-05  1.10417655e-05  1.04746897e-05 ...  1.16023789e-05  [-6.20638220e-04  7.14263432e-04 -8.00409389e-04 ...  2.16987572e-03 \n",
      "                                                             1.09994634e-05  1.06893728e-05]                                      -1.58089241e-03 -1.47283642e-03]                                    \n",
      "                                                           [ 1.13206873e-05 -1.00326768e-04  1.14390759e-05 ...  1.00801857e-05  [ 1.70329879e-05 -4.55824524e-04 -1.02615960e-03 ...  2.41730916e-03 \n",
      "                                                             1.11766381e-05  1.16440890e-05]                                      -1.05314537e-03 -1.44631854e-03]                                    \n",
      "                                                           [ 1.16266652e-05 -9.99478535e-05  1.13420511e-05 ...  9.67344832e-06  [ 3.40943510e-02 -9.92874030e-02 -4.98392176e-02 ...  2.37614764e-02 \n",
      "                                                             1.15005498e-05  1.21182143e-05]]                                      3.37933072e-02  3.83138781e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.4278664372463842 [[-0.01269814 -0.02400198 -0.03783957 ... -0.013563   -0.01927728\n",
      "                                       0.00514909]                                                   \n",
      "                                     [ 0.0075187  -0.01214441 -0.05091021 ...  0.08290952  0.00083486\n",
      "                                       0.04117398]                                                   \n",
      "                                     [-0.00196626  0.06771669 -0.04841286 ...  0.01807701 -0.07442033\n",
      "                                      -0.03674908]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.04988431  0.01910564 -0.02841751 ...  0.00702323 -0.00982618\n",
      "                                      -0.00997814]                                                   \n",
      "                                     [ 0.02308274  0.04197056  0.0498305  ... -0.00651058 -0.01150665\n",
      "                                       0.02982227]                                                   \n",
      "                                     [ 0.06332247 -0.24357331 -0.14675114 ...  0.0408809   0.06146423\n",
      "                                       0.06177232]]                                                  \n",
      "Epoch 41, loss: 11.849961\n",
      "== W == -1.5176220693113405\n",
      "enter of the function =  [[ 0.0405455   0.00704012  0.01328354 ... -0.09786741  0.05524948 [8 4 8 ... 3 1 2]\n",
      "                            0.06949834]                                                                     \n",
      "                          [ 0.03234513 -0.02239288  0.0031708  ... -0.0866315   0.05687982                  \n",
      "                            0.06577375]                                                                     \n",
      "                          [-0.03548113 -0.02576335  0.00398067 ...  0.04924376 -0.03609748                  \n",
      "                           -0.05652401]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.03238625 -0.04571275  0.03459279 ... -0.08119397  0.10076654                  \n",
      "                            0.10483731]                                                                     \n",
      "                          [-0.06853205 -0.07352977 -0.05415603 ...  0.04012634 -0.06728033                  \n",
      "                           -0.0243838 ]                                                                     \n",
      "                          [-0.06182233  0.03436626 -0.07492903 ...  0.10839583 -0.02950332                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.0549786 ]]                                                                    \n",
      "soft max = [[1.15315812e-05 1.11516123e-05 1.12214543e-05 ... 1.00409984e-05\n",
      "             1.17023941e-05 1.18703335e-05]                                 \n",
      "            [1.14374046e-05 1.08281701e-05 1.11085465e-05 ... 1.01544543e-05\n",
      "             1.17214885e-05 1.18262035e-05]                                 \n",
      "            [1.06873717e-05 1.07917356e-05 1.11175466e-05 ... 1.16323234e-05\n",
      "             1.06807866e-05 1.04648283e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.14378750e-05 1.05785801e-05 1.14631410e-05 ... 1.02098199e-05\n",
      "             1.22473613e-05 1.22973190e-05]                                 \n",
      "            [1.03399177e-05 1.02883706e-05 1.04896383e-05 ... 1.15267486e-05\n",
      "             1.03528685e-05 1.08066336e-05]                                 \n",
      "            [1.04095290e-05 1.14605445e-05 1.02739846e-05 ... 1.23411573e-05\n",
      "             1.07514502e-05 1.04810133e-05]]                                \n",
      "log =  102801.7268507238\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.422414094524868 [[ 1.15315812e-05  1.11516123e-05  1.12214543e-05 ...  1.00409984e-05 \n",
      "                                                  -9.94087170e-05  1.18703335e-05]                                    \n",
      "                                                 [ 1.14374046e-05  1.08281701e-05  1.11085465e-05 ...  1.01544543e-05 \n",
      "                                                   1.17214885e-05  1.18262035e-05]                                    \n",
      "                                                 [ 1.06873717e-05  1.07917356e-05  1.11175466e-05 ...  1.16323234e-05 \n",
      "                                                  -1.00430325e-04  1.04648283e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.14378750e-05  1.05785801e-05  1.14631410e-05 ...  1.02098199e-05 \n",
      "                                                   1.22473613e-05  1.22973190e-05]                                    \n",
      "                                                 [ 1.03399177e-05 -1.00822741e-04  1.04896383e-05 ...  1.15267486e-05 \n",
      "                                                   1.03528685e-05  1.08066336e-05]                                    \n",
      "                                                 [ 1.04095290e-05  1.14605445e-05 -1.00837127e-04 ...  1.23411573e-05 \n",
      "                                                   1.07514502e-05  1.04810133e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.422414094524868 [[ 1.15315812e-05  1.11516123e-05  1.12214543e-05 ...  1.00409984e-05 [[-8.46340422e-04 -2.75885492e-04 -3.22885627e-05 ...  1.50452485e-03 \n",
      "                                                             -9.94087170e-05  1.18703335e-05]                                      -1.89915558e-03 -7.22588902e-04]                                    \n",
      "                                                            [ 1.14374046e-05  1.08281701e-05  1.11085465e-05 ...  1.01544543e-05  [-1.40123569e-03  4.58294214e-04  4.91858033e-04 ...  1.22803456e-03 \n",
      "                                                              1.17214885e-05  1.18262035e-05]                                      -2.20298860e-03 -7.90178075e-04]                                    \n",
      "                                                            [ 1.06873717e-05  1.07917356e-05  1.11175466e-05 ...  1.16323234e-05  [-1.71606075e-03  1.03709764e-03  1.06183755e-03 ...  1.15593648e-03 \n",
      "                                                             -1.00430325e-04  1.04648283e-05]                                      -2.28803451e-03 -1.41165787e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.14378750e-05  1.05785801e-05  1.14631410e-05 ...  1.02098199e-05  [-6.31532041e-04  7.15709474e-04 -8.09869530e-04 ...  2.20093951e-03 \n",
      "                                                              1.22473613e-05  1.22973190e-05]                                      -1.59896603e-03 -1.49291610e-03]                                    \n",
      "                                                            [ 1.03399177e-05 -1.00822741e-04  1.04896383e-05 ...  1.15267486e-05  [ 6.49088443e-06 -4.54919863e-04 -1.03553584e-03 ...  2.44970844e-03 \n",
      "                                                              1.03528685e-05  1.08066336e-05]                                      -1.07125737e-03 -1.46743798e-03]                                    \n",
      "                                                            [ 1.04095290e-05  1.14605445e-05 -1.00837127e-04 ...  1.23411573e-05  [ 3.40925427e-02 -9.93338949e-02 -4.98694871e-02 ...  2.37737730e-02 \n",
      "                                                              1.07514502e-05  1.04810133e-05]]                                      3.37969865e-02  3.83203328e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.4365057368179146 [[-0.01283347 -0.02424474 -0.0382182  ... -0.01368386 -0.01948885\n",
      "                                       0.00519355]                                                   \n",
      "                                     [ 0.00758    -0.01226126 -0.05141431 ...  0.0837506   0.00082138\n",
      "                                       0.04157802]                                                   \n",
      "                                     [-0.00200296  0.06840424 -0.04888629 ...  0.01826903 -0.07518721\n",
      "                                      -0.03713047]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.05038936  0.01930384 -0.02870969 ...  0.00711516 -0.00994025\n",
      "                                      -0.01009265]                                                   \n",
      "                                     [ 0.02331374  0.04238571  0.05031855 ... -0.00655152 -0.01163225\n",
      "                                       0.03010603]                                                   \n",
      "                                     [ 0.06429664 -0.24700192 -0.14871704 ...  0.04152732  0.06241681\n",
      "                                       0.06277318]]                                                  \n",
      "Epoch 42, loss: 11.858920\n",
      "== W == -1.5427672513134647\n",
      "enter of the function =  [[ 2.52784080e-02 -8.60671954e-02  1.05731324e-03 ... -8.87829488e-03 [4 7 1 ... 2 1 7]\n",
      "                            6.65312398e-02  5.58387974e-02]                                                     \n",
      "                          [-3.65277192e-02 -2.47110071e-03 -1.78412763e-02 ... -6.00937373e-02                  \n",
      "                            1.84432671e-02  6.50299775e-02]                                                     \n",
      "                          [ 8.91982553e-02 -6.35952973e-02  6.96093300e-02 ... -9.98658745e-02                  \n",
      "                            1.43420880e-01  1.17196296e-01]                                                     \n",
      "                          ...                                                                                   \n",
      "                          [ 5.64683399e-02 -3.46600589e-02 -8.70700571e-05 ... -6.68307444e-02                  \n",
      "                            4.61862675e-02  9.46772633e-02]                                                     \n",
      "                          [-6.15122671e-04 -2.27512355e-02  3.44385590e-02 ... -9.93893794e-02                  \n",
      "                            1.48689632e-02  6.43332059e-02]                                                     \n",
      "                          [ 1.77748317e-02 -2.59909747e-02  6.39007257e-02 ... -3.57206583e-02                  \n",
      "                            9.07621781e-03 -5.62277716e-03]]                                                    \n",
      "soft max = [[1.13550182e-05 1.01585343e-05 1.10832913e-05 ... 1.09737173e-05\n",
      "             1.18332411e-05 1.17073888e-05]                                 \n",
      "            [1.06744566e-05 1.10442537e-05 1.08757995e-05 ... 1.04258431e-05\n",
      "             1.12776697e-05 1.18154896e-05]                                 \n",
      "            [1.21045283e-05 1.03894002e-05 1.18697210e-05 ... 1.00193227e-05\n",
      "             1.27789879e-05 1.24482203e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.17147615e-05 1.06944115e-05 1.10706150e-05 ... 1.03558401e-05\n",
      "             1.15949266e-05 1.21710312e-05]                                 \n",
      "            [1.10647707e-05 1.08225307e-05 1.14595097e-05 ... 1.00240980e-05\n",
      "             1.12374318e-05 1.18072598e-05]                                 \n",
      "            [1.12701338e-05 1.07875252e-05 1.18021545e-05 ... 1.06830750e-05\n",
      "             1.11725244e-05 1.10095006e-05]]                                \n",
      "log =  102804.6779807655\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.422741997862833 [[ 1.13550182e-05  1.01585343e-05  1.10832913e-05 ...  1.09737173e-05 \n",
      "                                                   1.18332411e-05  1.17073888e-05]                                    \n",
      "                                                 [ 1.06744566e-05  1.10442537e-05  1.08757995e-05 ... -1.00685268e-04 \n",
      "                                                   1.12776697e-05  1.18154896e-05]                                    \n",
      "                                                 [ 1.21045283e-05 -1.00721711e-04  1.18697210e-05 ...  1.00193227e-05 \n",
      "                                                   1.27789879e-05  1.24482203e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.17147615e-05  1.06944115e-05 -1.00040496e-04 ...  1.03558401e-05 \n",
      "                                                   1.15949266e-05  1.21710312e-05]                                    \n",
      "                                                 [ 1.10647707e-05 -1.00288580e-04  1.14595097e-05 ...  1.00240980e-05 \n",
      "                                                   1.12374318e-05  1.18072598e-05]                                    \n",
      "                                                 [ 1.12701338e-05  1.07875252e-05  1.18021545e-05 ... -1.00428036e-04 \n",
      "                                                   1.11725244e-05  1.10095006e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.422741997862833 [[ 1.13550182e-05  1.01585343e-05  1.10832913e-05 ...  1.09737173e-05 [[-8.58758512e-04 -2.77399548e-04 -4.03209573e-05 ...  1.53308233e-03 \n",
      "                                                              1.18332411e-05  1.17073888e-05]                                      -1.91909770e-03 -7.42167113e-04]                                    \n",
      "                                                            [ 1.06744566e-05  1.10442537e-05  1.08757995e-05 ... -1.00685268e-04  [-1.41381630e-03  4.56950136e-04  4.83835319e-04 ...  1.25780962e-03 \n",
      "                                                              1.12776697e-05  1.18154896e-05]                                      -2.22381353e-03 -8.10759541e-04]                                    \n",
      "                                                            [ 1.21045283e-05 -1.00721711e-04  1.18697210e-05 ...  1.00193227e-05  [-1.72843469e-03  1.03552131e-03  1.05362762e-03 ...  1.18743329e-03 \n",
      "                                                              1.27789879e-05  1.24482203e-05]                                      -2.30912195e-03 -1.43350119e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.17147615e-05  1.06944115e-05 -1.00040496e-04 ...  1.03558401e-05  [-6.42608793e-04  7.17158752e-04 -8.19478189e-04 ...  2.23259272e-03 \n",
      "                                                              1.15949266e-05  1.21710312e-05]                                      -1.61733715e-03 -1.51332362e-03]                                    \n",
      "                                                            [ 1.10647707e-05 -1.00288580e-04  1.14595097e-05 ...  1.00240980e-05  [-4.23172191e-06 -4.54017061e-04 -1.04506054e-03 ...  2.48271694e-03 \n",
      "                                                              1.12374318e-05  1.18072598e-05]                                      -1.08966876e-03 -1.48889829e-03]                                    \n",
      "                                                            [ 1.12701338e-05  1.07875252e-05  1.18021545e-05 ... -1.00428036e-04  [ 3.40903672e-02 -9.93812407e-02 -4.99004040e-02 ...  2.37863311e-02 \n",
      "                                                              1.11725244e-05  1.10095006e-05]]                                      3.38004942e-02  3.83266955e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.4453196891084364 [[-0.01297026 -0.02448995 -0.03860071 ... -0.01380565 -0.01970273\n",
      "                                       0.00523826]                                                   \n",
      "                                     [ 0.00764179 -0.01237929 -0.05192354 ...  0.08460039  0.00080757\n",
      "                                       0.0419859 ]                                                   \n",
      "                                     [-0.00204015  0.06909865 -0.04936454 ...  0.01846328 -0.07596196\n",
      "                                      -0.0375159 ]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.05089957  0.01950403 -0.02900488 ...  0.00720832 -0.01005564\n",
      "                                      -0.0102085 ]                                                   \n",
      "                                     [ 0.02354694  0.04280502  0.05081138 ... -0.00659253 -0.01175929\n",
      "                                       0.03039241]                                                   \n",
      "                                     [ 0.06528053 -0.25046528 -0.15070291 ...  0.04218033  0.06337894\n",
      "                                       0.06378412]]                                                  \n",
      "Epoch 43, loss: 11.868062\n",
      "== W == -1.568296289060939\n",
      "enter of the function =  [[ 0.03105255 -0.01848991 -0.02700872 ... -0.01990177  0.04603429 [3 5 7 ... 7 3 9]\n",
      "                            0.04341009]                                                                     \n",
      "                          [-0.07615914  0.08425092 -0.09754773 ...  0.16383722 -0.13232256                  \n",
      "                           -0.13274101]                                                                     \n",
      "                          [ 0.08105216  0.01426452 -0.00089488 ... -0.09793139  0.08662999                  \n",
      "                            0.09013395]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.00798326 -0.07065096  0.01720859 ...  0.01690637 -0.01436407                  \n",
      "                           -0.01157644]                                                                     \n",
      "                          [-0.0998861  -0.04105466 -0.05228918 ...  0.27541944 -0.19725938                  \n",
      "                           -0.15092997]                                                                     \n",
      "                          [-0.06401392 -0.04296738 -0.07974321 ...  0.14105587 -0.09594085                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.09748581]]                                                                    \n",
      "soft max = [[1.14188455e-05 1.08669128e-05 1.07747328e-05 ... 1.08515811e-05\n",
      "             1.15912076e-05 1.15608298e-05]                                 \n",
      "            [1.02579541e-05 1.20427579e-05 1.00408806e-05 ... 1.30403680e-05\n",
      "             9.69771217e-06 9.69365502e-06]                                 \n",
      "            [1.20042975e-05 1.12287457e-05 1.10598084e-05 ... 1.00370291e-05\n",
      "             1.20714426e-05 1.21138146e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.11584361e-05 1.03146127e-05 1.12618527e-05 ... 1.12584496e-05\n",
      "             1.09118405e-05 1.09423011e-05]                                 \n",
      "            [1.00174288e-05 1.06244495e-05 1.05057568e-05 ... 1.45797268e-05\n",
      "             9.08798468e-06 9.51893131e-06]                                 \n",
      "            [1.03832989e-05 1.06041472e-05 1.02212547e-05 ... 1.27466492e-05\n",
      "             1.00570281e-05 1.00415024e-05]]                                \n",
      "log =  102807.70264291881\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.423078071435423 [[ 1.14188455e-05  1.08669128e-05  1.07747328e-05 ...  1.08515811e-05 \n",
      "                                                   1.15912076e-05  1.15608298e-05]                                    \n",
      "                                                 [ 1.02579541e-05  1.20427579e-05  1.00408806e-05 ...  1.30403680e-05 \n",
      "                                                   9.69771217e-06  9.69365502e-06]                                    \n",
      "                                                 [ 1.20042975e-05  1.12287457e-05  1.10598084e-05 ... -1.01074082e-04 \n",
      "                                                   1.20714426e-05  1.21138146e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11584361e-05  1.03146127e-05  1.12618527e-05 ... -9.98526615e-05 \n",
      "                                                   1.09118405e-05  1.09423011e-05]                                    \n",
      "                                                 [ 1.00174288e-05  1.06244495e-05  1.05057568e-05 ...  1.45797268e-05 \n",
      "                                                   9.08798468e-06  9.51893131e-06]                                    \n",
      "                                                 [ 1.03832989e-05  1.06041472e-05  1.02212547e-05 ...  1.27466492e-05 \n",
      "                                                   1.00570281e-05 -1.01069609e-04]]                                   \n",
      "loss , grand (prediction), grad by W =  11.423078071435423 [[ 1.14188455e-05  1.08669128e-05  1.07747328e-05 ...  1.08515811e-05 [[-8.71371769e-04 -2.78935842e-04 -4.84874467e-05 ...  1.56219030e-03 \n",
      "                                                              1.15912076e-05  1.15608298e-05]                                      -1.93935141e-03 -7.62065157e-04]                                    \n",
      "                                                            [ 1.02579541e-05  1.20427579e-05  1.00408806e-05 ...  1.30403680e-05  [-1.42659581e-03  4.55584903e-04  4.75676595e-04 ...  1.28815777e-03 \n",
      "                                                              9.69771217e-06  9.69365502e-06]                                      -2.24496185e-03 -8.31674775e-04]                                    \n",
      "                                                            [ 1.20042975e-05  1.12287457e-05  1.10598084e-05 ... -1.01074082e-04  [-1.74100777e-03  1.03392134e-03  1.04527831e-03 ...  1.21953070e-03 \n",
      "                                                              1.20714426e-05  1.21138146e-05]                                      -2.33053895e-03 -1.45569529e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.11584361e-05  1.03146127e-05  1.12618527e-05 ... -9.98526615e-05  [-6.53871222e-04  7.18611081e-04 -8.29237355e-04 ...  2.26484754e-03 \n",
      "                                                              1.09118405e-05  1.09423011e-05]                                      -1.63601015e-03 -1.53406379e-03]                                    \n",
      "                                                            [ 1.00174288e-05  1.06244495e-05  1.05057568e-05 ...  1.45797268e-05  [-1.51375726e-05 -4.53116331e-04 -1.05473571e-03 ...  2.51634719e-03 \n",
      "                                                              9.08798468e-06  9.51893131e-06]                                      -1.10838396e-03 -1.51070443e-03]                                    \n",
      "                                                            [ 1.03832989e-05  1.06041472e-05  1.02212547e-05 ...  1.27466492e-05  [ 3.40878078e-02 -9.94294630e-02 -4.99319877e-02 ...  2.37991587e-02 \n",
      "                                                              1.00570281e-05 -1.01069609e-04]]                                      3.38038202e-02  3.83329583e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.4543118336740336 [[-0.01310856 -0.02473762 -0.03898712 ... -0.01392838 -0.01991894\n",
      "                                       0.00528322]                                                   \n",
      "                                     [ 0.00770407 -0.01249851 -0.05243793 ...  0.08545897  0.0007934 \n",
      "                                       0.04239765]                                                   \n",
      "                                     [-0.00207783  0.0698     -0.04984765 ...  0.01865978 -0.07674467\n",
      "                                      -0.03790539]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.05141499  0.01970624 -0.02930313 ...  0.00730273 -0.01017237\n",
      "                                      -0.01032572]                                                   \n",
      "                                     [ 0.02378237  0.04322853  0.05130904 ... -0.00663363 -0.01188778\n",
      "                                       0.03068145]                                                   \n",
      "                                     [ 0.06627424 -0.25396374 -0.15270894 ...  0.04284     0.06435074\n",
      "                                       0.06480523]]                                                  \n",
      "Epoch 44, loss: 11.877390\n",
      "== W == -1.5942137590183525\n",
      "enter of the function =  [[-0.09725624 -0.02104089 -0.10645144 ...  0.12408639 -0.13791172 [2 0 3 ... 1 0 6]\n",
      "                           -0.10003255]                                                                     \n",
      "                          [ 0.01993309 -0.08108892  0.01235251 ... -0.06554889  0.05251206                  \n",
      "                            0.06283206]                                                                     \n",
      "                          [ 0.0981811  -0.02039019  0.0244199  ... -0.10925544  0.10916272                  \n",
      "                            0.1089538 ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.02062458 -0.04471418 -0.01217875 ... -0.02624872  0.02587903                  \n",
      "                            0.02888314]                                                                     \n",
      "                          [ 0.03492043 -0.0118821   0.03396388 ... -0.14640315  0.0827201                   \n",
      "                            0.11449034]                                                                     \n",
      "                          [ 0.00247336 -0.03044821 -0.01628447 ... -0.03156399 -0.02570576                  \n",
      "                           -0.01922112]]                                                                    \n",
      "soft max = [[1.00420477e-05 1.08373271e-05 9.95013230e-06 ... 1.25299738e-05\n",
      "             9.64197121e-06 1.00142065e-05]                                 \n",
      "            [1.12905984e-05 1.02057202e-05 1.12053326e-05 ... 1.03655561e-05\n",
      "             1.16644919e-05 1.17854927e-05]                                 \n",
      "            [1.22095494e-05 1.08443813e-05 1.13413709e-05 ... 9.92227120e-06\n",
      "             1.23443690e-05 1.23417902e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.12984084e-05 1.05837849e-05 1.09337959e-05 ... 1.07810350e-05\n",
      "             1.13579315e-05 1.13921033e-05]                                 \n",
      "            [1.14610888e-05 1.09370400e-05 1.14501309e-05 ... 9.56044371e-06\n",
      "             1.20222293e-05 1.24103106e-05]                                 \n",
      "            [1.10951785e-05 1.07358550e-05 1.08889969e-05 ... 1.07238828e-05\n",
      "             1.07868902e-05 1.08570666e-05]]                                \n",
      "log =  102810.8031311892\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.423422570132134 [[ 1.00420477e-05  1.08373271e-05 -1.01160979e-04 ...  1.25299738e-05 \n",
      "                                                   9.64197121e-06  1.00142065e-05]                                    \n",
      "                                                 [-9.98205128e-05  1.02057202e-05  1.12053326e-05 ...  1.03655561e-05 \n",
      "                                                   1.16644919e-05  1.17854927e-05]                                    \n",
      "                                                 [ 1.22095494e-05  1.08443813e-05  1.13413709e-05 ...  9.92227120e-06 \n",
      "                                                   1.23443690e-05  1.23417902e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.12984084e-05 -1.00527326e-04  1.09337959e-05 ...  1.07810350e-05 \n",
      "                                                   1.13579315e-05  1.13921033e-05]                                    \n",
      "                                                 [-9.96500223e-05  1.09370400e-05  1.14501309e-05 ...  9.56044371e-06 \n",
      "                                                   1.20222293e-05  1.24103106e-05]                                    \n",
      "                                                 [ 1.10951785e-05  1.07358550e-05  1.08889969e-05 ...  1.07238828e-05 \n",
      "                                                   1.07868902e-05  1.08570666e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.423422570132134 [[ 1.00420477e-05  1.08373271e-05 -1.01160979e-04 ...  1.25299738e-05 [[-8.84182958e-04 -2.80494653e-04 -5.67899189e-05 ...  1.59186018e-03 \n",
      "                                                              9.64197121e-06  1.00142065e-05]                                      -1.95992113e-03 -7.82287712e-04]                                    \n",
      "                                                            [-9.98205128e-05  1.02057202e-05  1.12053326e-05 ...  1.03655561e-05  [-1.43957703e-03  4.54198235e-04  4.67379928e-04 ...  1.31909091e-03 \n",
      "                                                              1.16644919e-05  1.17854927e-05]                                      -2.26643810e-03 -8.52928624e-04]                                    \n",
      "                                                            [ 1.22095494e-05  1.08443813e-05  1.13413709e-05 ...  9.92227120e-06  [-1.75378285e-03  1.03229742e-03  1.03678764e-03 ...  1.25224113e-03 \n",
      "                                                              1.23443690e-05  1.23417902e-05]                                      -2.35229014e-03 -1.47824525e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.12984084e-05 -1.00527326e-04  1.09337959e-05 ...  1.07810350e-05  [-6.65322104e-04  7.20066267e-04 -8.39149029e-04 ...  2.29771645e-03 \n",
      "                                                              1.13579315e-05  1.13921033e-05]                                      -1.65498948e-03 -1.55514144e-03]                                    \n",
      "                                                            [-9.96500223e-05  1.09370400e-05  1.14501309e-05 ...  9.56044371e-06  [-2.62294351e-05 -4.52217894e-04 -1.06456336e-03 ...  2.55061199e-03 \n",
      "                                                              1.20222293e-05  1.24103106e-05]                                      -1.12740745e-03 -1.53286139e-03]                                    \n",
      "                                                            [ 1.10951785e-05  1.07358550e-05  1.08889969e-05 ...  1.07238828e-05  [ 3.40848470e-02 -9.94785853e-02 -4.99642582e-02 ...  2.38122642e-02 \n",
      "                                                              1.07868902e-05  1.08570666e-05]]                                      3.38069538e-02  3.83391134e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.4634857822030446 [[-0.01324835 -0.02498779 -0.03937748 ... -0.01405204 -0.02013753\n",
      "                                       0.00532843]                                                   \n",
      "                                     [ 0.00776684 -0.01261894 -0.05295756 ...  0.08632644  0.00077889\n",
      "                                       0.04281331]                                                   \n",
      "                                     [-0.00211602  0.07050833 -0.05033567 ...  0.01885858 -0.07753543\n",
      "                                      -0.038299  ]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.05193568  0.01991049 -0.02960445 ...  0.0073984  -0.01029045\n",
      "                                      -0.01044432]                                                   \n",
      "                                     [ 0.02402004  0.04365628  0.05181158 ... -0.00667481 -0.01201774\n",
      "                                       0.03097316]                                                   \n",
      "                                     [ 0.06727786 -0.25749768 -0.15473535 ...  0.04350639  0.06533228\n",
      "                                       0.06583661]]                                                  \n",
      "Epoch 45, loss: 11.886908\n",
      "== W == -1.6205242312696229\n",
      "enter of the function =  [[ 0.04744496 -0.05288008  0.03418028 ... -0.03538582  0.08713464 [1 1 7 ... 9 8 6]\n",
      "                            0.04631162]                                                                     \n",
      "                          [-0.02974032 -0.03518577 -0.03496408 ...  0.13414141 -0.04172151                  \n",
      "                           -0.03504362]                                                                     \n",
      "                          [ 0.04878841 -0.10366272  0.10685805 ... -0.14016197  0.10544516                  \n",
      "                            0.11991183]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.05794187  0.02188868 -0.07473363 ...  0.14695141 -0.10940365                  \n",
      "                           -0.09769316]                                                                     \n",
      "                          [ 0.03304476 -0.0642646   0.03912469 ... -0.06586374  0.05358962                  \n",
      "                            0.04387746]                                                                     \n",
      "                          [ 0.09078398 -0.02135408  0.05492998 ... -0.16401058  0.09669327                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.13062828]]                                                                    \n",
      "soft max = [[1.16034247e-05 1.04958008e-05 1.14505253e-05 ... 1.06810325e-05\n",
      "             1.20732223e-05 1.15902815e-05]                                 \n",
      "            [1.07415028e-05 1.06831695e-05 1.06855381e-05 ... 1.26542957e-05\n",
      "             1.06135748e-05 1.06846882e-05]                                 \n",
      "            [1.16190239e-05 9.97610377e-06 1.23137113e-05 ... 9.61854839e-06\n",
      "             1.22963256e-05 1.24755055e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.04428075e-05 1.13106415e-05 1.02689183e-05 ... 1.28174399e-05\n",
      "             9.91899571e-06 1.00358348e-05]                                 \n",
      "            [1.14375304e-05 1.03769887e-05 1.15072816e-05 ... 1.03604077e-05\n",
      "             1.16749433e-05 1.15621032e-05]                                 \n",
      "            [1.21173621e-05 1.08319625e-05 1.16906024e-05 ... 9.39187302e-06\n",
      "             1.21891791e-05 1.26099176e-05]]                                \n",
      "log =  102813.98182156852\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.423775757952058 [[ 1.16034247e-05 -1.00615310e-04  1.14505253e-05 ...  1.06810325e-05 \n",
      "                                                   1.20732223e-05  1.15902815e-05]                                    \n",
      "                                                 [ 1.07415028e-05 -1.00427942e-04  1.06855381e-05 ...  1.26542957e-05 \n",
      "                                                   1.06135748e-05  1.06846882e-05]                                    \n",
      "                                                 [ 1.16190239e-05  9.97610377e-06  1.23137113e-05 ... -1.01492563e-04 \n",
      "                                                   1.22963256e-05  1.24755055e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.04428075e-05  1.13106415e-05  1.02689183e-05 ...  1.28174399e-05 \n",
      "                                                   9.91899571e-06 -1.01075276e-04]                                    \n",
      "                                                 [ 1.14375304e-05  1.03769887e-05  1.15072816e-05 ...  1.03604077e-05 \n",
      "                                                  -9.94361678e-05  1.15621032e-05]                                    \n",
      "                                                 [ 1.21173621e-05  1.08319625e-05  1.16906024e-05 ...  9.39187302e-06 \n",
      "                                                   1.21891791e-05  1.26099176e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.423775757952058 [[ 1.16034247e-05 -1.00615310e-04  1.14505253e-05 ...  1.06810325e-05 [[-8.97194863e-04 -2.82076260e-04 -6.52302740e-05 ...  1.62210369e-03 \n",
      "                                                              1.20732223e-05  1.15902815e-05]                                      -1.98081129e-03 -8.02839501e-04]                                    \n",
      "                                                            [ 1.07415028e-05 -1.00427942e-04  1.06855381e-05 ...  1.26542957e-05  [-1.45276281e-03  4.52789851e-04  4.58943371e-04 ...  1.35062125e-03 \n",
      "                                                              1.06135748e-05  1.06846882e-05]                                      -2.28824686e-03 -8.74525978e-04]                                    \n",
      "                                                            [ 1.16190239e-05  9.97610377e-06  1.23137113e-05 ... -1.01492563e-04  [-1.76676281e-03  1.03064926e-03  1.02815361e-03 ...  1.28557727e-03 \n",
      "                                                              1.22963256e-05  1.24755055e-05]                                      -2.37438020e-03 -1.50115617e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.04428075e-05  1.13106415e-05  1.02689183e-05 ...  1.28174399e-05  [-6.76964236e-04  7.21524113e-04 -8.49215222e-04 ...  2.33121222e-03 \n",
      "                                                              9.91899571e-06 -1.01075276e-04]                                      -1.67427962e-03 -1.57656146e-03]                                    \n",
      "                                                            [ 1.14375304e-05  1.03769887e-05  1.15072816e-05 ...  1.03604077e-05  [-3.75101022e-05 -4.51321976e-04 -1.07454551e-03 ...  2.58552448e-03 \n",
      "                                                             -9.94361678e-05  1.15621032e-05]                                      -1.14674374e-03 -1.55537419e-03]                                    \n",
      "                                                            [ 1.21173621e-05  1.08319625e-05  1.16906024e-05 ...  9.39187302e-06  [ 3.40814667e-02 -9.95286318e-02 -4.99972363e-02 ...  2.38256561e-02 \n",
      "                                                              1.21891791e-05  1.26099176e-05]]                                      3.38098840e-02  3.83451521e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.4728452200033605 [[-0.01338968 -0.02524047 -0.03977182 ... -0.01417664 -0.0203585 \n",
      "                                       0.00537389]                                                   \n",
      "                                     [ 0.00783011 -0.01274059 -0.05348246 ...  0.08720289  0.00076401\n",
      "                                       0.04323291]                                                   \n",
      "                                     [-0.00215472  0.07122374 -0.05082866 ...  0.01905968 -0.0783343 \n",
      "                                      -0.03869677]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.05246169  0.0201168  -0.02990889 ...  0.00749536 -0.01040991\n",
      "                                      -0.01056431]                                                   \n",
      "                                     [ 0.02425998  0.04408832  0.05231905 ... -0.00671605 -0.01214919\n",
      "                                       0.03126756]                                                   \n",
      "                                     [ 0.06829148 -0.26106744 -0.15678235 ...  0.04417958  0.06632368\n",
      "                                       0.06687837]]                                                  \n",
      "Epoch 46, loss: 11.896621\n",
      "== W == -1.6472322655485363\n",
      "enter of the function =  [[ 0.01292768 -0.07262981 -0.04946008 ...  0.18318743 -0.03689711 [3 3 0 ... 9 7 5]\n",
      "                           -0.10210049]                                                                     \n",
      "                          [ 0.0963996  -0.09529049  0.04800838 ...  0.00072904 -0.08227306                  \n",
      "                            0.00688608]                                                                     \n",
      "                          [ 0.04651714  0.00529008  0.05039688 ... -0.06945124  0.0830439                   \n",
      "                            0.07925731]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.03803702  0.01638043 -0.08955461 ...  0.12378587 -0.12543458                  \n",
      "                           -0.14148019]                                                                     \n",
      "                          [ 0.06563764 -0.04928368 -0.01634711 ... -0.1305819   0.0460719                   \n",
      "                            0.10748894]                                                                     \n",
      "                          [-0.02190975 -0.01009406 -0.061005   ...  0.06401602 -0.01628875                  \n",
      "                           -0.08481362]]                                                                    \n",
      "soft max = [[1.12076223e-05 1.02886014e-05 1.05297686e-05 ... 1.32879002e-05\n",
      "             1.06628882e-05 9.98981366e-06]                                 \n",
      "            [1.21832984e-05 1.00580765e-05 1.16077713e-05 ... 1.10717351e-05\n",
      "             1.01898627e-05 1.11401144e-05]                                 \n",
      "            [1.15904742e-05 1.11223490e-05 1.16355296e-05 ... 1.03213565e-05\n",
      "             1.20216637e-05 1.19762286e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.06507403e-05 1.12463864e-05 1.01159342e-05 ... 1.25215643e-05\n",
      "             9.75940903e-06 9.60406303e-06]                                 \n",
      "            [1.18142222e-05 1.05316263e-05 1.08842775e-05 ... 9.70930327e-06\n",
      "             1.15853148e-05 1.23191550e-05]                                 \n",
      "            [1.08239003e-05 1.09525507e-05 1.04089022e-05 ... 1.17950794e-05\n",
      "             1.08849128e-05 1.01640075e-05]]                                \n",
      "log =  102817.24117532518\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.424137908369465 [[ 1.12076223e-05  1.02886014e-05  1.05297686e-05 ...  1.32879002e-05 \n",
      "                                                   1.06628882e-05  9.98981366e-06]                                    \n",
      "                                                 [ 1.21832984e-05  1.00580765e-05  1.16077713e-05 ...  1.10717351e-05 \n",
      "                                                   1.01898627e-05  1.11401144e-05]                                    \n",
      "                                                 [-9.95206369e-05  1.11223490e-05  1.16355296e-05 ...  1.03213565e-05 \n",
      "                                                   1.20216637e-05  1.19762286e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.06507403e-05  1.12463864e-05  1.01159342e-05 ...  1.25215643e-05 \n",
      "                                                   9.75940903e-06 -1.01507048e-04]                                    \n",
      "                                                 [ 1.18142222e-05  1.05316263e-05  1.08842775e-05 ... -1.01401808e-04 \n",
      "                                                   1.15853148e-05  1.23191550e-05]                                    \n",
      "                                                 [ 1.08239003e-05  1.09525507e-05  1.04089022e-05 ...  1.17950794e-05 \n",
      "                                                   1.08849128e-05  1.01640075e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.424137908369465 [[ 1.12076223e-05  1.02886014e-05  1.05297686e-05 ...  1.32879002e-05 [[-9.10410290e-04 -2.83680946e-04 -7.38104235e-05 ...  1.65293282e-03 \n",
      "                                                              1.06628882e-05  9.98981366e-06]                                      -2.00202639e-03 -8.23725290e-04]                                    \n",
      "                                                            [ 1.21832984e-05  1.00580765e-05  1.16077713e-05 ...  1.10717351e-05  [-1.46615602e-03  4.51359469e-04  4.50364968e-04 ...  1.38276126e-03 \n",
      "                                                              1.01898627e-05  1.11401144e-05]                                      -2.31039272e-03 -8.96471774e-04]                                    \n",
      "                                                            [-9.95206369e-05  1.11223490e-05  1.16355296e-05 ...  1.03213565e-05  [-1.77995056e-03  1.02897655e-03  1.01937423e-03 ...  1.31955214e-03 \n",
      "                                                              1.20216637e-05  1.19762286e-05]                                      -2.39681384e-03 -1.52443320e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.06507403e-05  1.12463864e-05  1.01159342e-05 ...  1.25215643e-05  [-6.88800443e-04  7.22984412e-04 -8.59437953e-04 ...  2.36534794e-03 \n",
      "                                                              9.75940903e-06 -1.01507048e-04]                                      -1.69388510e-03 -1.59832877e-03]                                    \n",
      "                                                            [ 1.18142222e-05  1.05316263e-05  1.08842775e-05 ... -1.01401808e-04  [-4.89823915e-05 -4.50428809e-04 -1.08468418e-03 ...  2.62109809e-03 \n",
      "                                                              1.15853148e-05  1.23191550e-05]                                      -1.16639737e-03 -1.57824791e-03]                                    \n",
      "                                                            [ 1.08239003e-05  1.09525507e-05  1.04089022e-05 ...  1.17950794e-05  [ 3.40776480e-02 -9.95796278e-02 -5.00309434e-02 ...  2.38393433e-02 \n",
      "                                                              1.08849128e-05  1.01640075e-05]]                                      3.38125990e-02  3.83510655e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.48239390752122013 [[-0.01353255 -0.0254957  -0.04017019 ... -0.01430219 -0.02058189\n",
      "                                        0.0054196 ]                                                   \n",
      "                                      [ 0.00789389 -0.01286347 -0.05401269 ...  0.08808843  0.00074877\n",
      "                                        0.04365649]                                                   \n",
      "                                      [-0.00219394  0.07194628 -0.05132666 ...  0.01926314 -0.07914139\n",
      "                                       -0.03909875]                                                   \n",
      "                                      ...                                                             \n",
      "                                      [-0.05299308  0.02032518 -0.03021647 ...  0.00759363 -0.01053075\n",
      "                                       -0.01068572]                                                   \n",
      "                                      [ 0.02450221  0.04452469  0.0528315  ... -0.00675735 -0.01228215\n",
      "                                        0.03156468]                                                   \n",
      "                                      [ 0.06931521 -0.2646734  -0.15885014 ...  0.04485963  0.06732501\n",
      "                                        0.0679306 ]]                                                  \n",
      "Epoch 47, loss: 11.906532\n",
      "== W == -1.674342407005255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[ 0.00126076 -0.02485669  0.04394532 ... -0.01951658  0.01654955 [4 5 2 ... 1 1 4]\n",
      "                           -0.00817689]                                                                     \n",
      "                          [-0.0159496  -0.01337064 -0.10549556 ...  0.11642966 -0.07362825                  \n",
      "                           -0.08855197]                                                                     \n",
      "                          [-0.04444073 -0.02160763 -0.10525999 ...  0.27668405 -0.15512873                  \n",
      "                           -0.15098016]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.0860182  -0.00342652 -0.04412163 ...  0.28120138 -0.15258619                  \n",
      "                           -0.24160096]                                                                     \n",
      "                          [ 0.05023924 -0.02013036  0.02084347 ... -0.14934646  0.07463841                  \n",
      "                            0.12289869]                                                                     \n",
      "                          [-0.10048676 -0.01361406 -0.03471866 ...  0.12833731 -0.16873911                  \n",
      "                           -0.07778257]]                                                                    \n",
      "soft max = [[1.10754517e-05 1.07899338e-05 1.15584371e-05 ... 1.08477074e-05\n",
      "             1.12460830e-05 1.09714171e-05]                                 \n",
      "            [1.08864700e-05 1.09145820e-05 9.95400294e-06 ... 1.24273538e-05\n",
      "             1.02763187e-05 1.01240964e-05]                                 \n",
      "            [1.05806791e-05 1.08250480e-05 9.95634810e-06 ... 1.45873452e-05\n",
      "             9.47201444e-06 9.51139138e-06]                                 \n",
      "            ...                                                             \n",
      "            [1.01497811e-05 1.10236594e-05 1.05840559e-05 ... 1.46533901e-05\n",
      "             9.49612807e-06 8.68736243e-06]                                 \n",
      "            [1.16314144e-05 1.08410513e-05 1.12944766e-05 ... 9.52694288e-06\n",
      "             1.19187018e-05 1.25080073e-05]                                 \n",
      "            [1.00039856e-05 1.09119255e-05 1.06840468e-05 ... 1.25762190e-05\n",
      "             9.34397006e-06 1.02337161e-05]]                                \n",
      "log =  102820.58374244659\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.424509304716288 [[ 1.10754517e-05  1.07899338e-05  1.15584371e-05 ...  1.08477074e-05 \n",
      "                                                   1.12460830e-05  1.09714171e-05]                                    \n",
      "                                                 [ 1.08864700e-05  1.09145820e-05  9.95400294e-06 ...  1.24273538e-05 \n",
      "                                                   1.02763187e-05  1.01240964e-05]                                    \n",
      "                                                 [ 1.05806791e-05  1.08250480e-05 -1.01154763e-04 ...  1.45873452e-05 \n",
      "                                                   9.47201444e-06  9.51139138e-06]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.01497811e-05 -1.00087452e-04  1.05840559e-05 ...  1.46533901e-05 \n",
      "                                                   9.49612807e-06  8.68736243e-06]                                    \n",
      "                                                 [ 1.16314144e-05 -1.00270060e-04  1.12944766e-05 ...  9.52694288e-06 \n",
      "                                                   1.19187018e-05  1.25080073e-05]                                    \n",
      "                                                 [ 1.00039856e-05  1.09119255e-05  1.06840468e-05 ...  1.25762190e-05 \n",
      "                                                   9.34397006e-06  1.02337161e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.424509304716288 [[ 1.10754517e-05  1.07899338e-05  1.15584371e-05 ...  1.08477074e-05 [[-9.23832066e-04 -2.85308990e-04 -8.25322892e-05 ...  1.68435982e-03 \n",
      "                                                              1.12460830e-05  1.09714171e-05]                                      -2.02357092e-03 -8.44949886e-04]                                    \n",
      "                                                            [ 1.08864700e-05  1.09145820e-05  9.95400294e-06 ...  1.24273538e-05  [-1.47975953e-03  4.49906805e-04  4.41642748e-04 ...  1.41552374e-03 \n",
      "                                                              1.02763187e-05  1.01240964e-05]                                      -2.33288034e-03 -9.18770987e-04]                                    \n",
      "                                                            [ 1.05806791e-05  1.08250480e-05 -1.01154763e-04 ...  1.45873452e-05  [-1.79334904e-03  1.02727900e-03  1.01044747e-03 ...  1.35417907e-03 \n",
      "                                                              9.47201444e-06  9.51139138e-06]                                      -2.41959581e-03 -1.54808152e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.01497811e-05 -1.00087452e-04  1.05840559e-05 ...  1.46533901e-05  [-7.00833572e-04  7.24446952e-04 -8.69819251e-04 ...  2.40013700e-03 \n",
      "                                                              9.49612807e-06  8.68736243e-06]                                      -1.71381048e-03 -1.62044835e-03]                                    \n",
      "                                                            [ 1.16314144e-05 -1.00270060e-04  1.12944766e-05 ...  9.52694288e-06  [-6.06491443e-05 -4.49538633e-04 -1.09498141e-03 ...  2.65734660e-03 \n",
      "                                                              1.19187018e-05  1.25080073e-05]                                      -1.18637294e-03 -1.60148769e-03]                                    \n",
      "                                                            [ 1.00039856e-05  1.09119255e-05  1.06840468e-05 ...  1.25762190e-05  [ 3.40733713e-02 -9.96315991e-02 -5.00654019e-02 ...  2.38533349e-02 \n",
      "                                                              9.34397006e-06  1.02337161e-05]]                                      3.38150867e-02  3.83568444e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.4921356818922168 [[-0.01367698 -0.02575349 -0.04057263 ... -0.01442868 -0.02080773\n",
      "                                       0.00546556]                                                   \n",
      "                                     [ 0.00795816 -0.01298759 -0.05454832 ...  0.08898314  0.00073315\n",
      "                                       0.04408409]                                                   \n",
      "                                     [-0.00223367  0.07267604 -0.05182974 ...  0.01946896 -0.07995677\n",
      "                                      -0.03950498]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.0535299   0.02053566 -0.03052723 ...  0.00769322 -0.010653  \n",
      "                                      -0.01080856]                                                   \n",
      "                                     [ 0.02474674  0.04496544  0.05334897 ... -0.00679872 -0.01241663\n",
      "                                       0.03186454]                                                   \n",
      "                                     [ 0.07034914 -0.26831593 -0.16093895 ...  0.04554662  0.06833639\n",
      "                                       0.06899342]]                                                  \n",
      "Epoch 48, loss: 11.916645\n",
      "== W == -1.7018591816929418\n",
      "enter of the function =  [[ 0.02613921 -0.07108799 -0.03255958 ...  0.05302314 -0.01585137 [5 1 5 ... 7 0 2]\n",
      "                           -0.02803056]                                                                     \n",
      "                          [-0.01953187 -0.02727566 -0.057706   ... -0.01239303  0.13287196                  \n",
      "                            0.00853863]                                                                     \n",
      "                          [ 0.04726863 -0.02914903  0.00900008 ... -0.05328822  0.05837002                  \n",
      "                            0.06705483]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.03038888 -0.02216458  0.08483755 ...  0.05273801  0.00850495                  \n",
      "                           -0.05990489]                                                                     \n",
      "                          [ 0.04854001 -0.01571882  0.00514601 ... -0.07838909  0.04525263                  \n",
      "                            0.07573145]                                                                     \n",
      "                          [-0.05832886  0.03294728 -0.05913275 ...  0.03054422  0.03999219                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.06542313]]                                                                    \n",
      "soft max = [[1.13521369e-05 1.03003594e-05 1.07049601e-05 ... 1.16614662e-05\n",
      "             1.08853235e-05 1.07535531e-05]                                 \n",
      "            [1.08453337e-05 1.07616740e-05 1.04391251e-05 ... 1.09230338e-05\n",
      "             1.26308059e-05 1.11540807e-05]                                 \n",
      "            [1.15945529e-05 1.07415323e-05 1.11592290e-05 ... 1.04853449e-05\n",
      "             1.17239857e-05 1.18262498e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.14004823e-05 1.08168186e-05 1.20384337e-05 ... 1.16581417e-05\n",
      "             1.11537050e-05 1.04161959e-05]                                 \n",
      "            [1.16093034e-05 1.08867664e-05 1.11163033e-05 ... 1.02254294e-05\n",
      "             1.15712018e-05 1.19293080e-05]                                 \n",
      "            [1.04326251e-05 1.14296867e-05 1.04242418e-05 ... 1.14022534e-05\n",
      "             1.15104920e-05 1.18069686e-05]]                                \n",
      "log =  102824.01216524307\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.424890240582563 [[ 1.13521369e-05  1.03003594e-05  1.07049601e-05 ...  1.16614662e-05 \n",
      "                                                   1.08853235e-05  1.07535531e-05]                                    \n",
      "                                                 [ 1.08453337e-05 -1.00349437e-04  1.04391251e-05 ...  1.09230338e-05 \n",
      "                                                   1.26308059e-05  1.11540807e-05]                                    \n",
      "                                                 [ 1.15945529e-05  1.07415323e-05  1.11592290e-05 ...  1.04853449e-05 \n",
      "                                                   1.17239857e-05  1.18262498e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.14004823e-05  1.08168186e-05  1.20384337e-05 ... -9.94529694e-05 \n",
      "                                                   1.11537050e-05  1.04161959e-05]                                    \n",
      "                                                 [-9.95018077e-05  1.08867664e-05  1.11163033e-05 ...  1.02254294e-05 \n",
      "                                                   1.15712018e-05  1.19293080e-05]                                    \n",
      "                                                 [ 1.04326251e-05  1.14296867e-05 -1.00686869e-04 ...  1.14022534e-05 \n",
      "                                                   1.15104920e-05  1.18069686e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.424890240582563 [[ 1.13521369e-05  1.03003594e-05  1.07049601e-05 ...  1.16614662e-05 [[-9.37463034e-04 -2.86960676e-04 -9.13978023e-05 ...  1.71639726e-03 \n",
      "                                                              1.08853235e-05  1.07535531e-05]                                      -2.04544944e-03 -8.66518138e-04]                                    \n",
      "                                                            [ 1.08453337e-05 -1.00349437e-04  1.04391251e-05 ...  1.09230338e-05  [-1.49357626e-03  4.48431574e-04  4.32774734e-04 ...  1.44892178e-03 \n",
      "                                                              1.26308059e-05  1.11540807e-05]                                      -2.35571438e-03 -9.41428637e-04]                                    \n",
      "                                                            [ 1.15945529e-05  1.07415323e-05  1.11592290e-05 ...  1.04853449e-05  [-1.80696119e-03  1.02555628e-03  1.00137132e-03 ...  1.38947170e-03 \n",
      "                                                              1.17239857e-05  1.18262498e-05]                                      -2.44273090e-03 -1.57210639e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.14004823e-05  1.08168186e-05  1.20384337e-05 ... -9.94529694e-05  [-7.13066491e-04  7.25911511e-04 -8.80361150e-04 ...  2.43559313e-03 \n",
      "                                                              1.11537050e-05  1.04161959e-05]                                      -1.73406036e-03 -1.64292522e-03]                                    \n",
      "                                                            [-9.95018077e-05  1.08867664e-05  1.11163033e-05 ...  1.02254294e-05  [-7.25132250e-05 -4.48651692e-04 -1.10543924e-03 ...  2.69428407e-03 \n",
      "                                                              1.15712018e-05  1.19293080e-05]                                      -1.20667508e-03 -1.62509867e-03]                                    \n",
      "                                                            [ 1.04326251e-05  1.14296867e-05 -1.00686869e-04 ...  1.14022534e-05  [ 3.40686161e-02 -9.96845726e-02 -5.01006348e-02 ...  2.38676404e-02 \n",
      "                                                              1.15104920e-05  1.18069686e-05]]                                      3.38173344e-02  3.83624787e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.502074458525247 [[-0.01382299 -0.02601388 -0.04097918 ... -0.01455613 -0.02103605\n",
      "                                      0.00551176]                                                   \n",
      "                                    [ 0.00802295 -0.01311296 -0.05508938 ...  0.08988713  0.00071716\n",
      "                                      0.04451575]                                                   \n",
      "                                    [-0.00227394  0.07341307 -0.05233793 ...  0.01967719 -0.08078054\n",
      "                                     -0.03991551]                                                   \n",
      "                                    ...                                                             \n",
      "                                    [-0.05407221  0.02074826 -0.0308412  ...  0.00779415 -0.01077666\n",
      "                                     -0.01093285]                                                   \n",
      "                                    [ 0.0249936   0.0454106   0.05387151 ... -0.00684013 -0.01255266\n",
      "                                      0.03216718]                                                   \n",
      "                                    [ 0.07139337 -0.27199541 -0.163049   ...  0.04624062  0.0693579 \n",
      "                                      0.07006692]]                                                  \n",
      "Epoch 49, loss: 11.926965\n",
      "== W == -1.7297870917575993\n",
      "enter of the function =  [[-0.02738261  0.01117193 -0.02925838 ... -0.00197969 -0.00310363 [6 1 4 ... 3 5 3]\n",
      "                           -0.00362519]                                                                     \n",
      "                          [-0.00147273 -0.03234177 -0.0628485  ...  0.01105115 -0.01727422                  \n",
      "                            0.0260867 ]                                                                     \n",
      "                          [-0.03991731 -0.05888834 -0.05121184 ...  0.15700806 -0.11292616                  \n",
      "                           -0.10792659]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.06998532 -0.00696897  0.07429419 ... -0.12971829  0.0804203                   \n",
      "                            0.06676965]                                                                     \n",
      "                          [-0.0148678   0.00348237 -0.03039999 ... -0.01226342 -0.01673474                  \n",
      "                           -0.01990253]                                                                     \n",
      "                          [ 0.00472189  0.00444915 -0.0403817  ...  0.02954329 -0.06195944                  \n",
      "                            0.00378133]]                                                                    \n",
      "soft max = [[1.07582497e-05 1.11811286e-05 1.07380886e-05 ... 1.10350415e-05\n",
      "             1.10226457e-05 1.10168983e-05]                                 \n",
      "            [1.10406372e-05 1.07050300e-05 1.03833856e-05 ... 1.11797783e-05\n",
      "             1.08675498e-05 1.13491425e-05]                                 \n",
      "            [1.06242399e-05 1.04245870e-05 1.05049192e-05 ... 1.29366393e-05\n",
      "             9.87621487e-06 9.92571536e-06]                                 \n",
      "            ...                                                             \n",
      "            [1.18584513e-05 1.09801217e-05 1.19096581e-05 ... 9.71175680e-06\n",
      "             1.19828419e-05 1.18203798e-05]                                 \n",
      "            [1.08937332e-05 1.10954805e-05 1.07258369e-05 ... 1.09221416e-05\n",
      "             1.08734142e-05 1.08390240e-05]                                 \n",
      "            [1.11092420e-05 1.11062125e-05 1.06193073e-05 ... 1.13884397e-05\n",
      "             1.03926212e-05 1.10987981e-05]]                                \n",
      "log =  102827.52918212177\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.425281020235753 [[ 1.07582497e-05  1.11811286e-05  1.07380886e-05 ...  1.10350415e-05 \n",
      "                                                   1.10226457e-05  1.10168983e-05]                                    \n",
      "                                                 [ 1.10406372e-05 -1.00406081e-04  1.03833856e-05 ...  1.11797783e-05 \n",
      "                                                   1.08675498e-05  1.13491425e-05]                                    \n",
      "                                                 [ 1.06242399e-05  1.04245870e-05  1.05049192e-05 ...  1.29366393e-05 \n",
      "                                                   9.87621487e-06  9.92571536e-06]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.18584513e-05  1.09801217e-05  1.19096581e-05 ...  9.71175680e-06 \n",
      "                                                   1.19828419e-05  1.18203798e-05]                                    \n",
      "                                                 [ 1.08937332e-05  1.10954805e-05  1.07258369e-05 ...  1.09221416e-05 \n",
      "                                                   1.08734142e-05  1.08390240e-05]                                    \n",
      "                                                 [ 1.11092420e-05  1.11062125e-05  1.06193073e-05 ...  1.13884397e-05 \n",
      "                                                   1.03926212e-05  1.10987981e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.425281020235753 [[ 1.07582497e-05  1.11811286e-05  1.07380886e-05 ...  1.10350415e-05 [[-9.51306058e-04 -2.88636286e-04 -1.00408903e-04 ...  1.74905802e-03 \n",
      "                                                              1.10226457e-05  1.10168983e-05]                                      -2.06766653e-03 -8.88434936e-04]                                    \n",
      "                                                            [ 1.10406372e-05 -1.00406081e-04  1.03833856e-05 ...  1.11797783e-05  [-1.50760913e-03  4.46933490e-04  4.23758937e-04 ...  1.48296878e-03 \n",
      "                                                              1.08675498e-05  1.13491425e-05]                                      -2.37889956e-03 -9.64449783e-04]                                    \n",
      "                                                            [ 1.06242399e-05  1.04245870e-05  1.05049192e-05 ...  1.29366393e-05  [-1.82078998e-03  1.02380810e-03  9.92143732e-04 ...  1.42544399e-03 \n",
      "                                                              9.87621487e-06  9.92571536e-06]                                      -2.46622392e-03 -1.59651307e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.18584513e-05  1.09801217e-05  1.19096581e-05 ...  9.71175680e-06  [-7.25502092e-04  7.27377862e-04 -8.91065692e-04 ...  2.47173037e-03 \n",
      "                                                              1.19828419e-05  1.18203798e-05]                                      -1.75463939e-03 -1.66576441e-03]                                    \n",
      "                                                            [ 1.08937332e-05  1.10954805e-05  1.07258369e-05 ...  1.09221416e-05  [-8.45775198e-05 -4.47768237e-04 -1.11605972e-03 ...  2.73192496e-03 \n",
      "                                                              1.08734142e-05  1.08390240e-05]                                      -1.22730846e-03 -1.64908606e-03]                                    \n",
      "                                                            [ 1.11092420e-05  1.11062125e-05  1.06193073e-05 ...  1.13884397e-05  [ 3.40633611e-02 -9.97385759e-02 -5.01366660e-02 ...  2.38822697e-02 \n",
      "                                                              1.03926212e-05  1.10987981e-05]]                                      3.38193287e-02  3.83679581e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.5122142327201589 [[-0.01397059 -0.02627689 -0.04138989 ... -0.01468452 -0.02126686\n",
      "                                       0.00555822]                                                   \n",
      "                                     [ 0.00808824 -0.01323961 -0.05563595 ...  0.09080049  0.00070077\n",
      "                                       0.04495149]                                                   \n",
      "                                     [-0.00231475  0.07415746 -0.05285129 ...  0.01988786 -0.08161277\n",
      "                                      -0.04033039]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.05462006  0.02096301 -0.03115841 ...  0.00789645 -0.01090177\n",
      "                                      -0.01105861]                                                   \n",
      "                                     [ 0.02524281  0.04586021  0.05439917 ... -0.00688159 -0.01269026\n",
      "                                       0.0324726 ]                                                   \n",
      "                                     [ 0.07244799 -0.2757122  -0.16518049 ...  0.0469417   0.07038965\n",
      "                                       0.07115121]]                                                  \n",
      "Epoch 50, loss: 11.937495\n",
      "== W == -1.7581306103131489\n",
      "enter of the function =  [[-0.07193257  0.00942389 -0.14303927 ...  0.07608237 -0.04730831 [2 6 1 ... 2 6 3]\n",
      "                            0.0188848 ]                                                                     \n",
      "                          [-0.0217156  -0.00440149 -0.03047191 ... -0.04843102  0.01067195                  \n",
      "                            0.04266159]                                                                     \n",
      "                          [ 0.01065899  0.01536467 -0.00145905 ...  0.0327575   0.01282117                  \n",
      "                           -0.00574243]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.00720957 -0.02511702 -0.07350738 ...  0.01510254 -0.04333466                  \n",
      "                           -0.02008127]                                                                     \n",
      "                          [-0.0835235   0.04441874  0.00445037 ...  0.09639452 -0.05389559                  \n",
      "                           -0.05318062]                                                                     \n",
      "                          [-0.00881352 -0.04835643 -0.03650405 ...  0.13009123 -0.03961331                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.1070824 ]]                                                                    \n",
      "soft max = [[1.02872328e-05 1.11591529e-05 9.58114303e-06 ... 1.19283573e-05\n",
      "             1.05436929e-05 1.12652297e-05]                                 \n",
      "            [1.08170173e-05 1.10059350e-05 1.07227136e-05 ... 1.05318621e-05\n",
      "             1.11730890e-05 1.15362904e-05]                                 \n",
      "            [1.11729442e-05 1.12256444e-05 1.10383670e-05 ... 1.14225979e-05\n",
      "             1.11971282e-05 1.09911867e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.11344704e-05 1.07802866e-05 1.02710452e-05 ... 1.12227022e-05\n",
      "             1.05856732e-05 1.08347103e-05]                                 \n",
      "            [1.01686826e-05 1.15565792e-05 1.11037904e-05 ... 1.21731254e-05\n",
      "             1.04744669e-05 1.04819585e-05]                                 \n",
      "            [1.09574835e-05 1.05326477e-05 1.06582274e-05 ... 1.25903090e-05\n",
      "             1.06251396e-05 9.93191951e-06]]                                \n",
      "log =  102831.13763154083\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.425681959060093 [[ 1.02872328e-05  1.11591529e-05 -1.01529968e-04 ...  1.19283573e-05 \n",
      "                                                   1.05436929e-05  1.12652297e-05]                                    \n",
      "                                                 [ 1.08170173e-05  1.10059350e-05  1.07227136e-05 ...  1.05318621e-05 \n",
      "                                                   1.11730890e-05  1.15362904e-05]                                    \n",
      "                                                 [ 1.11729442e-05 -9.98854667e-05  1.10383670e-05 ...  1.14225979e-05 \n",
      "                                                   1.11971282e-05  1.09911867e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11344704e-05  1.07802866e-05 -1.00840066e-04 ...  1.12227022e-05 \n",
      "                                                   1.05856732e-05  1.08347103e-05]                                    \n",
      "                                                 [ 1.01686826e-05  1.15565792e-05  1.11037904e-05 ...  1.21731254e-05 \n",
      "                                                   1.04744669e-05  1.04819585e-05]                                    \n",
      "                                                 [ 1.09574835e-05  1.05326477e-05  1.06582274e-05 ...  1.25903090e-05 \n",
      "                                                   1.06251396e-05  9.93191951e-06]]                                   \n",
      "loss , grand (prediction), grad by W =  11.425681959060093 [[ 1.02872328e-05  1.11591529e-05 -1.01529968e-04 ...  1.19283573e-05 [[-9.65364017e-04 -2.90336103e-04 -1.09567538e-04 ...  1.78235527e-03 \n",
      "                                                              1.05436929e-05  1.12652297e-05]                                      -2.09022680e-03 -9.10705207e-04]                                    \n",
      "                                                            [ 1.08170173e-05  1.10059350e-05  1.07227136e-05 ...  1.05318621e-05  [-1.52186107e-03  4.45412267e-04  4.14593362e-04 ...  1.51767848e-03 \n",
      "                                                              1.11730890e-05  1.15362904e-05]                                      -2.40244062e-03 -9.87839523e-04]                                    \n",
      "                                                            [ 1.11729442e-05 -9.98854667e-05  1.10383670e-05 ...  1.14225979e-05  [-1.83483840e-03  1.02203416e-03  9.82762670e-04 ...  1.46211024e-03 \n",
      "                                                              1.11971282e-05  1.09911867e-05]                                      -2.49007972e-03 -1.62130687e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.11344704e-05  1.07802866e-05 -1.00840066e-04 ...  1.12227022e-05  [-7.38143288e-04  7.28845769e-04 -9.01934922e-04 ...  2.50856312e-03 \n",
      "                                                              1.05856732e-05  1.08347103e-05]                                      -1.77555224e-03 -1.68897104e-03]                                    \n",
      "                                                            [ 1.01686826e-05  1.15565792e-05  1.11037904e-05 ...  1.21731254e-05  [-9.68449360e-05 -4.46888529e-04 -1.12684491e-03 ...  2.77028404e-03 \n",
      "                                                              1.04744669e-05  1.04819585e-05]                                      -1.24827777e-03 -1.67345512e-03]                                    \n",
      "                                                            [ 1.09574835e-05  1.05326477e-05  1.06582274e-05 ...  1.25903090e-05  [ 3.40575842e-02 -9.97936379e-02 -5.01735204e-02 ...  2.38972330e-02 \n",
      "                                                              1.06251396e-05  9.93191951e-06]]                                      3.38210557e-02  3.83732717e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.5225590813198678 [[-0.01411981 -0.02654254 -0.04180479 ... -0.01481388 -0.02150021\n",
      "                                       0.00560492]                                                   \n",
      "                                     [ 0.00815405 -0.01336754 -0.05618807 ...  0.09172332  0.00068399\n",
      "                                       0.04539136]                                                   \n",
      "                                     [-0.00235611  0.07490927 -0.05336989 ...  0.02010099 -0.08245356\n",
      "                                      -0.04074966]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.05517351  0.02117991 -0.03147891 ...  0.00800013 -0.01102834\n",
      "                                      -0.01118585]                                                   \n",
      "                                     [ 0.02549439  0.04631434  0.054932   ... -0.00692308 -0.01282943\n",
      "                                       0.03278083]                                                   \n",
      "                                     [ 0.0735131  -0.27946671 -0.16733366 ...  0.04764994  0.07143174\n",
      "                                       0.07224641]]                                                  \n",
      "Epoch 51, loss: 11.948241\n",
      "== W == -1.786894175982582\n",
      "enter of the function =  [[ 0.05650254 -0.03457252  0.02852224 ... -0.14672571  0.09535085 [6 3 1 ... 0 1 2]\n",
      "                            0.13370116]                                                                     \n",
      "                          [ 0.01363287  0.01526237  0.00892399 ...  0.03358445  0.01892862                  \n",
      "                           -0.01835692]                                                                     \n",
      "                          [ 0.03708374 -0.03708224  0.00475955 ...  0.05785587  0.02454651                  \n",
      "                           -0.07478309]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.07376489 -0.00669551  0.00539871 ... -0.12220799  0.05284703                  \n",
      "                            0.09944401]                                                                     \n",
      "                          [ 0.05864757 -0.07574809  0.04982193 ... -0.03681561  0.11652643                  \n",
      "                            0.09808451]                                                                     \n",
      "                          [ 0.08663887 -0.04013369  0.05884988 ... -0.16664101  0.10331744                  \n",
      "                            0.12260187]]                                                                    \n",
      "soft max = [[1.16944113e-05 1.06764032e-05 1.13717335e-05 ... 9.54371491e-06\n",
      "             1.21576593e-05 1.26329651e-05]                                 \n",
      "            [1.12036698e-05 1.12219411e-05 1.11510371e-05 ... 1.14294456e-05\n",
      "             1.12631591e-05 1.08509388e-05]                                 \n",
      "            [1.14695106e-05 1.06496421e-05 1.11046959e-05 ... 1.17102484e-05\n",
      "             1.13266124e-05 1.02556157e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.18980367e-05 1.09782167e-05 1.11117958e-05 ... 9.78059703e-06\n",
      "             1.16517403e-05 1.22075246e-05]                                 \n",
      "            [1.17195230e-05 1.02457238e-05 1.16165459e-05 ... 1.06524819e-05\n",
      "             1.24178499e-05 1.21909397e-05]                                 \n",
      "            [1.20522021e-05 1.06171947e-05 1.17218943e-05 ... 9.35552906e-06\n",
      "             1.22549012e-05 1.24935235e-05]]                                \n",
      "log =  102834.84045615351\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.426093384017056 [[ 1.16944113e-05  1.06764032e-05  1.13717335e-05 ...  9.54371491e-06 \n",
      "                                                   1.21576593e-05  1.26329651e-05]                                    \n",
      "                                                 [ 1.12036698e-05  1.12219411e-05  1.11510371e-05 ...  1.14294456e-05 \n",
      "                                                   1.12631591e-05  1.08509388e-05]                                    \n",
      "                                                 [ 1.14695106e-05 -1.00461469e-04  1.11046959e-05 ...  1.17102484e-05 \n",
      "                                                   1.13266124e-05  1.02556157e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [-9.92130744e-05  1.09782167e-05  1.11117958e-05 ...  9.78059703e-06 \n",
      "                                                   1.16517403e-05  1.22075246e-05]                                    \n",
      "                                                 [ 1.17195230e-05 -1.00865387e-04  1.16165459e-05 ...  1.06524819e-05 \n",
      "                                                   1.24178499e-05  1.21909397e-05]                                    \n",
      "                                                 [ 1.20522021e-05  1.06171947e-05 -9.93892168e-05 ...  9.35552906e-06 \n",
      "                                                   1.22549012e-05  1.24935235e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.426093384017056 [[ 1.16944113e-05  1.06764032e-05  1.13717335e-05 ...  9.54371491e-06 [[-0.00097964 -0.00029206 -0.00011888 ...  0.0018163  -0.00211313 \n",
      "                                                              1.21576593e-05  1.26329651e-05]                                      -0.00093333]                                                    \n",
      "                                                            [ 1.12036698e-05  1.12219411e-05  1.11510371e-05 ...  1.14294456e-05  [-0.00153634  0.00044387  0.00040528 ...  0.00155306 -0.00242634 \n",
      "                                                              1.12631591e-05  1.08509388e-05]                                      -0.0010116 ]                                                    \n",
      "                                                            [ 1.14695106e-05 -1.00461469e-04  1.11046959e-05 ...  1.17102484e-05  [-0.00184911  0.00102023  0.00097323 ...  0.00149949 -0.0025143  \n",
      "                                                              1.13266124e-05  1.02556157e-05]                                      -0.00164649]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [-9.92130744e-05  1.09782167e-05  1.11117958e-05 ...  9.78059703e-06  [-0.00075099  0.00073031 -0.00091297 ...  0.00254611 -0.0017968  \n",
      "                                                              1.16517403e-05  1.22075246e-05]                                      -0.00171255]                                                    \n",
      "                                                            [ 1.17195230e-05 -1.00865387e-04  1.16165459e-05 ...  1.06524819e-05  [-0.00010932 -0.00044601 -0.0011378  ...  0.00280938 -0.00126959 \n",
      "                                                              1.24178499e-05  1.21909397e-05]                                      -0.00169821]                                                    \n",
      "                                                            [ 1.20522021e-05  1.06171947e-05 -9.93892168e-05 ...  9.35552906e-06  [ 0.03405126 -0.09984979 -0.05021122 ...  0.02391254  0.0338225  \n",
      "                                                              1.22549012e-05  1.24935235e-05]]                                      0.03837841]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.5331131643977421 [[-0.01427066 -0.02681087 -0.04222393 ... -0.01494419 -0.02173611\n",
      "                                       0.00565186]                                                   \n",
      "                                     [ 0.00822037 -0.01349676 -0.0567458  ...  0.09265573  0.00066681\n",
      "                                       0.0458354 ]                                                   \n",
      "                                     [-0.00239802  0.07566858 -0.05389376 ...  0.02031663 -0.08330299\n",
      "                                      -0.04117337]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.05573263  0.021399   -0.03180272 ...  0.00810522 -0.01115637\n",
      "                                      -0.0113146 ]                                                   \n",
      "                                     [ 0.02574837  0.04677301  0.05547005 ... -0.00696461 -0.01297021\n",
      "                                       0.03309191]                                                   \n",
      "                                     [ 0.07458881 -0.28325932 -0.16950874 ...  0.04836541  0.07248427\n",
      "                                       0.0733526 ]]                                                  \n",
      "Epoch 52, loss: 11.959207\n",
      "== W == -1.8160821870847492\n",
      "enter of the function =  [[-0.03591328 -0.06252139 -0.0146657  ... -0.01111941  0.03054432 [6 2 1 ... 6 1 4]\n",
      "                            0.0526612 ]                                                                     \n",
      "                          [ 0.02113736  0.00329555 -0.01349207 ... -0.0995761   0.06910371                  \n",
      "                            0.09236739]                                                                     \n",
      "                          [ 0.05029137 -0.08881974  0.05858354 ... -0.13961823  0.08486222                  \n",
      "                            0.071818  ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.0998476  -0.04938107 -0.0517709  ...  0.18559162 -0.1417872                   \n",
      "                           -0.11289392]                                                                     \n",
      "                          [ 0.03726165 -0.02624024  0.06191408 ... -0.12518328  0.04495058                  \n",
      "                            0.08536792]                                                                     \n",
      "                          [ 0.03006221 -0.0352483   0.01816438 ... -0.04349846  0.04731319                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.03789333]]                                                                    \n",
      "soft max = [[1.06595793e-05 1.03796883e-05 1.08884930e-05 ... 1.09271753e-05\n",
      "             1.13920593e-05 1.16468229e-05]                                 \n",
      "            [1.12853970e-05 1.10858308e-05 1.09012795e-05 ... 1.00021107e-05\n",
      "             1.18399090e-05 1.21185777e-05]                                 \n",
      "            [1.16192547e-05 1.01102776e-05 1.17160041e-05 ... 9.60951743e-06\n",
      "             1.20279661e-05 1.18720896e-05]                                 \n",
      "            ...                                                             \n",
      "            [9.99939545e-06 1.05169807e-05 1.04918770e-05 ... 1.33026579e-05\n",
      "             9.58869729e-06 9.86978743e-06]                                 \n",
      "            [1.14688411e-05 1.07631902e-05 1.17550897e-05 ... 9.74923628e-06\n",
      "             1.15573641e-05 1.20340502e-05]                                 \n",
      "            [1.13865684e-05 1.06666701e-05 1.12518956e-05 ... 1.05790304e-05\n",
      "             1.15847019e-05 1.14760879e-05]]                                \n",
      "log =  102838.6407071545\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.426515634128277 [[ 1.06595793e-05  1.03796883e-05  1.08884930e-05 ...  1.09271753e-05 \n",
      "                                                   1.13920593e-05  1.16468229e-05]                                    \n",
      "                                                 [ 1.12853970e-05  1.10858308e-05 -1.00209832e-04 ...  1.00021107e-05 \n",
      "                                                   1.18399090e-05  1.21185777e-05]                                    \n",
      "                                                 [ 1.16192547e-05 -1.01000833e-04  1.17160041e-05 ...  9.60951743e-06 \n",
      "                                                   1.20279661e-05  1.18720896e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 9.99939545e-06  1.05169807e-05  1.04918770e-05 ...  1.33026579e-05 \n",
      "                                                   9.58869729e-06  9.86978743e-06]                                    \n",
      "                                                 [ 1.14688411e-05 -1.00347921e-04  1.17550897e-05 ...  9.74923628e-06 \n",
      "                                                   1.15573641e-05  1.20340502e-05]                                    \n",
      "                                                 [ 1.13865684e-05  1.06666701e-05  1.12518956e-05 ...  1.05790304e-05 \n",
      "                                                   1.15847019e-05  1.14760879e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.426515634128277 [[ 1.06595793e-05  1.03796883e-05  1.08884930e-05 ...  1.09271753e-05 [[-0.00099414 -0.00029381 -0.00012834 ...  0.00185091 -0.0021364  \n",
      "                                                              1.13920593e-05  1.16468229e-05]                                      -0.00095633]                                                    \n",
      "                                                            [ 1.12853970e-05  1.10858308e-05 -1.00209832e-04 ...  1.00021107e-05  [-0.00155103  0.0004423   0.0003958  ...  0.00158914 -0.00245061 \n",
      "                                                              1.18399090e-05  1.21185777e-05]                                      -0.00103575]                                                    \n",
      "                                                            [ 1.16192547e-05 -1.01000833e-04  1.17160041e-05 ...  9.60951743e-06  [-0.00186361  0.00101841  0.00096353 ...  0.00153758 -0.0025389  \n",
      "                                                              1.20279661e-05  1.18720896e-05]                                      -0.00167208]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 9.99939545e-06  1.05169807e-05  1.04918770e-05 ...  1.33026579e-05  [-0.00076405  0.00073179 -0.00092418 ...  0.00258437 -0.0018184  \n",
      "                                                              9.58869729e-06  9.86978743e-06]                                      -0.00173651]                                                    \n",
      "                                                            [ 1.14688411e-05 -1.00347921e-04  1.17550897e-05 ...  9.74923628e-06  [-0.000122   -0.00044514 -0.00114892 ...  0.00284922 -0.00129124 \n",
      "                                                              1.15573641e-05  1.20340502e-05]                                      -0.00172336]                                                    \n",
      "                                                            [ 1.13865684e-05  1.06666701e-05  1.12518956e-05 ...  1.05790304e-05  [ 0.03404437 -0.09990706 -0.0502498  ...  0.0239282   0.03382365 \n",
      "                                                              1.15847019e-05  1.14760879e-05]]                                      0.03838335]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.5438807269810696 [[-0.01442316 -0.0270819  -0.04264736 ... -0.01507547 -0.0219746 \n",
      "                                       0.00569904]                                                   \n",
      "                                     [ 0.00828721 -0.01362729 -0.05730921 ...  0.09359782  0.00064921\n",
      "                                       0.04628363]                                                   \n",
      "                                     [-0.00244049  0.07643547 -0.05442296 ...  0.02053479 -0.08416117\n",
      "                                      -0.04160157]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.05629747  0.02162029 -0.03212987 ...  0.00821173 -0.01128591\n",
      "                                      -0.01144487]                                                   \n",
      "                                     [ 0.02600476  0.04723628  0.05601337 ... -0.00700616 -0.01311261\n",
      "                                       0.03340584]                                                   \n",
      "                                     [ 0.07567521 -0.28709041 -0.17170594 ...  0.04908819  0.07354734\n",
      "                                       0.07446991]]                                                  \n",
      "Epoch 53, loss: 11.970396\n",
      "== W == -1.8456989954449963\n",
      "enter of the function =  [[ 0.05529251  0.0115512   0.05158874 ... -0.12024719  0.07494189 [6 8 7 ... 7 1 3]\n",
      "                            0.0892626 ]                                                                     \n",
      "                          [ 0.05125136  0.04293263 -0.03284587 ... -0.0186295   0.05889213                  \n",
      "                           -0.02315144]                                                                     \n",
      "                          [-0.01654898 -0.03644557  0.0359446  ...  0.0750801  -0.02717397                  \n",
      "                           -0.02879669]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.00386791  0.00580704 -0.10471838 ...  0.05388193 -0.04291855                  \n",
      "                           -0.05902039]                                                                     \n",
      "                          [-0.01678376 -0.03945609 -0.13846767 ...  0.06641149  0.029427                    \n",
      "                            0.01789122]                                                                     \n",
      "                          [ 0.0449642   0.01034403 -0.0192737  ... -0.01334388 -0.01800531                  \n",
      "                           -0.03453733]]                                                                    \n",
      "soft max = [[1.16746462e-05 1.11749894e-05 1.16314860e-05 ... 9.79507594e-06\n",
      "             1.19063144e-05 1.20780480e-05]                                 \n",
      "            [1.16275624e-05 1.15312371e-05 1.06897050e-05 ... 1.08427591e-05\n",
      "             1.17167463e-05 1.07938395e-05]                                 \n",
      "            [1.08653412e-05 1.06512944e-05 1.14509374e-05 ... 1.19079601e-05\n",
      "             1.07505081e-05 1.07330773e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.10040026e-05 1.11109825e-05 9.94836899e-06 ... 1.16581899e-05\n",
      "             1.05825714e-05 1.04135371e-05]                                 \n",
      "            [1.08627905e-05 1.06192767e-05 9.61822103e-06 ... 1.18051807e-05\n",
      "             1.13765474e-05 1.12460641e-05]                                 \n",
      "            [1.15546874e-05 1.11615075e-05 1.08357765e-05 ... 1.09002216e-05\n",
      "             1.08495292e-05 1.06716390e-05]]                                \n",
      "log =  102842.54154883922\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.426949060982135 [[ 1.16746462e-05  1.11749894e-05  1.16314860e-05 ...  9.79507594e-06 \n",
      "                                                   1.19063144e-05  1.20780480e-05]                                    \n",
      "                                                 [ 1.16275624e-05  1.15312371e-05  1.06897050e-05 ...  1.08427591e-05 \n",
      "                                                  -9.93943648e-05  1.07938395e-05]                                    \n",
      "                                                 [ 1.08653412e-05  1.06512944e-05  1.14509374e-05 ... -9.92031510e-05 \n",
      "                                                   1.07505081e-05  1.07330773e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.10040026e-05  1.11109825e-05  9.94836899e-06 ... -9.94529212e-05 \n",
      "                                                   1.05825714e-05  1.04135371e-05]                                    \n",
      "                                                 [ 1.08627905e-05 -1.00491834e-04  9.61822103e-06 ...  1.18051807e-05 \n",
      "                                                   1.13765474e-05  1.12460641e-05]                                    \n",
      "                                                 [ 1.15546874e-05  1.11615075e-05  1.08357765e-05 ...  1.09002216e-05 \n",
      "                                                   1.08495292e-05  1.06716390e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.426949060982135 [[ 1.16746462e-05  1.11749894e-05  1.16314860e-05 ...  9.79507594e-06 [[-0.00100886 -0.00029558 -0.00013795 ...  0.0018862  -0.00216001 \n",
      "                                                              1.19063144e-05  1.20780480e-05]                                      -0.00097969]                                                    \n",
      "                                                            [ 1.16275624e-05  1.15312371e-05  1.06897050e-05 ...  1.08427591e-05  [-0.00156596  0.00044071  0.00038618 ...  0.00162593 -0.00247525 \n",
      "                                                             -9.93943648e-05  1.07938395e-05]                                      -0.00106027]                                                    \n",
      "                                                            [ 1.08653412e-05  1.06512944e-05  1.14509374e-05 ... -9.92031510e-05  [-0.00187833  0.00101655  0.00095368 ...  0.00157642 -0.00256387 \n",
      "                                                              1.07505081e-05  1.07330773e-05]                                      -0.00169806]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.10040026e-05  1.11109825e-05  9.94836899e-06 ... -9.94529212e-05  [-0.00077733  0.00073326 -0.00093555 ...  0.00262338 -0.00184034 \n",
      "                                                              1.05825714e-05  1.04135371e-05]                                      -0.00176085]                                                    \n",
      "                                                            [ 1.08627905e-05 -1.00491834e-04  9.61822103e-06 ...  1.18051807e-05  [-0.0001349  -0.00044427 -0.00116021 ...  0.00288982 -0.00131325 \n",
      "                                                              1.13765474e-05  1.12460641e-05]                                      -0.00174891]                                                    \n",
      "                                                            [ 1.15546874e-05  1.11615075e-05  1.08357765e-05 ...  1.09002216e-05  [ 0.03403689 -0.09996548 -0.05028928 ...  0.02394423  0.03382448 \n",
      "                                                              1.08495292e-05  1.06716390e-05]]                                      0.0383881 ]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.5548661008114535 [[-0.01457734 -0.02735566 -0.04307512 ... -0.01520772 -0.02221571\n",
      "                                       0.00574647]                                                   \n",
      "                                     [ 0.00835457 -0.01375914 -0.05787834 ...  0.09454969  0.0006312 \n",
      "                                       0.04673611]                                                   \n",
      "                                     [-0.00248353  0.07721001 -0.05495756 ...  0.02075551 -0.08502817\n",
      "                                      -0.0420343 ]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.05686808  0.02184381 -0.03246041 ...  0.00831969 -0.01141695\n",
      "                                      -0.01157669]                                                   \n",
      "                                     [ 0.02626359  0.0477042   0.05656202 ... -0.00704773 -0.01325665\n",
      "                                       0.03372267]                                                   \n",
      "                                     [ 0.0767724  -0.29096038 -0.17392549 ...  0.04981835  0.07462105\n",
      "                                       0.07559845]]                                                  \n",
      "Epoch 54, loss: 11.981815\n",
      "== W == -1.8757488998063863\n",
      "enter of the function =  [[ 0.02252926 -0.03663889 -0.0184194  ... -0.0932859   0.05202906 [1 5 7 ... 6 1 8]\n",
      "                            0.05381932]                                                                     \n",
      "                          [ 0.02718866 -0.06011214  0.01420839 ... -0.00576701 -0.03202712                  \n",
      "                            0.02057449]                                                                     \n",
      "                          [-0.06552582  0.02293226 -0.09957941 ...  0.17819905 -0.10992242                  \n",
      "                           -0.13311135]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.03788481 -0.02182624 -0.23801747 ...  0.04401311  0.14983652                  \n",
      "                           -0.01856972]                                                                     \n",
      "                          [ 0.0360519  -0.06133336  0.00194715 ... -0.14805932  0.01327878                  \n",
      "                            0.08993274]                                                                     \n",
      "                          [ 0.05071253 -0.05587033 -0.06997969 ... -0.04751784  0.07116007                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.03896363]]                                                                    \n",
      "soft max = [[1.12954695e-05 1.06465252e-05 1.08422773e-05 ... 1.00601951e-05\n",
      "             1.16336471e-05 1.16544930e-05]                                 \n",
      "            [1.13482225e-05 1.03995269e-05 1.12018713e-05 ... 1.09803295e-05\n",
      "             1.06957379e-05 1.12734110e-05]                                 \n",
      "            [1.03433793e-05 1.13000225e-05 9.99707993e-06 ... 1.31980819e-05\n",
      "             9.89421298e-06 9.66741652e-06]                                 \n",
      "            ...                                                             \n",
      "            [1.14702561e-05 1.08054022e-05 8.70462931e-06 ... 1.15407652e-05\n",
      "             1.28290095e-05 1.08406477e-05]                                 \n",
      "            [1.14492515e-05 1.03868346e-05 1.10653611e-05 ... 9.52398292e-06\n",
      "             1.11914627e-05 1.20830688e-05]                                 \n",
      "            [1.16183412e-05 1.04437334e-05 1.02974137e-05 ... 1.05313299e-05\n",
      "             1.18583532e-05 1.14826372e-05]]                                \n",
      "log =  102846.54626338984\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.427394029265537 [[ 1.12954695e-05 -1.00464586e-04  1.08422773e-05 ...  1.00601951e-05 \n",
      "                                                   1.16336471e-05  1.16544930e-05]                                    \n",
      "                                                 [ 1.13482225e-05  1.03995269e-05  1.12018713e-05 ...  1.09803295e-05 \n",
      "                                                   1.06957379e-05  1.12734110e-05]                                    \n",
      "                                                 [ 1.03433793e-05  1.13000225e-05  9.99707993e-06 ... -9.79130292e-05 \n",
      "                                                   9.89421298e-06  9.66741652e-06]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.14702561e-05  1.08054022e-05  8.70462931e-06 ...  1.15407652e-05 \n",
      "                                                   1.28290095e-05  1.08406477e-05]                                    \n",
      "                                                 [ 1.14492515e-05 -1.00724277e-04  1.10653611e-05 ...  9.52398292e-06 \n",
      "                                                   1.11914627e-05  1.20830688e-05]                                    \n",
      "                                                 [ 1.16183412e-05  1.04437334e-05  1.02974137e-05 ...  1.05313299e-05 \n",
      "                                                  -9.92527580e-05  1.14826372e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.427394029265537 [[ 1.12954695e-05 -1.00464586e-04  1.08422773e-05 ...  1.00601951e-05 [[-0.0010238  -0.00029738 -0.00014772 ...  0.00192218 -0.00218399 \n",
      "                                                              1.16336471e-05  1.16544930e-05]                                      -0.00100342]                                                    \n",
      "                                                            [ 1.13482225e-05  1.03995269e-05  1.12018713e-05 ...  1.09803295e-05  [-0.00158112  0.00043909  0.00037639 ...  0.00166343 -0.00250026 \n",
      "                                                              1.06957379e-05  1.12734110e-05]                                      -0.00108519]                                                    \n",
      "                                                            [ 1.03433793e-05  1.13000225e-05  9.99707993e-06 ... -9.79130292e-05  [-0.00189329  0.00101467  0.00094366 ...  0.00161601 -0.00258923 \n",
      "                                                              9.89421298e-06  9.66741652e-06]                                      -0.00172446]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.14702561e-05  1.08054022e-05  8.70462931e-06 ...  1.15407652e-05  [-0.00079082  0.00073473 -0.0009471  ...  0.00266315 -0.00186264 \n",
      "                                                              1.28290095e-05  1.08406477e-05]                                      -0.00178557]                                                    \n",
      "                                                            [ 1.14492515e-05 -1.00724277e-04  1.10653611e-05 ...  9.52398292e-06  [-0.000148   -0.00044341 -0.00117167 ...  0.00293121 -0.00133561 \n",
      "                                                              1.11914627e-05  1.20830688e-05]                                      -0.00177485]                                                    \n",
      "                                                            [ 1.16183412e-05  1.04437334e-05  1.02974137e-05 ...  1.05313299e-05  [ 0.03402878 -0.10002508 -0.0503297  ...  0.02396064  0.03382499 \n",
      "                                                             -9.92527580e-05  1.14826372e-05]]                                      0.03839263]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.5660737061429989 [[-0.0147332  -0.02763217 -0.04350725 ... -0.01534093 -0.02245947\n",
      "                                       0.00579414]                                                   \n",
      "                                     [ 0.00842246 -0.01389232 -0.05845327 ...  0.09551145  0.00061276\n",
      "                                       0.04719287]                                                   \n",
      "                                     [-0.00252715  0.07799228 -0.0554976  ...  0.02097883 -0.08590409\n",
      "                                      -0.04247163]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.05744454  0.02206958 -0.03279437 ...  0.00842912 -0.01154952\n",
      "                                      -0.01171006]                                                   \n",
      "                                     [ 0.02652487  0.04817679  0.05711603 ... -0.00708931 -0.01340235\n",
      "                                       0.0340424 ]                                                   \n",
      "                                     [ 0.0778805  -0.29486964 -0.17616764 ...  0.05055598  0.07570551\n",
      "                                       0.07673831]]                                                  \n",
      "Epoch 55, loss: 11.993468\n",
      "== W == -1.906236138816672\n",
      "enter of the function =  [[-0.03628881  0.04907968 -0.13434395 ...  0.16233811 -0.0764673  [2 5 3 ... 6 6 1]\n",
      "                           -0.11506786]                                                                     \n",
      "                          [ 0.01431387 -0.0755999   0.03623525 ... -0.072985    0.06121042                  \n",
      "                            0.09606697]                                                                     \n",
      "                          [ 0.08092809 -0.06808994 -0.0044312  ... -0.04975863  0.11619268                  \n",
      "                            0.06074473]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.02857596 -0.02114198 -0.04901439 ... -0.02518149  0.08613986                  \n",
      "                            0.02241362]                                                                     \n",
      "                          [-0.02917443  0.01180169 -0.03184555 ... -0.00116473 -0.00383492                  \n",
      "                           -0.00426604]                                                                     \n",
      "                          [ 0.01047198  0.0107622   0.04186819 ... -0.11927462  0.07132478                  \n",
      "                            0.08230139]]                                                                    \n",
      "soft max = [[1.06474398e-05 1.15963218e-05 9.65295728e-06 ... 1.29869680e-05\n",
      "             1.02281219e-05 9.84083350e-06]                                 \n",
      "            [1.12000937e-05 1.02369976e-05 1.14483261e-05 ... 1.02638014e-05\n",
      "             1.17378505e-05 1.21542055e-05]                                 \n",
      "            [1.19715903e-05 1.03141665e-05 1.09921027e-05 ... 1.05049823e-05\n",
      "             1.24012957e-05 1.17323855e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.07298794e-05 1.08099423e-05 1.05128034e-05 ... 1.07663636e-05\n",
      "             1.20341464e-05 1.12911801e-05]                                 \n",
      "            [1.07234598e-05 1.11719924e-05 1.06948543e-05 ... 1.10280667e-05\n",
      "             1.09986590e-05 1.09939183e-05]                                 \n",
      "            [1.11571468e-05 1.11603853e-05 1.15129957e-05 ... 9.79952243e-06\n",
      "             1.18571737e-05 1.19880422e-05]]                                \n",
      "log =  102850.65825590144\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.427850917322381 [[ 1.06474398e-05  1.15963218e-05 -1.01458154e-04 ...  1.29869680e-05 \n",
      "                                                   1.02281219e-05  9.84083350e-06]                                    \n",
      "                                                 [ 1.12000937e-05  1.02369976e-05  1.14483261e-05 ...  1.02638014e-05 \n",
      "                                                   1.17378505e-05  1.21542055e-05]                                    \n",
      "                                                 [ 1.19715903e-05  1.03141665e-05  1.09921027e-05 ...  1.05049823e-05 \n",
      "                                                   1.24012957e-05  1.17323855e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.07298794e-05  1.08099423e-05  1.05128034e-05 ...  1.07663636e-05 \n",
      "                                                   1.20341464e-05  1.12911801e-05]                                    \n",
      "                                                 [ 1.07234598e-05  1.11719924e-05  1.06948543e-05 ...  1.10280667e-05 \n",
      "                                                   1.09986590e-05  1.09939183e-05]                                    \n",
      "                                                 [ 1.11571468e-05 -9.99507258e-05  1.15129957e-05 ...  9.79952243e-06 \n",
      "                                                   1.18571737e-05  1.19880422e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.427850917322381 [[ 1.06474398e-05  1.15963218e-05 -1.01458154e-04 ...  1.29869680e-05 [[-0.00103898 -0.00029921 -0.00015764 ...  0.00195887 -0.00220834 \n",
      "                                                              1.02281219e-05  9.84083350e-06]                                      -0.00102753]                                                    \n",
      "                                                            [ 1.12000937e-05  1.02369976e-05  1.14483261e-05 ...  1.02638014e-05  [-0.00159651  0.00043745  0.00036645 ...  0.00170167 -0.00252565 \n",
      "                                                              1.17378505e-05  1.21542055e-05]                                      -0.0011105 ]                                                    \n",
      "                                                            [ 1.19715903e-05  1.03141665e-05  1.09921027e-05 ...  1.05049823e-05  [-0.00190848  0.00101277  0.00093348 ...  0.00165638 -0.00261497 \n",
      "                                                              1.24012957e-05  1.17323855e-05]                                      -0.00175127]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.07298794e-05  1.08099423e-05  1.05128034e-05 ...  1.07663636e-05  [-0.00080454  0.0007362  -0.00095882 ...  0.00270369 -0.00188529 \n",
      "                                                              1.20341464e-05  1.12911801e-05]                                      -0.0018107 ]                                                    \n",
      "                                                            [ 1.07234598e-05  1.11719924e-05  1.06948543e-05 ...  1.10280667e-05  [-0.00016133 -0.00044256 -0.00118331 ...  0.0029734  -0.00135833 \n",
      "                                                              1.09986590e-05  1.09939183e-05]                                      -0.00180121]                                                    \n",
      "                                                            [ 1.11571468e-05 -9.99507258e-05  1.15129957e-05 ...  9.79952243e-06  [ 0.03402003 -0.1000859  -0.05037107 ...  0.02397744  0.03382514 \n",
      "                                                              1.18571737e-05  1.19880422e-05]]                                      0.03839693]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.5775080535791843 [[-0.01489077 -0.02791147 -0.0439438  ... -0.01547512 -0.02270591\n",
      "                                       0.00584204]                                                   \n",
      "                                     [ 0.00849087 -0.01402685 -0.05903403 ...  0.0964832   0.00059388\n",
      "                                       0.04765395]                                                   \n",
      "                                     [-0.00257135  0.07878234 -0.05604314 ...  0.02120478 -0.08678902\n",
      "                                      -0.04291359]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.05802689  0.02229762 -0.03313179 ...  0.00854005 -0.01168364\n",
      "                                      -0.01184502]                                                   \n",
      "                                     [ 0.02678864  0.04865413  0.05767548 ... -0.00713089 -0.01354973\n",
      "                                       0.03436508]                                                   \n",
      "                                     [ 0.07899959 -0.29881859 -0.17843261 ...  0.05130115  0.07680081\n",
      "                                       0.07788962]]                                                  \n",
      "Epoch 56, loss: 12.005359\n",
      "== W == -1.937164883564497\n",
      "enter of the function =  [[ 8.43647678e-03 -2.82855625e-02 -7.58244612e-02 ...  5.88104467e-02 [1 4 7 ... 1 1 1]\n",
      "                           -2.30288512e-04 -1.36952919e-02]                                                     \n",
      "                          [ 7.99825273e-02 -2.84472962e-02  5.34614976e-02 ... -1.71020272e-01                  \n",
      "                            9.68759255e-02  1.74368439e-01]                                                     \n",
      "                          [ 4.72340092e-02 -1.57921386e-02  8.31612267e-02 ... -1.13647198e-01                  \n",
      "                            7.99278089e-02  1.06089411e-01]                                                     \n",
      "                          ...                                                                                   \n",
      "                          [ 6.58794217e-02  6.47836255e-03  4.47910876e-02 ... -2.36274893e-01                  \n",
      "                            1.13215660e-01  1.75936857e-01]                                                     \n",
      "                          [-2.50817775e-02 -3.13716189e-02 -5.20484067e-02 ... -7.42125527e-02                  \n",
      "                            3.42879053e-03  6.95260247e-02]                                                     \n",
      "                          [-2.21808449e-02 -1.26605072e-01 -9.90597121e-02 ...  1.29837381e-01                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -9.11124311e-02 -1.43561706e-01]]                                                    \n",
      "soft max = [[1.11314070e-05 1.07300534e-05 1.02318933e-05 ... 1.17065035e-05\n",
      "             1.10353505e-05 1.08877554e-05]                                 \n",
      "            [1.19569969e-05 1.07283181e-05 1.16440532e-05 ... 9.30278504e-06\n",
      "             1.21607070e-05 1.31405456e-05]                                 \n",
      "            [1.15717653e-05 1.08649494e-05 1.19950651e-05 ... 9.85212231e-06\n",
      "             1.19563426e-05 1.22732673e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.17895496e-05 1.11096317e-05 1.15435309e-05 ... 8.71511784e-06\n",
      "             1.23610420e-05 1.31611716e-05]                                 \n",
      "            [1.07644853e-05 1.06969909e-05 1.04780824e-05 ... 1.02483994e-05\n",
      "             1.10758037e-05 1.18326199e-05]                                 \n",
      "            [1.07957577e-05 9.72528331e-06 9.99689337e-06 ... 1.25682209e-05\n",
      "             1.00766580e-05 9.56176551e-06]]                                \n",
      "log =  102854.88105966298\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.42832011774033 [[ 1.11314070e-05 -1.00381058e-04  1.02318933e-05 ...  1.17065035e-05 \n",
      "                                                  1.10353505e-05  1.08877554e-05]                                    \n",
      "                                                [ 1.19569969e-05  1.07283181e-05  1.16440532e-05 ...  9.30278504e-06 \n",
      "                                                  1.21607070e-05  1.31405456e-05]                                    \n",
      "                                                [ 1.15717653e-05  1.08649494e-05  1.19950651e-05 ... -1.01258989e-04 \n",
      "                                                  1.19563426e-05  1.22732673e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [ 1.17895496e-05 -1.00001479e-04  1.15435309e-05 ...  8.71511784e-06 \n",
      "                                                  1.23610420e-05  1.31611716e-05]                                    \n",
      "                                                [ 1.07644853e-05 -1.00414120e-04  1.04780824e-05 ...  1.02483994e-05 \n",
      "                                                  1.10758037e-05  1.18326199e-05]                                    \n",
      "                                                [ 1.07957577e-05 -1.01385828e-04  9.99689337e-06 ...  1.25682209e-05 \n",
      "                                                  1.00766580e-05  9.56176551e-06]]                                   \n",
      "loss , grand (prediction), grad by W =  11.42832011774033 [[ 1.11314070e-05 -1.00381058e-04  1.02318933e-05 ...  1.17065035e-05 [[-0.00105439 -0.00030106 -0.00016773 ...  0.00199628 -0.00223306 \n",
      "                                                             1.10353505e-05  1.08877554e-05]                                      -0.00105203]                                                    \n",
      "                                                           [ 1.19569969e-05  1.07283181e-05  1.16440532e-05 ...  9.30278504e-06  [-0.00161214  0.00043578  0.00035634 ...  0.00174067 -0.00255143 \n",
      "                                                             1.21607070e-05  1.31405456e-05]                                      -0.00113621]                                                    \n",
      "                                                           [ 1.15717653e-05  1.08649494e-05  1.19950651e-05 ... -1.01258989e-04  [-0.00192391  0.00101083  0.00092314 ...  0.00169753 -0.00264111 \n",
      "                                                             1.19563426e-05  1.22732673e-05]                                      -0.0017785 ]                                                    \n",
      "                                                           ...                                                                   ...                                                              \n",
      "                                                           [ 1.17895496e-05 -1.00001479e-04  1.15435309e-05 ...  8.71511784e-06  [-0.00081847  0.00073767 -0.00097072 ...  0.00274502 -0.00190831 \n",
      "                                                             1.23610420e-05  1.31611716e-05]                                      -0.00183622]                                                    \n",
      "                                                           [ 1.07644853e-05 -1.00414120e-04  1.04780824e-05 ...  1.02483994e-05  [-0.00017488 -0.0004417  -0.00119513 ...  0.0030164  -0.00138142 \n",
      "                                                             1.10758037e-05  1.18326199e-05]                                      -0.00182798]                                                    \n",
      "                                                           [ 1.07957577e-05 -1.01385828e-04  9.99689337e-06 ...  1.25682209e-05  [ 0.0340106  -0.10014798 -0.05041344 ...  0.02399464  0.03382493 \n",
      "                                                             1.00766580e-05  9.56176551e-06]]                                      0.03840098]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.5891737459493369 [[-0.01505007 -0.02819357 -0.04438481 ... -0.01561028 -0.02295505\n",
      "                                       0.00589019]                                                   \n",
      "                                     [ 0.00855982 -0.01416275 -0.05962071 ...  0.09746504  0.00057456\n",
      "                                       0.04811938]                                                   \n",
      "                                     [-0.00261615  0.0795803  -0.05659423 ...  0.02143339 -0.08768306\n",
      "                                      -0.04336024]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.0586152   0.02252796 -0.03347269 ...  0.00865248 -0.01181933\n",
      "                                      -0.01198157]                                                   \n",
      "                                     [ 0.02705491  0.04913624  0.0582404  ... -0.00717247 -0.01369881\n",
      "                                       0.03469072]                                                   \n",
      "                                     [ 0.08012979 -0.30280763 -0.18072065 ...  0.05205393  0.07790707\n",
      "                                       0.07905249]]                                                  \n",
      "Epoch 57, loss: 12.017494\n",
      "== W == -1.9685392296364421\n",
      "enter of the function =  [[-0.01555551 -0.0741279  -0.02518196 ... -0.00208777  0.05723577 [1 2 2 ... 8 1 1]\n",
      "                            0.01802508]                                                                     \n",
      "                          [ 0.08595637 -0.02852726  0.03121343 ... -0.04952389  0.11952036                  \n",
      "                            0.10602353]                                                                     \n",
      "                          [ 0.09920756 -0.01634345  0.12608475 ... -0.13143353  0.15775939                  \n",
      "                            0.03835911]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.01247638 -0.09448712 -0.02516421 ...  0.05899783  0.01175011                  \n",
      "                            0.00434894]                                                                     \n",
      "                          [-0.03206505 -0.00735927 -0.05212001 ...  0.11974874 -0.09462616                  \n",
      "                           -0.06384799]                                                                     \n",
      "                          [ 0.06037127 -0.20789935  0.10298805 ...  0.05047146  0.00792416                  \n",
      "                           -0.11843525]]                                                                    \n",
      "soft max = [[1.08644286e-05 1.02463509e-05 1.07603445e-05 ... 1.10117376e-05\n",
      "             1.16847585e-05 1.12354573e-05]                                 \n",
      "            [1.20252174e-05 1.07244081e-05 1.13846159e-05 ... 1.05015791e-05\n",
      "             1.24356816e-05 1.22689669e-05]                                 \n",
      "            [1.21856263e-05 1.08558714e-05 1.25175828e-05 ... 9.67568464e-06\n",
      "             1.29204189e-05 1.14662581e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.11732878e-05 1.00398524e-05 1.07605355e-05 ... 1.17053659e-05\n",
      "             1.11651759e-05 1.10828456e-05]                                 \n",
      "            [1.06865344e-05 1.09538420e-05 1.04743511e-05 ... 1.24385220e-05\n",
      "             1.00384566e-05 1.03522257e-05]                                 \n",
      "            [1.17214535e-05 8.96340498e-06 1.22317812e-05 ... 1.16059859e-05\n",
      "             1.11225401e-05 9.80227277e-06]]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log =  102859.21834170935\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.428802037967705 [[ 1.08644286e-05 -1.00864760e-04  1.07603445e-05 ...  1.10117376e-05 \n",
      "                                                   1.16847585e-05  1.12354573e-05]                                    \n",
      "                                                 [ 1.20252174e-05  1.07244081e-05 -9.97264952e-05 ...  1.05015791e-05 \n",
      "                                                   1.24356816e-05  1.22689669e-05]                                    \n",
      "                                                 [ 1.21856263e-05  1.08558714e-05 -9.85935283e-05 ...  9.67568464e-06 \n",
      "                                                   1.29204189e-05  1.14662581e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11732878e-05  1.00398524e-05  1.07605355e-05 ...  1.17053659e-05 \n",
      "                                                  -9.99459352e-05  1.10828456e-05]                                    \n",
      "                                                 [ 1.06865344e-05 -1.00157269e-04  1.04743511e-05 ...  1.24385220e-05 \n",
      "                                                   1.00384566e-05  1.03522257e-05]                                    \n",
      "                                                 [ 1.17214535e-05 -1.02147706e-04  1.22317812e-05 ...  1.16059859e-05 \n",
      "                                                   1.11225401e-05  9.80227277e-06]]                                   \n",
      "loss , grand (prediction), grad by W =  11.428802037967705 [[ 1.08644286e-05 -1.00864760e-04  1.07603445e-05 ...  1.10117376e-05 [[-0.00107003 -0.00030294 -0.00017797 ...  0.00203443 -0.00225815 \n",
      "                                                              1.16847585e-05  1.12354573e-05]                                      -0.00107692]                                                    \n",
      "                                                            [ 1.20252174e-05  1.07244081e-05 -9.97264952e-05 ...  1.05015791e-05  [-0.00162801  0.00043409  0.00034607 ...  0.00178043 -0.0025776  \n",
      "                                                              1.24356816e-05  1.22689669e-05]                                      -0.00116232]                                                    \n",
      "                                                            [ 1.21856263e-05  1.08558714e-05 -9.85935283e-05 ...  9.67568464e-06  [-0.00193958  0.00100887  0.00091263 ...  0.00173948 -0.00266764 \n",
      "                                                              1.29204189e-05  1.14662581e-05]                                      -0.00180616]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.11732878e-05  1.00398524e-05  1.07605355e-05 ...  1.17053659e-05  [-0.00083264  0.00073914 -0.0009828  ...  0.00278715 -0.00193169 \n",
      "                                                             -9.99459352e-05  1.10828456e-05]                                      -0.00186214]                                                    \n",
      "                                                            [ 1.06865344e-05 -1.00157269e-04  1.04743511e-05 ...  1.24385220e-05  [-0.00018865 -0.00044086 -0.00120713 ...  0.00306023 -0.00140487 \n",
      "                                                              1.00384566e-05  1.03522257e-05]                                      -0.00185517]                                                    \n",
      "                                                            [ 1.17214535e-05 -1.02147706e-04  1.22317812e-05 ...  1.16059859e-05  [ 0.03400047 -0.10021134 -0.05045684 ...  0.02401225  0.03382434 \n",
      "                                                              1.11225401e-05  9.80227277e-06]]                                      0.03840478]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.6010754802256574 [[-0.01521111 -0.02847852 -0.04483034 ... -0.01574642 -0.02320693\n",
      "                                       0.00593857]                                                   \n",
      "                                     [ 0.00862929 -0.01430002 -0.06021335 ...  0.0984571   0.00055479\n",
      "                                       0.04858922]                                                   \n",
      "                                     [-0.00266155  0.08038621 -0.05715094 ...  0.0216647  -0.0885863 \n",
      "                                      -0.04381163]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.05920954  0.02276062 -0.03381713 ...  0.00876646 -0.01195661\n",
      "                                      -0.01211975]                                                   \n",
      "                                     [ 0.02732372  0.04962319  0.05881085 ... -0.00721403 -0.01384961\n",
      "                                       0.03501935]                                                   \n",
      "                                     [ 0.08127119 -0.30683719 -0.18303199 ...  0.05281442  0.07902439\n",
      "                                       0.08022702]]                                                  \n",
      "Epoch 58, loss: 12.029878\n",
      "== W == -2.000363188664603\n",
      "enter of the function =  [[-0.02668407  0.00020877 -0.01829805 ...  0.00075589 -0.04092777 [1 6 1 ... 1 7 3]\n",
      "                           -0.00226789]                                                                     \n",
      "                          [ 0.04280834 -0.03721387  0.05210497 ... -0.10611394  0.0724265                   \n",
      "                            0.09419161]                                                                     \n",
      "                          [-0.04596669  0.00048254 -0.03946931 ...  0.07885223 -0.01682583                  \n",
      "                            0.0093516 ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.07350858 -0.0602675   0.04716202 ... -0.11667625  0.1021017                   \n",
      "                            0.06712174]                                                                     \n",
      "                          [-0.03176654 -0.08967009 -0.01839036 ... -0.03648207 -0.01253604                  \n",
      "                            0.0347978 ]                                                                     \n",
      "                          [-0.07248568 -0.01960184 -0.15116757 ...  0.05949858 -0.13938522                  \n",
      "                           -0.09308023]]                                                                    \n",
      "soft max = [[1.07410207e-05 1.10337963e-05 1.08314738e-05 ... 1.10398349e-05\n",
      "             1.05891133e-05 1.10065033e-05]                                 \n",
      "            [1.15139867e-05 1.06285132e-05 1.16215270e-05 ... 9.92086637e-06\n",
      "             1.18601102e-05 1.21210766e-05]                                 \n",
      "            [1.05358897e-05 1.10368175e-05 1.06045684e-05 ... 1.19365655e-05\n",
      "             1.08474320e-05 1.11351390e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.18729508e-05 1.03862903e-05 1.15642242e-05 ... 9.81663051e-06\n",
      "             1.22173356e-05 1.17973618e-05]                                 \n",
      "            [1.06865683e-05 1.00853523e-05 1.08304741e-05 ... 1.06362940e-05\n",
      "             1.08940650e-05 1.14221219e-05]                                 \n",
      "            [1.02601608e-05 1.08173610e-05 9.48381460e-06 ... 1.17077706e-05\n",
      "             9.59621708e-06 1.00510183e-05]]                                \n",
      "log =  102863.67390866159\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.429297100962398 [[ 1.07410207e-05 -1.00077315e-04  1.08314738e-05 ...  1.10398349e-05 \n",
      "                                                   1.05891133e-05  1.10065033e-05]                                    \n",
      "                                                 [ 1.15139867e-05  1.06285132e-05  1.16215270e-05 ...  9.92086637e-06 \n",
      "                                                   1.18601102e-05  1.21210766e-05]                                    \n",
      "                                                 [ 1.05358897e-05 -1.00074294e-04  1.06045684e-05 ...  1.19365655e-05 \n",
      "                                                   1.08474320e-05  1.11351390e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.18729508e-05 -1.00724821e-04  1.15642242e-05 ...  9.81663051e-06 \n",
      "                                                   1.22173356e-05  1.17973618e-05]                                    \n",
      "                                                 [ 1.06865683e-05  1.00853523e-05  1.08304741e-05 ... -1.00474817e-04 \n",
      "                                                   1.08940650e-05  1.14221219e-05]                                    \n",
      "                                                 [ 1.02601608e-05  1.08173610e-05  9.48381460e-06 ...  1.17077706e-05 \n",
      "                                                   9.59621708e-06  1.00510183e-05]]                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss , grand (prediction), grad by W =  11.429297100962398 [[ 1.07410207e-05 -1.00077315e-04  1.08314738e-05 ...  1.10398349e-05 [[-0.00108592 -0.00030484 -0.00018838 ...  0.00207334 -0.00228363 \n",
      "                                                              1.05891133e-05  1.10065033e-05]                                      -0.0011022 ]                                                    \n",
      "                                                            [ 1.15139867e-05  1.06285132e-05  1.16215270e-05 ...  9.92086637e-06  [-0.00164412  0.00043237  0.00033564 ...  0.00182098 -0.00260416 \n",
      "                                                              1.18601102e-05  1.21210766e-05]                                      -0.00118885]                                                    \n",
      "                                                            [ 1.05358897e-05 -1.00074294e-04  1.06045684e-05 ...  1.19365655e-05  [-0.0019555   0.00100688  0.00090194 ...  0.00178226 -0.00269458 \n",
      "                                                              1.08474320e-05  1.11351390e-05]                                      -0.00183424]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.18729508e-05 -1.00724821e-04  1.15642242e-05 ...  9.81663051e-06  [-0.00084703  0.00074061 -0.00099506 ...  0.00283011 -0.00195544 \n",
      "                                                              1.22173356e-05  1.17973618e-05]                                      -0.00188847]                                                    \n",
      "                                                            [ 1.06865683e-05  1.00853523e-05  1.08304741e-05 ... -1.00474817e-04  [-0.00020265 -0.00044002 -0.0012193  ...  0.00310491 -0.0014287  \n",
      "                                                              1.08940650e-05  1.14221219e-05]                                      -0.00188279]                                                    \n",
      "                                                            [ 1.02601608e-05  1.08173610e-05  9.48381460e-06 ...  1.17077706e-05  [ 0.0339896  -0.10027603 -0.05050129 ...  0.0240303   0.03382334 \n",
      "                                                              9.59621708e-06  1.00510183e-05]]                                      0.0384083 ]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.6132180494817732 [[-0.01537392 -0.02876633 -0.04528042 ... -0.01588354 -0.02346158\n",
      "                                       0.00598719]                                                   \n",
      "                                     [ 0.00869931 -0.01443868 -0.06081203 ...  0.09945948  0.00053457\n",
      "                                       0.04906348]                                                   \n",
      "                                     [-0.00270756  0.08120016 -0.05771333 ...  0.02189874 -0.08949884\n",
      "                                      -0.0442678 ]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.05980996  0.02299562 -0.03416513 ...  0.008882   -0.01209549\n",
      "                                      -0.01225957]                                                   \n",
      "                                     [ 0.02759507  0.05011501  0.05938689 ... -0.00725557 -0.01400215\n",
      "                                       0.03535099]                                                   \n",
      "                                     [ 0.08242391 -0.31090767 -0.18536688 ...  0.05358269  0.08015288\n",
      "                                       0.08141334]]                                                  \n",
      "Epoch 59, loss: 12.042515\n",
      "== W == -2.032640679332217\n",
      "enter of the function =  [[-0.08172136 -0.0254815  -0.06884208 ...  0.09834038 -0.14592609 [2 1 0 ... 5 4 3]\n",
      "                           -0.10275342]                                                                     \n",
      "                          [-0.04605246 -0.10997964 -0.08028503 ...  0.12764876 -0.06242304                  \n",
      "                           -0.1051487 ]                                                                     \n",
      "                          [ 0.09273205 -0.04763559  0.00982848 ... -0.15008611  0.05651496                  \n",
      "                            0.11630391]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.00557254 -0.05485674 -0.05382188 ...  0.0352244  -0.06262221                  \n",
      "                            0.0019917 ]                                                                     \n",
      "                          [ 0.00024985 -0.01453964  0.01758369 ... -0.05056416 -0.01910784                  \n",
      "                            0.01299314]                                                                     \n",
      "                          [-0.05052574 -0.00347717 -0.12063468 ...  0.12565552 -0.12005145                  \n",
      "                           -0.06116812]]                                                                    \n",
      "soft max = [[1.01627219e-05 1.07506496e-05 1.02944570e-05 ... 1.21677384e-05\n",
      "             9.53073252e-06 9.95121106e-06]                                 \n",
      "            [1.05317575e-05 9.87956057e-06 1.01773294e-05 ... 1.25296324e-05\n",
      "             1.03607500e-05 9.92740365e-06]                                 \n",
      "            [1.20996887e-05 1.05150975e-05 1.11370363e-05 ... 9.49116686e-06\n",
      "             1.16693137e-05 1.23882889e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.09668285e-05 1.04394400e-05 1.04502489e-05 ... 1.14234935e-05\n",
      "             1.03586867e-05 1.10500988e-05]                                 \n",
      "            [1.10308680e-05 1.08689275e-05 1.12237421e-05 ... 1.04843483e-05\n",
      "             1.08193893e-05 1.11723371e-05]                                 \n",
      "            [1.04847512e-05 1.09898323e-05 9.77485232e-06 ... 1.25046827e-05\n",
      "             9.78055496e-06 1.03737602e-05]]                                \n",
      "log =  102868.25171287345\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.429805745874827 [[ 1.01627219e-05  1.07506496e-05 -1.00816654e-04 ...  1.21677384e-05 \n",
      "                                                   9.53073252e-06  9.95121106e-06]                                    \n",
      "                                                 [ 1.05317575e-05 -1.01231551e-04  1.01773294e-05 ...  1.25296324e-05 \n",
      "                                                   1.03607500e-05  9.92740365e-06]                                    \n",
      "                                                 [-9.90114224e-05  1.05150975e-05  1.11370363e-05 ...  9.49116686e-06 \n",
      "                                                   1.16693137e-05  1.23882889e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.09668285e-05  1.04394400e-05  1.04502489e-05 ...  1.14234935e-05 \n",
      "                                                   1.03586867e-05  1.10500988e-05]                                    \n",
      "                                                 [ 1.10308680e-05  1.08689275e-05  1.12237421e-05 ...  1.04843483e-05 \n",
      "                                                   1.08193893e-05  1.11723371e-05]                                    \n",
      "                                                 [ 1.04847512e-05  1.09898323e-05  9.77485232e-06 ...  1.25046827e-05 \n",
      "                                                   9.78055496e-06  1.03737602e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.429805745874827 [[ 1.01627219e-05  1.07506496e-05 -1.00816654e-04 ...  1.21677384e-05 [[-0.00110204 -0.00030677 -0.00019896 ...  0.00211301 -0.00230949 \n",
      "                                                              9.53073252e-06  9.95121106e-06]                                      -0.00112787]                                                    \n",
      "                                                            [ 1.05317575e-05 -1.01231551e-04  1.01773294e-05 ...  1.25296324e-05  [-0.00166048  0.00043063  0.00032503 ...  0.00186233 -0.00263112 \n",
      "                                                              1.03607500e-05  9.92740365e-06]                                      -0.00121579]                                                    \n",
      "                                                            [-9.90114224e-05  1.05150975e-05  1.11370363e-05 ...  9.49116686e-06  [-0.00197166  0.00100486  0.00089109 ...  0.00182588 -0.00272192 \n",
      "                                                              1.16693137e-05  1.23882889e-05]                                      -0.00186277]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.09668285e-05  1.04394400e-05  1.04502489e-05 ...  1.14234935e-05  [-0.00086165  0.00074208 -0.00100751 ...  0.00287391 -0.00197958 \n",
      "                                                              1.03586867e-05  1.10500988e-05]                                      -0.00191522]                                                    \n",
      "                                                            [ 1.10308680e-05  1.08689275e-05  1.12237421e-05 ...  1.04843483e-05  [-0.00021688 -0.00043919 -0.00123166 ...  0.00315046 -0.00145291 \n",
      "                                                              1.08193893e-05  1.11723371e-05]                                      -0.00191083]                                                    \n",
      "                                                            [ 1.04847512e-05  1.09898323e-05  9.77485232e-06 ...  1.25046827e-05  [ 0.03397796 -0.10034209 -0.05054683 ...  0.02404879  0.03382191 \n",
      "                                                              9.78055496e-06  1.03737602e-05]]                                      0.03841152]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.6256063448938242 [[-0.01553852 -0.02905705 -0.04573511 ... -0.01602164 -0.02371903\n",
      "                                       0.00603604]                                                   \n",
      "                                     [ 0.00876986 -0.01457874 -0.06141679 ...  0.10047228  0.00051387\n",
      "                                       0.04954223]                                                   \n",
      "                                     [-0.00275419  0.08202223 -0.05828144 ...  0.02213555 -0.09042078\n",
      "                                      -0.04472882]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.06041653  0.02323298 -0.03451673 ...  0.00899912 -0.012236  \n",
      "                                      -0.01240105]                                                   \n",
      "                                     [ 0.02786899  0.05061176  0.05996857 ... -0.00729707 -0.01415646\n",
      "                                       0.03568567]                                                   \n",
      "                                     [ 0.08358804 -0.31501951 -0.18772556 ...  0.05435881  0.08129264\n",
      "                                       0.08261156]]                                                  \n",
      "Epoch 60, loss: 12.055412\n",
      "== W == -2.0653755178025754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[ 7.73227457e-02 -2.30065415e-02  2.14229926e-02 ... -1.59411783e-01 [4 9 8 ... 2 2 3]\n",
      "                            1.22374414e-01  1.38199722e-01]                                                     \n",
      "                          [-3.27093585e-02  1.60740562e-04 -2.39394607e-02 ...  1.56645294e-01                  \n",
      "                           -6.79124419e-02 -1.27089679e-01]                                                     \n",
      "                          [ 7.13852017e-02 -1.77030748e-03 -1.03285862e-01 ...  1.07781498e-01                  \n",
      "                            6.72807991e-02 -3.90895770e-02]                                                     \n",
      "                          ...                                                                                   \n",
      "                          [ 2.58427683e-02 -2.99602291e-02  3.44059470e-02 ... -1.57628498e-01                  \n",
      "                            4.50276156e-02  1.03337888e-01]                                                     \n",
      "                          [ 8.58055872e-02 -2.65478987e-02  1.44185813e-01 ... -1.78016850e-01                  \n",
      "                            9.98696544e-02  1.36522371e-01]                                                     \n",
      "                          [-7.32313226e-02 -3.50531994e-02 -1.41629275e-01 ...  3.05647945e-01                  \n",
      "                           -1.21559536e-01 -1.84569277e-01]]                                                    \n",
      "soft max = [[1.19108804e-05 1.07738620e-05 1.12633326e-05 ... 9.40007596e-06\n",
      "             1.24597565e-05 1.26585044e-05]                                 \n",
      "            [1.06698307e-05 1.10263768e-05 1.07638155e-05 ... 1.28941645e-05\n",
      "             1.03007542e-05 9.70886974e-06]                                 \n",
      "            [1.18403685e-05 1.10051049e-05 9.94275049e-06 ... 1.22792525e-05\n",
      "             1.17918705e-05 1.06019715e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.13132242e-05 1.06992038e-05 1.14105173e-05 ... 9.41685394e-06\n",
      "             1.15323620e-05 1.22248094e-05]                                 \n",
      "            [1.20123482e-05 1.07357753e-05 1.27345066e-05 ... 9.22680379e-06\n",
      "             1.21824843e-05 1.26372895e-05]                                 \n",
      "            [1.02461111e-05 1.06448516e-05 9.56872797e-06 ... 1.49659482e-05\n",
      "             9.76270992e-06 9.16654347e-06]]                                \n",
      "log =  102872.95585890462\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.43032842876718 [[ 1.19108804e-05  1.07738620e-05  1.12633326e-05 ...  9.40007596e-06 \n",
      "                                                  1.24597565e-05  1.26585044e-05]                                    \n",
      "                                                [ 1.06698307e-05  1.10263768e-05  1.07638155e-05 ...  1.28941645e-05 \n",
      "                                                  1.03007542e-05 -1.01402241e-04]                                    \n",
      "                                                [ 1.18403685e-05  1.10051049e-05  9.94275049e-06 ...  1.22792525e-05 \n",
      "                                                 -9.93192406e-05  1.06019715e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [ 1.13132242e-05  1.06992038e-05 -9.97005938e-05 ...  9.41685394e-06 \n",
      "                                                  1.15323620e-05  1.22248094e-05]                                    \n",
      "                                                [ 1.20123482e-05  1.07357753e-05 -9.83766045e-05 ...  9.22680379e-06 \n",
      "                                                  1.21824843e-05  1.26372895e-05]                                    \n",
      "                                                [ 1.02461111e-05  1.06448516e-05  9.56872797e-06 ...  1.49659482e-05 \n",
      "                                                  9.76270992e-06  9.16654347e-06]]                                   \n",
      "loss , grand (prediction), grad by W =  11.43032842876718 [[ 1.19108804e-05  1.07738620e-05  1.12633326e-05 ...  9.40007596e-06 [[-0.00111841 -0.00030873 -0.0002097  ...  0.00215348 -0.00233574 \n",
      "                                                             1.24597565e-05  1.26585044e-05]                                      -0.00115396]                                                    \n",
      "                                                           [ 1.06698307e-05  1.10263768e-05  1.07638155e-05 ...  1.28941645e-05  [-0.00167709  0.00042886  0.00031426 ...  0.0019045  -0.00265849 \n",
      "                                                             1.03007542e-05 -1.01402241e-04]                                      -0.00124315]                                                    \n",
      "                                                           [ 1.18403685e-05  1.10051049e-05  9.94275049e-06 ...  1.22792525e-05  [-0.00198807  0.00100281  0.00088006 ...  0.00187035 -0.00274968 \n",
      "                                                            -9.93192406e-05  1.06019715e-05]                                      -0.00189173]                                                    \n",
      "                                                           ...                                                                   ...                                                              \n",
      "                                                           [ 1.13132242e-05  1.06992038e-05 -9.97005938e-05 ...  9.41685394e-06  [-0.00087652  0.00074355 -0.00102014 ...  0.00291858 -0.00200409 \n",
      "                                                             1.15323620e-05  1.22248094e-05]                                      -0.00194239]                                                    \n",
      "                                                           [ 1.20123482e-05  1.07357753e-05 -9.83766045e-05 ...  9.22680379e-06  [-0.00023134 -0.00043836 -0.00124421 ...  0.0031969  -0.0014775  \n",
      "                                                             1.21824843e-05  1.26372895e-05]                                      -0.00193931]                                                    \n",
      "                                                           [ 1.02461111e-05  1.06448516e-05  9.56872797e-06 ...  1.49659482e-05  [ 0.03396552 -0.10040955 -0.05059351 ...  0.02406773  0.03382003 \n",
      "                                                             9.76270992e-06  9.16654347e-06]]                                      0.03841443]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.6382453577851249 [[-0.01570493 -0.02935068 -0.04619445 ... -0.01616073 -0.02397932\n",
      "                                       0.00608512]                                                   \n",
      "                                     [ 0.00884095 -0.01472022 -0.06202771 ...  0.10149563  0.0004927 \n",
      "                                       0.05002549]                                                   \n",
      "                                     [-0.00280145  0.0828525  -0.05885534 ...  0.02237516 -0.0913522 \n",
      "                                      -0.04519474]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.06102931  0.02347273 -0.03487197 ...  0.00911785 -0.01237816\n",
      "                                      -0.01254421]                                                   \n",
      "                                     [ 0.02814551  0.05111349  0.06055593 ... -0.00733854 -0.01431256\n",
      "                                       0.03602342]                                                   \n",
      "                                     [ 0.0847637  -0.31917313 -0.19010829 ...  0.05514289  0.08244379\n",
      "                                       0.08382179]]                                                  \n",
      "Epoch 61, loss: 12.068574\n",
      "== W == -2.09857140753397\n",
      "enter of the function =  [[ 0.00740138  0.0660192  -0.0685147  ... -0.02111375  0.03594728 [2 2 9 ... 8 2 0]\n",
      "                           -0.00824582]                                                                     \n",
      "                          [-0.08196057  0.11877159 -0.05217224 ...  0.11407188 -0.30989636                  \n",
      "                           -0.13566351]                                                                     \n",
      "                          [-0.02247591 -0.12721015 -0.06688965 ...  0.06535699 -0.06071151                  \n",
      "                           -0.08938667]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.07102015 -0.08152785 -0.08598176 ...  0.1610044  -0.07376701                  \n",
      "                           -0.0594399 ]                                                                     \n",
      "                          [ 0.10475558 -0.01883136  0.07002777 ... -0.13328886  0.16823616                  \n",
      "                            0.13771739]                                                                     \n",
      "                          [ 0.09235683 -0.01357469 -0.02224008 ... -0.03556915  0.07478013                  \n",
      "                            0.04343983]]                                                                    \n",
      "soft max = [[1.11028382e-05 1.17731156e-05 1.02911540e-05 ... 1.07907107e-05\n",
      "             1.14243458e-05 1.09304620e-05]                                 \n",
      "            [1.01537065e-05 1.24108486e-05 1.04607185e-05 ... 1.23526581e-05\n",
      "             8.08413118e-06 9.62280579e-06]                                 \n",
      "            [1.07760220e-05 9.70449556e-06 1.03078912e-05 ... 1.17653219e-05\n",
      "             1.03717719e-05 1.00785834e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.02654022e-05 1.01581012e-05 1.01129586e-05 ... 1.29462192e-05\n",
      "             1.02372434e-05 1.03849692e-05]                                 \n",
      "            [1.22381114e-05 1.08153674e-05 1.18204036e-05 ... 9.64568372e-06\n",
      "             1.30401825e-05 1.26482235e-05]                                 \n",
      "            [1.20873110e-05 1.08723699e-05 1.07785636e-05 ... 1.06358486e-05\n",
      "             1.18767121e-05 1.15102647e-05]]                                \n",
      "log =  102877.7906103422\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.430865623371355 [[ 1.11028382e-05  1.17731156e-05 -1.00819957e-04 ...  1.07907107e-05 \n",
      "                                                   1.14243458e-05  1.09304620e-05]                                    \n",
      "                                                 [ 1.01537065e-05  1.24108486e-05 -1.00650393e-04 ...  1.23526581e-05 \n",
      "                                                   8.08413118e-06  9.62280579e-06]                                    \n",
      "                                                 [ 1.07760220e-05  9.70449556e-06  1.03078912e-05 ...  1.17653219e-05 \n",
      "                                                   1.03717719e-05 -1.01032528e-04]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.02654022e-05  1.01581012e-05  1.01129586e-05 ...  1.29462192e-05 \n",
      "                                                  -1.00873868e-04  1.03849692e-05]                                    \n",
      "                                                 [ 1.22381114e-05  1.08153674e-05 -9.92907075e-05 ...  9.64568372e-06 \n",
      "                                                   1.30401825e-05  1.26482235e-05]                                    \n",
      "                                                 [-9.90238001e-05  1.08723699e-05  1.07785636e-05 ...  1.06358486e-05 \n",
      "                                                   1.18767121e-05  1.15102647e-05]]                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss , grand (prediction), grad by W =  11.430865623371355 [[ 1.11028382e-05  1.17731156e-05 -1.00819957e-04 ...  1.07907107e-05 [[-0.00113502 -0.00031071 -0.00022061 ...  0.00219474 -0.00236238 \n",
      "                                                              1.14243458e-05  1.09304620e-05]                                      -0.00118045]                                                    \n",
      "                                                            [ 1.01537065e-05  1.24108486e-05 -1.00650393e-04 ...  1.23526581e-05  [-0.00169394  0.00042707  0.00030331 ...  0.00194751 -0.00268626 \n",
      "                                                              8.08413118e-06  9.62280579e-06]                                      -0.00127094]                                                    \n",
      "                                                            [ 1.07760220e-05  9.70449556e-06  1.03078912e-05 ...  1.17653219e-05  [-0.00200473  0.00100073  0.00086885 ...  0.0019157  -0.00277785 \n",
      "                                                              1.03717719e-05 -1.01032528e-04]                                      -0.00192114]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.02654022e-05  1.01581012e-05  1.01129586e-05 ...  1.29462192e-05  [-0.00089161  0.00074502 -0.00103295 ...  0.00296412 -0.00202899 \n",
      "                                                             -1.00873868e-04  1.03849692e-05]                                      -0.00196998]                                                    \n",
      "                                                            [ 1.22381114e-05  1.08153674e-05 -9.92907075e-05 ...  9.64568372e-06  [-0.00024604 -0.00043754 -0.00125694 ...  0.00324425 -0.00150249 \n",
      "                                                              1.30401825e-05  1.26482235e-05]                                      -0.00196824]                                                    \n",
      "                                                            [-9.90238001e-05  1.08723699e-05  1.07785636e-05 ...  1.06358486e-05  [ 0.03395224 -0.10047847 -0.05064135 ...  0.02408714  0.03381768 \n",
      "                                                              1.18767121e-05  1.15102647e-05]]                                      0.03841702]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.6511401817154743 [[-0.01587316 -0.02964728 -0.04665849 ... -0.0163008  -0.02424247\n",
      "                                       0.00613443]                                                   \n",
      "                                     [ 0.00891259 -0.01486313 -0.06264484 ...  0.10252963  0.00047104\n",
      "                                       0.05051332]                                                   \n",
      "                                     [-0.00284935  0.08369105 -0.0594351  ...  0.02261762 -0.09229322\n",
      "                                      -0.0456656 ]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.06164837  0.02371489 -0.03523089 ...  0.00923821 -0.01252198\n",
      "                                      -0.01268908]                                                   \n",
      "                                     [ 0.02842465  0.05162024  0.06114905 ... -0.00737996 -0.01447046\n",
      "                                       0.03636426]                                                   \n",
      "                                     [ 0.08595099 -0.32336895 -0.1925153  ...  0.055935    0.08360642\n",
      "                                       0.08504415]]                                                  \n",
      "Epoch 62, loss: 12.082006\n",
      "== W == -2.1322319284407154\n",
      "enter of the function =  [[ 0.05559562 -0.0355902  -0.02471083 ... -0.08476306  0.04244707 [0 6 6 ... 2 5 5]\n",
      "                            0.0837764 ]                                                                     \n",
      "                          [ 0.03422089 -0.03123448 -0.03379841 ...  0.11608727  0.00539677                  \n",
      "                           -0.0408039 ]                                                                     \n",
      "                          [ 0.12131413  0.0285646  -0.03664937 ... -0.12362527  0.13458455                  \n",
      "                            0.132176  ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.06274602 -0.0026841  -0.03167911 ... -0.114796    0.12855087                  \n",
      "                            0.10072576]                                                                     \n",
      "                          [-0.1095767  -0.04321631 -0.14029572 ...  0.23713462 -0.1282358                   \n",
      "                           -0.11941203]                                                                     \n",
      "                          [ 0.03986942 -0.01551399  0.0609338  ... -0.07046659  0.06506964                  \n",
      "                            0.08940737]]                                                                    \n",
      "soft max = [[1.16470432e-05 1.06319809e-05 1.07482817e-05 ... 1.01218217e-05\n",
      "             1.14949039e-05 1.19799345e-05]                                 \n",
      "            [1.14007325e-05 1.06783918e-05 1.06510482e-05 ... 1.23733379e-05\n",
      "             1.10768073e-05 1.05766932e-05]                                 \n",
      "            [1.24381810e-05 1.13364287e-05 1.06207258e-05 ... 9.73601067e-06\n",
      "             1.26043409e-05 1.25740193e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.17306226e-05 1.09876578e-05 1.06736449e-05 ... 9.82235312e-06\n",
      "             1.25285193e-05 1.21847172e-05]                                 \n",
      "            [9.87375303e-06 1.05512085e-05 9.57505230e-06 ... 1.39655187e-05\n",
      "             9.69122579e-06 9.77711732e-06]                                 \n",
      "            [1.14653121e-05 1.08475878e-05 1.17093835e-05 ... 1.02675675e-05\n",
      "             1.17579119e-05 1.20475834e-05]]                                \n",
      "log =  102882.76039699308\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.43141782188812 [[-9.94640679e-05  1.06319809e-05  1.07482817e-05 ...  1.01218217e-05 \n",
      "                                                  1.14949039e-05  1.19799345e-05]                                    \n",
      "                                                [ 1.14007325e-05  1.06783918e-05  1.06510482e-05 ...  1.23733379e-05 \n",
      "                                                  1.10768073e-05  1.05766932e-05]                                    \n",
      "                                                [ 1.24381810e-05  1.13364287e-05  1.06207258e-05 ...  9.73601067e-06 \n",
      "                                                  1.26043409e-05  1.25740193e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [ 1.17306226e-05  1.09876578e-05 -1.00437466e-04 ...  9.82235312e-06 \n",
      "                                                  1.25285193e-05  1.21847172e-05]                                    \n",
      "                                                [ 9.87375303e-06  1.05512085e-05  9.57505230e-06 ...  1.39655187e-05 \n",
      "                                                  9.69122579e-06  9.77711732e-06]                                    \n",
      "                                                [ 1.14653121e-05  1.08475878e-05  1.17093835e-05 ...  1.02675675e-05 \n",
      "                                                  1.17579119e-05  1.20475834e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.43141782188812 [[-9.94640679e-05  1.06319809e-05  1.07482817e-05 ...  1.01218217e-05 [[-0.00115189 -0.00031272 -0.00023169 ...  0.00223683 -0.00238943 \n",
      "                                                             1.14949039e-05  1.19799345e-05]                                      -0.00120736]                                                    \n",
      "                                                           [ 1.14007325e-05  1.06783918e-05  1.06510482e-05 ...  1.23733379e-05  [-0.00171106  0.00042525  0.00029219 ...  0.00199137 -0.00271445 \n",
      "                                                             1.10768073e-05  1.05766932e-05]                                      -0.00129917]                                                    \n",
      "                                                           [ 1.24381810e-05  1.13364287e-05  1.06207258e-05 ...  9.73601067e-06  [-0.00202165  0.00099863  0.00085747 ...  0.00196195 -0.00280644 \n",
      "                                                             1.26043409e-05  1.25740193e-05]                                      -0.00195101]                                                    \n",
      "                                                           ...                                                                   ...                                                              \n",
      "                                                           [ 1.17306226e-05  1.09876578e-05 -1.00437466e-04 ...  9.82235312e-06  [-0.00090696  0.00074648 -0.00104596 ...  0.00301056 -0.00205429 \n",
      "                                                             1.25285193e-05  1.21847172e-05]                                      -0.00199801]                                                    \n",
      "                                                           [ 9.87375303e-06  1.05512085e-05  9.57505230e-06 ...  1.39655187e-05  [-0.00026098 -0.00043673 -0.00126986 ...  0.00329252 -0.00152786 \n",
      "                                                             9.69122579e-06  9.77711732e-06]                                      -0.00199761]                                                    \n",
      "                                                           [ 1.14653121e-05  1.08475878e-05  1.17093835e-05 ...  1.02675675e-05  [ 0.03393809 -0.10054887 -0.05069039 ...  0.02410704  0.03381483 \n",
      "                                                             1.17579119e-05  1.20475834e-05]]                                      0.03841925]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.6642960146162251 [[-0.01604324 -0.02994686 -0.04712728 ... -0.01644186 -0.02450852\n",
      "                                       0.00618397]                                                   \n",
      "                                     [ 0.00898478 -0.01500749 -0.06326826 ...  0.1035744   0.00044889\n",
      "                                       0.05100574]                                                   \n",
      "                                     [-0.00289789  0.08453797 -0.06002076 ...  0.02286295 -0.09324393\n",
      "                                      -0.04614147]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.06227377  0.02395949 -0.03559353 ...  0.00936023 -0.01266749\n",
      "                                      -0.01283567]                                                   \n",
      "                                     [ 0.02870644  0.05213207  0.06174797 ... -0.00742131 -0.01463019\n",
      "                                       0.03670822]                                                   \n",
      "                                     [ 0.08715003 -0.32760743 -0.19494687 ...  0.05673522  0.08478066\n",
      "                                       0.08627876]]                                                  \n",
      "Epoch 63, loss: 12.095714\n",
      "== W == -2.166360525357396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[-0.00508903 -0.04022396  0.01426911 ... -0.06455441  0.03633101 [6 8 5 ... 3 2 2]\n",
      "                            0.06743616]                                                                     \n",
      "                          [-0.02852221 -0.07579184 -0.06800869 ...  0.17198352 -0.07064037                  \n",
      "                           -0.08555563]                                                                     \n",
      "                          [-0.01696568 -0.01342331 -0.05316013 ...  0.02476946  0.04100162                  \n",
      "                            0.06061466]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.05767375 -0.00845514  0.05285946 ... -0.1746055   0.08888821                  \n",
      "                            0.1085301 ]                                                                     \n",
      "                          [ 0.14126656 -0.05175113  0.06643243 ... -0.33190286  0.23977391                  \n",
      "                            0.26379278]                                                                     \n",
      "                          [ 0.0273644  -0.02199005 -0.01712678 ... -0.10471199  0.07340922                  \n",
      "                            0.07690371]]                                                                    \n",
      "soft max = [[1.09573674e-05 1.05790657e-05 1.11715480e-05 ... 1.03247783e-05\n",
      "             1.14207524e-05 1.17815793e-05]                                 \n",
      "            [1.07035864e-05 1.02094038e-05 1.02891751e-05 ... 1.30800033e-05\n",
      "             1.02621329e-05 1.01102064e-05]                                 \n",
      "            [1.08280003e-05 1.08664251e-05 1.04430945e-05 ... 1.12894712e-05\n",
      "             1.14742191e-05 1.17014847e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.16671223e-05 1.09205457e-05 1.16110884e-05 ... 9.24881628e-06\n",
      "             1.20370488e-05 1.22758163e-05]                                 \n",
      "            [1.26843333e-05 1.04578192e-05 1.17697597e-05 ... 7.90265054e-06\n",
      "             1.39974474e-05 1.43377204e-05]                                 \n",
      "            [1.13188047e-05 1.07737328e-05 1.08262560e-05 ... 9.91837487e-06\n",
      "             1.18521619e-05 1.18936516e-05]]                                \n",
      "log =  102887.86982247347\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.431985535830385 [[ 1.09573674e-05  1.05790657e-05  1.11715480e-05 ...  1.03247783e-05 \n",
      "                                                   1.14207524e-05  1.17815793e-05]                                    \n",
      "                                                 [ 1.07035864e-05  1.02094038e-05  1.02891751e-05 ...  1.30800033e-05 \n",
      "                                                  -1.00848978e-04  1.01102064e-05]                                    \n",
      "                                                 [ 1.08280003e-05  1.08664251e-05  1.04430945e-05 ...  1.12894712e-05 \n",
      "                                                   1.14742191e-05  1.17014847e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.16671223e-05  1.09205457e-05  1.16110884e-05 ...  9.24881628e-06 \n",
      "                                                   1.20370488e-05  1.22758163e-05]                                    \n",
      "                                                 [ 1.26843333e-05  1.04578192e-05 -9.93413514e-05 ...  7.90265054e-06 \n",
      "                                                   1.39974474e-05  1.43377204e-05]                                    \n",
      "                                                 [ 1.13188047e-05  1.07737328e-05 -1.00284855e-04 ...  9.91837487e-06 \n",
      "                                                   1.18521619e-05  1.18936516e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.431985535830385 [[ 1.09573674e-05  1.05790657e-05  1.11715480e-05 ...  1.03247783e-05 [[-0.00116901 -0.00031476 -0.00024295 ...  0.00227976 -0.00241688 \n",
      "                                                              1.14207524e-05  1.17815793e-05]                                      -0.00123469]                                                    \n",
      "                                                            [ 1.07035864e-05  1.02094038e-05  1.02891751e-05 ...  1.30800033e-05  [-0.00172843  0.0004234   0.0002809  ...  0.00203611 -0.00274306 \n",
      "                                                             -1.00848978e-04  1.01102064e-05]                                      -0.00132783]                                                    \n",
      "                                                            [ 1.08280003e-05  1.08664251e-05  1.04430945e-05 ...  1.12894712e-05  [-0.00203884  0.00099649  0.00084591 ...  0.00200911 -0.00283547 \n",
      "                                                              1.14742191e-05  1.17014847e-05]                                      -0.00198133]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.16671223e-05  1.09205457e-05  1.16110884e-05 ...  9.24881628e-06  [-0.00092254  0.00074795 -0.00105915 ...  0.00305792 -0.00207998 \n",
      "                                                              1.20370488e-05  1.22758163e-05]                                      -0.00202647]                                                    \n",
      "                                                            [ 1.26843333e-05  1.04578192e-05 -9.93413514e-05 ...  7.90265054e-06  [-0.00027616 -0.00043592 -0.00128297 ...  0.00334174 -0.00155364 \n",
      "                                                              1.39974474e-05  1.43377204e-05]                                      -0.00202743]                                                    \n",
      "                                                            [ 1.13188047e-05  1.07737328e-05 -1.00284855e-04 ...  9.91837487e-06  [ 0.03392304 -0.10062082 -0.05074068 ...  0.02412745  0.03381145 \n",
      "                                                              1.18521619e-05  1.18936516e-05]]                                      0.0384211 ]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.6777181609722607 [[-0.01621519 -0.03024945 -0.04760087 ... -0.01658391 -0.0247775 \n",
      "                                       0.00623374]                                                   \n",
      "                                     [ 0.00905751 -0.01515332 -0.06389802 ...  0.10463006  0.00042623\n",
      "                                       0.05150281]                                                   \n",
      "                                     [-0.00294708  0.08539334 -0.06061239 ...  0.0231112  -0.09420444\n",
      "                                      -0.0466224 ]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.06290558  0.02420655 -0.03595993 ...  0.00948394 -0.01281471\n",
      "                                      -0.01298401]                                                   \n",
      "                                     [ 0.02899089  0.05264902  0.06235275 ... -0.0074626  -0.01479177\n",
      "                                       0.03705533]                                                   \n",
      "                                     [ 0.08836091 -0.33188899 -0.19740324 ...  0.05754364  0.08596662\n",
      "                                       0.08752574]]                                                  \n",
      "Epoch 64, loss: 12.109704\n",
      "== W == -2.2009604957603073\n",
      "enter of the function =  [[ 0.01826105 -0.01833552 -0.02659059 ... -0.01283648 -0.03126195 [1 9 1 ... 3 2 1]\n",
      "                           -0.02237089]                                                                     \n",
      "                          [ 0.11209252 -0.07234183  0.06644938 ... -0.23507195  0.17359249                  \n",
      "                            0.19257409]                                                                     \n",
      "                          [ 0.02507742 -0.06396179 -0.05031274 ...  0.00287769 -0.02370047                  \n",
      "                            0.00699786]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.04627308 -0.02651031  0.04768096 ... -0.10072955  0.07521279                  \n",
      "                            0.11308513]                                                                     \n",
      "                          [ 0.1301591  -0.02458612 -0.034593   ... -0.07864176  0.16299199                  \n",
      "                            0.11219994]                                                                     \n",
      "                          [-0.12564269 -0.11732669 -0.09670333 ...  0.25488288 -0.16940102                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.20164639]]                                                                    \n",
      "soft max = [[1.12120939e-05 1.08091871e-05 1.07203238e-05 ... 1.08687910e-05\n",
      "             1.06703621e-05 1.07656560e-05]                                 \n",
      "            [1.23150793e-05 1.02409064e-05 1.17656155e-05 ... 8.70293222e-06\n",
      "             1.30962305e-05 1.33471923e-05]                                 \n",
      "            [1.12887807e-05 1.03270861e-05 1.04690073e-05 ... 1.10409341e-05\n",
      "             1.07513517e-05 1.10865184e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.15306076e-05 1.07211845e-05 1.15468528e-05 ... 9.95427791e-06\n",
      "             1.18691755e-05 1.23273095e-05]                                 \n",
      "            [1.25395927e-05 1.07418340e-05 1.06348778e-05 ... 1.01765922e-05\n",
      "             1.29581372e-05 1.23164022e-05]                                 \n",
      "            [9.70934930e-06 9.79042891e-06 9.99443689e-06 ... 1.42052957e-05\n",
      "             9.29364594e-06 8.99874900e-06]]                                \n",
      "log =  102893.12367222176\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.43256929691353 [[ 1.12120939e-05 -1.00301924e-04  1.07203238e-05 ...  1.08687910e-05 \n",
      "                                                  1.06703621e-05  1.07656560e-05]                                    \n",
      "                                                [ 1.23150793e-05  1.02409064e-05  1.17656155e-05 ...  8.70293222e-06 \n",
      "                                                  1.30962305e-05 -9.77639188e-05]                                    \n",
      "                                                [ 1.12887807e-05 -1.00784025e-04  1.04690073e-05 ...  1.10409341e-05 \n",
      "                                                  1.07513517e-05  1.10865184e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [ 1.15306076e-05  1.07211845e-05  1.15468528e-05 ...  9.95427791e-06 \n",
      "                                                  1.18691755e-05  1.23273095e-05]                                    \n",
      "                                                [ 1.25395927e-05  1.07418340e-05 -1.00476233e-04 ...  1.01765922e-05 \n",
      "                                                  1.29581372e-05  1.23164022e-05]                                    \n",
      "                                                [ 9.70934930e-06 -1.01320682e-04  9.99443689e-06 ...  1.42052957e-05 \n",
      "                                                  9.29364594e-06  8.99874900e-06]]                                   \n",
      "loss , grand (prediction), grad by W =  11.43256929691353 [[ 1.12120939e-05 -1.00301924e-04  1.07203238e-05 ...  1.08687910e-05 [[-0.00118638 -0.00031683 -0.00025438 ...  0.00232355 -0.00244474 \n",
      "                                                             1.06703621e-05  1.07656560e-05]                                      -0.00126244]                                                    \n",
      "                                                           [ 1.23150793e-05  1.02409064e-05  1.17656155e-05 ...  8.70293222e-06  [-0.00174606  0.00042153  0.00026942 ...  0.00208175 -0.00277209 \n",
      "                                                             1.30962305e-05 -9.77639188e-05]                                      -0.00135693]                                                    \n",
      "                                                           [ 1.12887807e-05 -1.00784025e-04  1.04690073e-05 ...  1.10409341e-05  [-0.00205628  0.00099432  0.00083416 ...  0.00205721 -0.00286492 \n",
      "                                                             1.07513517e-05  1.10865184e-05]                                      -0.00201212]                                                    \n",
      "                                                           ...                                                                   ...                                                              \n",
      "                                                           [ 1.15306076e-05  1.07211845e-05  1.15468528e-05 ...  9.95427791e-06  [-0.00093837  0.00074941 -0.00107254 ...  0.00310622 -0.00210607 \n",
      "                                                             1.18691755e-05  1.23273095e-05]                                      -0.00205537]                                                    \n",
      "                                                           [ 1.25395927e-05  1.07418340e-05 -1.00476233e-04 ...  1.01765922e-05  [-0.00029158 -0.00043513 -0.00129628 ...  0.00339193 -0.00157982 \n",
      "                                                             1.29581372e-05  1.23164022e-05]                                      -0.00205771]                                                    \n",
      "                                                           [ 9.70934930e-06 -1.01320682e-04  9.99443689e-06 ...  1.42052957e-05  [ 0.03390703 -0.10069435 -0.05079226 ...  0.02414836  0.03380753 \n",
      "                                                             9.29364594e-06  8.99874900e-06]]                                      0.03842256]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.6914120340520689 [[-0.01638904 -0.0305551  -0.04807931 ... -0.01672696 -0.02504944\n",
      "                                       0.00628373]                                                   \n",
      "                                     [ 0.0091308  -0.01530062 -0.06453419 ...  0.10569672  0.00040306\n",
      "                                       0.05200456]                                                   \n",
      "                                     [-0.00299694  0.08625723 -0.06121006 ...  0.02336241 -0.09517484\n",
      "                                      -0.04710843]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.06354386  0.0244561  -0.03633012 ...  0.00960936 -0.01296365\n",
      "                                      -0.01313411]                                                   \n",
      "                                     [ 0.02927804  0.05317115  0.06296345 ... -0.00750381 -0.01495522\n",
      "                                       0.0374056 ]                                                   \n",
      "                                     [ 0.08958375 -0.33621409 -0.19988468 ...  0.05836035  0.0871644 \n",
      "                                       0.08878521]]                                                  \n",
      "Epoch 65, loss: 12.123981\n",
      "== W == -2.236034976696655\n",
      "enter of the function =  [[ 0.0813443  -0.05189123 -0.02917333 ... -0.04791024  0.14218865 [9 4 8 ... 5 9 2]\n",
      "                            0.05652684]                                                                     \n",
      "                          [-0.08471984 -0.03568812 -0.10871999 ...  0.21710842 -0.11465152                  \n",
      "                           -0.14253895]                                                                     \n",
      "                          [ 0.01664878  0.02156044 -0.01497096 ... -0.1543357   0.10182354                  \n",
      "                            0.15774566]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.05080849 -0.00101506 -0.08753973 ... -0.09047343  0.06668263                  \n",
      "                            0.00837864]                                                                     \n",
      "                          [-0.03782333  0.01462725 -0.0513143  ...  0.08057971 -0.04681999                  \n",
      "                           -0.14171176]                                                                     \n",
      "                          [ 0.11603957  0.04046075  0.07339506 ... -0.09122883  0.13270191                  \n",
      "                            0.12873623]]                                                                    \n",
      "soft max = [[1.19376007e-05 1.04484914e-05 1.06885759e-05 ... 1.04901697e-05\n",
      "             1.26864880e-05 1.16449857e-05]                                 \n",
      "            [1.01110511e-05 1.06191685e-05 9.87127326e-06 ... 1.36734673e-05\n",
      "             9.81289476e-06 9.54301897e-06]                                 \n",
      "            [1.11897437e-05 1.12448391e-05 1.08414622e-05 ... 9.43110377e-06\n",
      "             1.21845941e-05 1.28853950e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.15785857e-05 1.09938252e-05 1.00825793e-05 ... 1.00530433e-05\n",
      "             1.17638523e-05 1.10975845e-05]                                 \n",
      "            [1.05965185e-05 1.11671462e-05 1.04545212e-05 ... 1.19284768e-05\n",
      "             1.05016129e-05 9.55091607e-06]                                 \n",
      "            [1.23590477e-05 1.14593912e-05 1.18430820e-05 ... 1.00454521e-05\n",
      "             1.25667036e-05 1.25169668e-05]]                                \n",
      "log =  102898.5269219651\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.433169657996121 [[ 1.19376007e-05  1.04484914e-05  1.06885759e-05 ...  1.04901697e-05 \n",
      "                                                   1.26864880e-05 -9.94661254e-05]                                    \n",
      "                                                 [ 1.01110511e-05  1.06191685e-05  9.87127326e-06 ...  1.36734673e-05 \n",
      "                                                   9.81289476e-06  9.54301897e-06]                                    \n",
      "                                                 [ 1.11897437e-05  1.12448391e-05  1.08414622e-05 ...  9.43110377e-06 \n",
      "                                                  -9.89265170e-05  1.28853950e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.15785857e-05  1.09938252e-05  1.00825793e-05 ...  1.00530433e-05 \n",
      "                                                   1.17638523e-05  1.10975845e-05]                                    \n",
      "                                                 [ 1.05965185e-05  1.11671462e-05  1.04545212e-05 ...  1.19284768e-05 \n",
      "                                                   1.05016129e-05 -1.01560195e-04]                                    \n",
      "                                                 [ 1.23590477e-05  1.14593912e-05 -9.92680291e-05 ...  1.00454521e-05 \n",
      "                                                   1.25667036e-05  1.25169668e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.433169657996121 [[ 1.19376007e-05  1.04484914e-05  1.06885759e-05 ...  1.04901697e-05 [[-0.00120401 -0.00031893 -0.00026598 ...  0.00236822 -0.00247302 \n",
      "                                                              1.26864880e-05 -9.94661254e-05]                                      -0.00129062]                                                    \n",
      "                                                            [ 1.01110511e-05  1.06191685e-05  9.87127326e-06 ...  1.36734673e-05  [-0.00176396  0.00041962  0.00025777 ...  0.0021283  -0.00280155 \n",
      "                                                              9.81289476e-06  9.54301897e-06]                                      -0.00138648]                                                    \n",
      "                                                            [ 1.11897437e-05  1.12448391e-05  1.08414622e-05 ...  9.43110377e-06  [-0.00207399  0.00099213  0.00082223 ...  0.00210626 -0.00289482 \n",
      "                                                             -9.89265170e-05  1.28853950e-05]                                      -0.00204338]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.15785857e-05  1.09938252e-05  1.00825793e-05 ...  1.00530433e-05  [-0.00095446  0.00075086 -0.00108612 ...  0.00315548 -0.00213257 \n",
      "                                                              1.17638523e-05  1.10975845e-05]                                      -0.00208472]                                                    \n",
      "                                                            [ 1.05965185e-05  1.11671462e-05  1.04545212e-05 ...  1.19284768e-05  [-0.00030726 -0.00043434 -0.00130978 ...  0.00344311 -0.00160641 \n",
      "                                                              1.05016129e-05 -1.01560195e-04]                                      -0.00208845]                                                    \n",
      "                                                            [ 1.23590477e-05  1.14593912e-05 -9.92680291e-05 ...  1.00454521e-05  [ 0.03389003 -0.10076951 -0.05084516 ...  0.02416982  0.03380303 \n",
      "                                                              1.25667036e-05  1.25169668e-05]]                                      0.03842361]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.7053831581871439 [[-0.01656479 -0.03086381 -0.04856265 ... -0.01687099 -0.02532438\n",
      "                                       0.00633394]                                                   \n",
      "                                     [ 0.00920465 -0.01544941 -0.06517684 ...  0.1067745   0.00037937\n",
      "                                       0.05251103]                                                   \n",
      "                                     [-0.00304748  0.08712975 -0.06181382 ...  0.0236166  -0.09615523\n",
      "                                      -0.04759964]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.06418868  0.02470815 -0.03670414 ...  0.00973652 -0.01311435\n",
      "                                      -0.01328601]                                                   \n",
      "                                     [ 0.02956791  0.05369851  0.06358012 ... -0.00754493 -0.01512057\n",
      "                                       0.03775908]                                                   \n",
      "                                     [ 0.09081865 -0.34058317 -0.20239145 ...  0.05918544  0.08837412\n",
      "                                       0.09005729]]                                                  \n",
      "Epoch 66, loss: 12.138553\n",
      "== W == -2.271586930868334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[ 0.04120164 -0.01731776  0.03681596 ... -0.06995598  0.05454184 [5 0 5 ... 2 7 8]\n",
      "                            0.05678062]                                                                     \n",
      "                          [ 0.04238968 -0.04899163  0.03559544 ... -0.14664433  0.07045193                  \n",
      "                            0.10526431]                                                                     \n",
      "                          [-0.15666476 -0.03581659 -0.06589141 ...  0.30820063 -0.21947243                  \n",
      "                           -0.19764051]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.09265226 -0.02815248  0.01617591 ... -0.00145815  0.1505295                   \n",
      "                           -0.02618556]                                                                     \n",
      "                          [-0.11183751 -0.13124013  0.00660545 ...  0.17021463 -0.11927261                  \n",
      "                           -0.15343   ]                                                                     \n",
      "                          [ 0.05865146  0.09426025  0.06317451 ... -0.14126167  0.05976991                  \n",
      "                            0.08223788]]                                                                    \n",
      "soft max = [[1.14633252e-05 1.08117491e-05 1.14131608e-05 ... 1.02573571e-05\n",
      "             1.16172728e-05 1.16433105e-05]                                 \n",
      "            [1.14769522e-05 1.04746658e-05 1.13992393e-05 ... 9.50014304e-06\n",
      "             1.18035829e-05 1.22217298e-05]                                 \n",
      "            [9.40542287e-06 1.06135830e-05 1.02991336e-05 ... 1.49715512e-05\n",
      "             8.83285914e-06 9.02781779e-06]                                 \n",
      "            ...                                                             \n",
      "            [1.20685566e-05 1.06952391e-05 1.11800070e-05 ... 1.09845863e-05\n",
      "             1.27876606e-05 1.07162966e-05]                                 \n",
      "            [9.83663493e-06 9.64761811e-06 1.10735196e-05 ... 1.30418813e-05\n",
      "             9.76376981e-06 9.43589648e-06]                                 \n",
      "            [1.16651137e-05 1.20879784e-05 1.17179950e-05 ... 9.55141697e-06\n",
      "             1.16781678e-05 1.19435223e-05]]                                \n",
      "log =  102904.08474667135\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.433787194074593 [[ 1.14633252e-05  1.08117491e-05  1.14131608e-05 ...  1.02573571e-05 \n",
      "                                                   1.16172728e-05  1.16433105e-05]                                    \n",
      "                                                 [-9.96341589e-05  1.04746658e-05  1.13992393e-05 ...  9.50014304e-06 \n",
      "                                                   1.18035829e-05  1.22217298e-05]                                    \n",
      "                                                 [ 9.40542287e-06  1.06135830e-05  1.02991336e-05 ...  1.49715512e-05 \n",
      "                                                   8.83285914e-06  9.02781779e-06]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.20685566e-05  1.06952391e-05 -9.99311041e-05 ...  1.09845863e-05 \n",
      "                                                   1.27876606e-05  1.07162966e-05]                                    \n",
      "                                                 [ 9.83663493e-06  9.64761811e-06  1.10735196e-05 ... -9.80692298e-05 \n",
      "                                                   9.76376981e-06  9.43589648e-06]                                    \n",
      "                                                 [ 1.16651137e-05  1.20879784e-05  1.17179950e-05 ...  9.55141697e-06 \n",
      "                                                  -9.94329433e-05  1.19435223e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.433787194074593 [[ 1.14633252e-05  1.08117491e-05  1.14131608e-05 ...  1.02573571e-05 [[-0.00122191 -0.00032105 -0.00027777 ...  0.00241379 -0.00250172 \n",
      "                                                              1.16172728e-05  1.16433105e-05]                                      -0.00131924]                                                    \n",
      "                                                            [-9.96341589e-05  1.04746658e-05  1.13992393e-05 ...  9.50014304e-06  [-0.00178212  0.00041769  0.00024594 ...  0.00217579 -0.00283145 \n",
      "                                                              1.18035829e-05  1.22217298e-05]                                      -0.00141648]                                                    \n",
      "                                                            [ 9.40542287e-06  1.06135830e-05  1.02991336e-05 ...  1.49715512e-05  [-0.00209196  0.0009899   0.00081012 ...  0.0021563  -0.00292515 \n",
      "                                                              8.83285914e-06  9.02781779e-06]                                      -0.00207512]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.20685566e-05  1.06952391e-05 -9.99311041e-05 ...  1.09845863e-05  [-0.0009708   0.00075232 -0.0010999  ...  0.00320572 -0.00215948 \n",
      "                                                              1.27876606e-05  1.07162966e-05]                                      -0.00211452]                                                    \n",
      "                                                            [ 9.83663493e-06  9.64761811e-06  1.10735196e-05 ... -9.80692298e-05  [-0.00032318 -0.00043356 -0.00132348 ...  0.0034953  -0.00163342 \n",
      "                                                              9.76376981e-06  9.43589648e-06]                                      -0.00211966]                                                    \n",
      "                                                            [ 1.16651137e-05  1.20879784e-05  1.17179950e-05 ...  9.55141697e-06  [ 0.03387199 -0.10084636 -0.05089945 ...  0.02419182  0.03379792 \n",
      "                                                             -9.94329433e-05  1.19435223e-05]]                                      0.0384242 ]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.7196371711019927 [[-0.01674248 -0.03117564 -0.04905093 ... -0.01701602 -0.02560236\n",
      "                                       0.00638437]                                                   \n",
      "                                     [ 0.00927906 -0.0155997  -0.06582603 ...  0.10786353  0.00035515\n",
      "                                       0.05302228]                                                   \n",
      "                                     [-0.00309869  0.08801097 -0.06242373 ...  0.02387383 -0.09714573\n",
      "                                      -0.04809607]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.06484012  0.02496274 -0.03708205 ...  0.00986544 -0.01326682\n",
      "                                      -0.01343972]                                                   \n",
      "                                     [ 0.02986051  0.05423115  0.06420283 ... -0.00758595 -0.01528784\n",
      "                                       0.03811579]                                                   \n",
      "                                     [ 0.09206574 -0.3449967  -0.20492382 ...  0.06001899  0.08959589\n",
      "                                       0.0913421 ]]                                                  \n",
      "Epoch 67, loss: 12.153424\n",
      "== W == -2.3076191318130967\n",
      "enter of the function =  [[-0.1075117   0.06752991 -0.10560475 ...  0.26178927 -0.13809981 [4 6 0 ... 7 6 9]\n",
      "                           -0.20110043]                                                                     \n",
      "                          [-0.01103124 -0.04252054 -0.02548521 ... -0.04990565  0.02740977                  \n",
      "                            0.05572042]                                                                     \n",
      "                          [ 0.10725574  0.05910194  0.03780314 ... -0.15986484  0.0732506                   \n",
      "                            0.06198675]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.0573408  -0.02175082 -0.18005485 ...  0.2816544  -0.08025534                  \n",
      "                           -0.15794929]                                                                     \n",
      "                          [ 0.01717333 -0.02035088  0.00234979 ...  0.06469712 -0.02256459                  \n",
      "                           -0.03696431]                                                                     \n",
      "                          [ 0.02282595 -0.09158526 -0.16898902 ...  0.07727334  0.02017218                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.03426241]]                                                                    \n",
      "soft max = [[9.87520180e-06 1.17642862e-05 9.89405122e-06 ... 1.42866812e-05\n",
      "             9.57771103e-06 8.99292370e-06]                                 \n",
      "            [1.08754418e-05 1.05383176e-05 1.07193791e-05 ... 1.04607775e-05\n",
      "             1.13016442e-05 1.16261732e-05]                                 \n",
      "            [1.22410392e-05 1.16655538e-05 1.14197189e-05 ... 9.37150407e-06\n",
      "             1.18317789e-05 1.16992553e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.03832886e-05 1.07594842e-05 9.18419057e-06 ... 1.45733256e-05\n",
      "             1.01480656e-05 9.38947289e-06]                                 \n",
      "            [1.11865457e-05 1.07745574e-05 1.10219444e-05 ... 1.17310076e-05\n",
      "             1.07507321e-05 1.05970338e-05]                                 \n",
      "            [1.12499579e-05 1.00337377e-05 9.28638567e-06 ... 1.18794710e-05\n",
      "             1.12201427e-05 1.06257046e-05]]                                \n",
      "log =  102909.80253002155\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.434422503335728 [[ 9.87520180e-06  1.17642862e-05  9.89405122e-06 ...  1.42866812e-05 \n",
      "                                                   9.57771103e-06  8.99292370e-06]                                    \n",
      "                                                 [ 1.08754418e-05  1.05383176e-05  1.07193791e-05 ...  1.04607775e-05 \n",
      "                                                   1.13016442e-05  1.16261732e-05]                                    \n",
      "                                                 [-9.88700719e-05  1.16655538e-05  1.14197189e-05 ...  9.37150407e-06 \n",
      "                                                   1.18317789e-05  1.16992553e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.03832886e-05  1.07594842e-05  9.18419057e-06 ... -9.65377855e-05 \n",
      "                                                   1.01480656e-05  9.38947289e-06]                                    \n",
      "                                                 [ 1.11865457e-05  1.07745574e-05  1.10219444e-05 ...  1.17310076e-05 \n",
      "                                                   1.07507321e-05  1.05970338e-05]                                    \n",
      "                                                 [ 1.12499579e-05  1.00337377e-05  9.28638567e-06 ...  1.18794710e-05 \n",
      "                                                   1.12201427e-05 -1.00485406e-04]]                                   \n",
      "loss , grand (prediction), grad by W =  11.434422503335728 [[ 9.87520180e-06  1.17642862e-05  9.89405122e-06 ...  1.42866812e-05 [[-0.00124007 -0.00032321 -0.00028973 ...  0.00246027 -0.00253084 \n",
      "                                                              9.57771103e-06  8.99292370e-06]                                      -0.0013483 ]                                                    \n",
      "                                                            [ 1.08754418e-05  1.05383176e-05  1.07193791e-05 ...  1.04607775e-05  [-0.00180056  0.00041573  0.00023392 ...  0.00222423 -0.00286178 \n",
      "                                                              1.13016442e-05  1.16261732e-05]                                      -0.00144694]                                                    \n",
      "                                                            [-9.88700719e-05  1.16655538e-05  1.14197189e-05 ...  9.37150407e-06  [-0.00211021  0.00098764  0.00079781 ...  0.00220733 -0.00295594 \n",
      "                                                              1.18317789e-05  1.16992553e-05]                                      -0.00210733]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.03832886e-05  1.07594842e-05  9.18419057e-06 ... -9.65377855e-05  [-0.00098739  0.00075377 -0.00111388 ...  0.00325697 -0.00218681 \n",
      "                                                              1.01480656e-05  9.38947289e-06]                                      -0.00214478]                                                    \n",
      "                                                            [ 1.11865457e-05  1.07745574e-05  1.10219444e-05 ...  1.17310076e-05  [-0.00033936 -0.00043279 -0.00133737 ...  0.00354852 -0.00166084 \n",
      "                                                              1.07507321e-05  1.05970338e-05]                                      -0.00215135]                                                    \n",
      "                                                            [ 1.12499579e-05  1.00337377e-05  9.28638567e-06 ...  1.18794710e-05  [ 0.03385288 -0.10092494 -0.05095516 ...  0.02421439  0.03379217 \n",
      "                                                              1.12201427e-05 -1.00485406e-04]]                                      0.03842433]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.7341798262960695 [[-1.69221215e-02 -3.14906094e-02 -4.95442211e-02 ... -1.71620396e-02\n",
      "                                      -2.58833960e-02  6.43502478e-03]                                   \n",
      "                                     [ 9.35402782e-03 -1.57515245e-02 -6.64818293e-02 ...  1.08963925e-01\n",
      "                                       3.30388240e-04  5.35383372e-02]                                   \n",
      "                                     [-3.15059681e-03  8.89009772e-02 -6.30398670e-02 ...  2.41341314e-02\n",
      "                                      -9.81464436e-02 -4.85977814e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-6.54982242e-02  2.52198921e-02 -3.74638667e-02 ...  9.99614836e-03\n",
      "                                      -1.34210837e-02 -1.35952575e-02]                                   \n",
      "                                     [ 3.01558853e-02  5.47691289e-02  6.48316207e-02 ... -7.62685373e-03\n",
      "                                      -1.54570531e-02  3.84757513e-02]                                   \n",
      "                                     [ 9.33251177e-02 -3.49455131e-01 -2.07482050e-01 ...  6.08610998e-02\n",
      "                                       9.08298287e-02  9.26397583e-02]]                                  \n",
      "Epoch 68, loss: 12.168602\n",
      "== W == -2.3441341481214977\n",
      "enter of the function =  [[-0.06478727 -0.06533369 -0.16672639 ...  0.25556588 -0.13313105 [1 4 8 ... 5 1 1]\n",
      "                           -0.16915008]                                                                     \n",
      "                          [ 0.00445292 -0.08800253 -0.06927552 ...  0.04532767  0.01258855                  \n",
      "                            0.05717569]                                                                     \n",
      "                          [ 0.05543035 -0.04343882  0.01267217 ... -0.10953094  0.07271906                  \n",
      "                            0.07319151]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.05066807  0.07824879  0.04780116 ... -0.04111608  0.07566796                  \n",
      "                            0.04119716]                                                                     \n",
      "                          [ 0.05824224 -0.08341241  0.01390143 ... -0.06163297  0.03460177                  \n",
      "                            0.05524092]                                                                     \n",
      "                          [ 0.05645585 -0.00270701  0.07954988 ... -0.10512508  0.13562771                  \n",
      "                            0.20652413]]                                                                    \n",
      "soft max = [[1.03018424e-05 1.02962147e-05 9.30343452e-06 ... 1.41919642e-05\n",
      "             9.62129597e-06 9.28091315e-06]                                 \n",
      "            [1.10404184e-05 1.00654371e-05 1.02557087e-05 ... 1.15010426e-05\n",
      "             1.11306056e-05 1.16381177e-05]                                 \n",
      "            [1.16178228e-05 1.05241351e-05 1.11315364e-05 ... 9.85106014e-06\n",
      "             1.18204264e-05 1.18260122e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.15626270e-05 1.18859712e-05 1.15295255e-05 ... 1.05486084e-05\n",
      "             1.18553350e-05 1.14536354e-05]                                 \n",
      "            [1.16505369e-05 1.01117449e-05 1.11452282e-05 ... 1.03343888e-05\n",
      "             1.13783427e-05 1.16156223e-05]                                 \n",
      "            [1.16297430e-05 1.09616521e-05 1.19014459e-05 ... 9.89455836e-06\n",
      "             1.25879214e-05 1.35127564e-05]]                                \n",
      "log =  102915.68587444077\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.435076208271196 [[ 1.03018424e-05 -1.00814896e-04  9.30343452e-06 ...  1.41919642e-05 \n",
      "                                                   9.62129597e-06  9.28091315e-06]                                    \n",
      "                                                 [ 1.10404184e-05  1.00654371e-05  1.02557087e-05 ...  1.15010426e-05 \n",
      "                                                   1.11306056e-05  1.16381177e-05]                                    \n",
      "                                                 [ 1.16178228e-05  1.05241351e-05  1.11315364e-05 ...  9.85106014e-06 \n",
      "                                                  -9.92906848e-05  1.18260122e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.15626270e-05  1.18859712e-05  1.15295255e-05 ...  1.05486084e-05 \n",
      "                                                   1.18553350e-05  1.14536354e-05]                                    \n",
      "                                                 [ 1.16505369e-05 -1.00999366e-04  1.11452282e-05 ...  1.03343888e-05 \n",
      "                                                   1.13783427e-05  1.16156223e-05]                                    \n",
      "                                                 [ 1.16297430e-05 -1.00149459e-04  1.19014459e-05 ...  9.89455836e-06 \n",
      "                                                   1.25879214e-05  1.35127564e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.435076208271196 [[ 1.03018424e-05 -1.00814896e-04  9.30343452e-06 ...  1.41919642e-05 [[-0.0012585  -0.00032539 -0.00030188 ...  0.00250769 -0.00256038 \n",
      "                                                              9.62129597e-06  9.28091315e-06]                                      -0.00137781]                                                    \n",
      "                                                            [ 1.10404184e-05  1.00654371e-05  1.02557087e-05 ...  1.15010426e-05  [-0.00181926  0.00041375  0.00022172 ...  0.00227365 -0.00289256 \n",
      "                                                              1.11306056e-05  1.16381177e-05]                                      -0.00147787]                                                    \n",
      "                                                            [ 1.16178228e-05  1.05241351e-05  1.11315364e-05 ...  9.85106014e-06  [-0.00212873  0.00098535  0.00078532 ...  0.00225938 -0.00298717 \n",
      "                                                             -9.92906848e-05  1.18260122e-05]                                      -0.00214003]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.15626270e-05  1.18859712e-05  1.15295255e-05 ...  1.05486084e-05  [-0.00100424  0.00075521 -0.00112806 ...  0.00330924 -0.00221456 \n",
      "                                                              1.18553350e-05  1.14536354e-05]                                      -0.0021755 ]                                                    \n",
      "                                                            [ 1.16505369e-05 -1.00999366e-04  1.11452282e-05 ...  1.03343888e-05  [-0.00035579 -0.00043203 -0.00135147 ...  0.00360281 -0.00168869 \n",
      "                                                              1.13783427e-05  1.16156223e-05]                                      -0.00218351]                                                    \n",
      "                                                            [ 1.16297430e-05 -1.00149459e-04  1.19014459e-05 ...  9.89455836e-06  [ 0.03383264 -0.10100532 -0.05101234 ...  0.02423755  0.03378574 \n",
      "                                                              1.25879214e-05  1.35127564e-05]]                                      0.03842396]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.7490169954790167 [[-1.71037434e-02 -3.18087476e-02 -5.00425607e-02 ... -1.73090573e-02\n",
      "                                      -2.61675383e-02  6.48589200e-03]                                   \n",
      "                                     [ 9.42956253e-03 -1.59048824e-02 -6.71443085e-02 ...  1.10075806e-01\n",
      "                                       3.05074298e-04  5.40592511e-02]                                   \n",
      "                                     [-3.20320488e-03  8.97998634e-02 -6.36622875e-02 ...  2.43975460e-02\n",
      "                                      -9.91574674e-02 -4.91048325e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-6.61630804e-02  2.54796286e-02 -3.78496441e-02 ...  1.01286795e-02\n",
      "                                      -1.35771626e-02 -1.37526579e-02]                                   \n",
      "                                     [ 3.04540506e-02  5.53124922e-02  6.54665632e-02 ... -7.66763702e-03\n",
      "                                      -1.56282320e-02  3.88389953e-02]                                   \n",
      "                                     [ 9.45968976e-02 -3.53958932e-01 -2.10066422e-01 ...  6.17118548e-02\n",
      "                                       9.20760487e-02  9.39503992e-02]]                                  \n",
      "Epoch 69, loss: 12.184093\n",
      "== W == -2.381134326623249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[-0.0554772   0.06169257 -0.14854624 ...  0.16502734  0.01543371 [2 7 1 ... 3 1 1]\n",
      "                           -0.10375104]                                                                     \n",
      "                          [-0.07148289 -0.09749935 -0.14586731 ...  0.15300299 -0.14325749                  \n",
      "                           -0.15642321]                                                                     \n",
      "                          [ 0.0449642   0.03131506  0.0790879  ... -0.10944294  0.10432362                  \n",
      "                            0.11968736]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.08388549 -0.06833021 -0.00285981 ...  0.09189337  0.00700042                  \n",
      "                           -0.05585342]                                                                     \n",
      "                          [ 0.08881627 -0.04206036  0.02131009 ... -0.13357771  0.12878126                  \n",
      "                            0.09311372]                                                                     \n",
      "                          [-0.01609453 -0.16966376 -0.08641987 ... -0.01234275 -0.04291358                  \n",
      "                            0.04445404]]                                                                    \n",
      "soft max = [[1.03935780e-05 1.16856066e-05 9.46990680e-06 ... 1.29577316e-05\n",
      "             1.11573561e-05 9.90375784e-06]                                 \n",
      "            [1.02285458e-05 9.96586703e-06 9.49531005e-06 ... 1.28028564e-05\n",
      "             9.52012344e-06 9.39560570e-06]                                 \n",
      "            [1.14917514e-05 1.13359645e-05 1.18906599e-05 ... 9.84754680e-06\n",
      "             1.21945476e-05 1.23833481e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.01024687e-05 1.02608440e-05 1.09551043e-05 ... 1.20439045e-05\n",
      "             1.10636585e-05 1.03896684e-05]                                 \n",
      "            [1.20069011e-05 1.05339666e-05 1.12231139e-05 ... 9.61272365e-06\n",
      "             1.24964746e-05 1.20586112e-05]                                 \n",
      "            [1.08110718e-05 9.27202262e-06 1.00768976e-05 ... 1.08517087e-05\n",
      "             1.05249826e-05 1.14858903e-05]]                                \n",
      "log =  102921.74061172844\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.435748956858715 [[ 1.03935780e-05  1.16856066e-05 -1.01641204e-04 ...  1.29577316e-05 \n",
      "                                                   1.11573561e-05  9.90375784e-06]                                    \n",
      "                                                 [ 1.02285458e-05  9.96586703e-06  9.49531005e-06 ... -9.83082547e-05 \n",
      "                                                   9.52012344e-06  9.39560570e-06]                                    \n",
      "                                                 [ 1.14917514e-05 -9.97751466e-05  1.18906599e-05 ...  9.84754680e-06 \n",
      "                                                   1.21945476e-05  1.23833481e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.01024687e-05  1.02608440e-05  1.09551043e-05 ...  1.20439045e-05 \n",
      "                                                   1.10636585e-05  1.03896684e-05]                                    \n",
      "                                                 [ 1.20069011e-05 -1.00577144e-04  1.12231139e-05 ...  9.61272365e-06 \n",
      "                                                   1.24964746e-05  1.20586112e-05]                                    \n",
      "                                                 [ 1.08110718e-05 -1.01839088e-04  1.00768976e-05 ...  1.08517087e-05 \n",
      "                                                   1.05249826e-05  1.14858903e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.435748956858715 [[ 1.03935780e-05  1.16856066e-05 -1.01641204e-04 ...  1.29577316e-05 [[-0.00127719 -0.0003276  -0.00031421 ...  0.00255607 -0.00259037 \n",
      "                                                              1.11573561e-05  9.90375784e-06]                                      -0.00140777]                                                    \n",
      "                                                            [ 1.02285458e-05  9.96586703e-06  9.49531005e-06 ... -9.83082547e-05  [-0.00183824  0.00041173  0.00020932 ...  0.00232407 -0.00292379 \n",
      "                                                              9.52012344e-06  9.39560570e-06]                                      -0.00150927]                                                    \n",
      "                                                            [ 1.14917514e-05 -9.97751466e-05  1.18906599e-05 ...  9.84754680e-06  [-0.00214753  0.00098303  0.00077264 ...  0.00231248 -0.00301887 \n",
      "                                                              1.21945476e-05  1.23833481e-05]                                      -0.00217323]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.01024687e-05  1.02608440e-05  1.09551043e-05 ...  1.20439045e-05  [-0.00102136  0.00075665 -0.00114244 ...  0.00336256 -0.00224273 \n",
      "                                                              1.10636585e-05  1.03896684e-05]                                      -0.00220669]                                                    \n",
      "                                                            [ 1.20069011e-05 -1.00577144e-04  1.12231139e-05 ...  9.61272365e-06  [-0.00037249 -0.00043129 -0.00136577 ...  0.00365817 -0.00171696 \n",
      "                                                              1.24964746e-05  1.20586112e-05]                                      -0.00221616]                                                    \n",
      "                                                            [ 1.08110718e-05 -1.01839088e-04  1.00768976e-05 ...  1.08517087e-05  [ 0.03381122 -0.10108754 -0.05107104 ...  0.02426132  0.03377861 \n",
      "                                                              1.05249826e-05  1.14858903e-05]]                                      0.03842306]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.7641546710606401 [[-1.72873658e-02 -3.21300889e-02 -5.05460051e-02 ... -1.74570710e-02\n",
      "                                      -2.64548176e-02  6.53697283e-03]                                   \n",
      "                                     [ 9.50566554e-03 -1.60597938e-02 -6.78135344e-02 ...  1.11199301e-01\n",
      "                                       2.79199442e-04  5.45850649e-02]                                   \n",
      "                                     [-3.25652424e-03  9.07077155e-02 -6.42910572e-02 ...  2.46641153e-02\n",
      "                                      -1.00178914e-01 -4.96172812e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-6.68347536e-02  2.57419770e-02 -3.82394212e-02 ...  1.02630587e-02\n",
      "                                      -1.37350798e-02 -1.39119395e-02]                                   \n",
      "                                     [ 3.07550332e-02  5.58612968e-02  6.61077142e-02 ... -7.70828532e-03\n",
      "                                      -1.58014012e-02  3.92055501e-02]                                   \n",
      "                                     [ 9.58811930e-02 -3.58508574e-01 -2.12677210e-01 ...  6.25713488e-02\n",
      "                                       9.33346666e-02  9.52741428e-02]]                                  \n",
      "Epoch 70, loss: 12.199904\n",
      "== W == -2.418621774471384\n",
      "enter of the function =  [[-0.03968163 -0.16679374 -0.2838855  ...  0.45982869 -0.15694368 [1 7 2 ... 4 2 5]\n",
      "                           -0.23879834]                                                                     \n",
      "                          [-0.17377473  0.04242898 -0.19880107 ...  0.36783337 -0.29599666                  \n",
      "                           -0.25915202]                                                                     \n",
      "                          [-0.01030402  0.02890586 -0.06084499 ...  0.03809868 -0.09124427                  \n",
      "                           -0.10320159]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.0741179  -0.121851    0.02942761 ... -0.24229435  0.13213136                  \n",
      "                            0.17067167]                                                                     \n",
      "                          [-0.11200902  0.00879157 -0.05914256 ...  0.10894951 -0.11083091                  \n",
      "                           -0.08432511]                                                                     \n",
      "                          [-0.02856852 -0.02483903 -0.03110762 ...  0.23952827 -0.15526577                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.1319918 ]]                                                                    \n",
      "soft max = [[1.05541805e-05 9.29438016e-06 8.26738435e-06 ... 1.73923832e-05\n",
      "             9.38638269e-06 8.64866810e-06]                                 \n",
      "            [9.22972209e-06 1.14573638e-05 9.00160233e-06 ... 1.58637565e-05\n",
      "             8.16786058e-06 8.47441521e-06]                                 \n",
      "            [1.08688365e-05 1.13034675e-05 1.03331656e-05 ... 1.14078573e-05\n",
      "             1.00237713e-05 9.90462763e-06]                                 \n",
      "            ...                                                             \n",
      "            [1.18262492e-05 9.72162390e-06 1.13093666e-05 ... 8.61848507e-06\n",
      "             1.25326223e-05 1.30250620e-05]                                 \n",
      "            [9.81777636e-06 1.10783776e-05 1.03507721e-05 ... 1.22454347e-05\n",
      "             9.82934956e-06 1.00933678e-05]                                 \n",
      "            [1.06721244e-05 1.07120003e-05 1.06450613e-05 ... 1.39535219e-05\n",
      "             9.40214541e-06 9.62353705e-06]]                                \n",
      "log =  102927.97281433357\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.436441423814841 [[ 1.05541805e-05 -1.01816731e-04  8.26738435e-06 ...  1.73923832e-05 \n",
      "                                                   9.38638269e-06  8.64866810e-06]                                    \n",
      "                                                 [ 9.22972209e-06  1.14573638e-05  9.00160233e-06 ... -9.52473546e-05 \n",
      "                                                   8.16786058e-06  8.47441521e-06]                                    \n",
      "                                                 [ 1.08688365e-05  1.13034675e-05 -1.00777946e-04 ...  1.14078573e-05 \n",
      "                                                   1.00237713e-05  9.90462763e-06]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.18262492e-05  9.72162390e-06  1.13093666e-05 ...  8.61848507e-06 \n",
      "                                                   1.25326223e-05  1.30250620e-05]                                    \n",
      "                                                 [ 9.81777636e-06  1.10783776e-05 -1.00760339e-04 ...  1.22454347e-05 \n",
      "                                                   9.82934956e-06  1.00933678e-05]                                    \n",
      "                                                 [ 1.06721244e-05  1.07120003e-05  1.06450613e-05 ...  1.39535219e-05 \n",
      "                                                   9.40214541e-06  9.62353705e-06]]                                   \n",
      "loss , grand (prediction), grad by W =  11.436441423814841 [[ 1.05541805e-05 -1.01816731e-04  8.26738435e-06 ...  1.73923832e-05 [[-0.00129616 -0.00032985 -0.00032673 ...  0.00260544 -0.00262078 \n",
      "                                                              9.38638269e-06  8.64866810e-06]                                      -0.00143818]                                                    \n",
      "                                                            [ 9.22972209e-06  1.14573638e-05  9.00160233e-06 ... -9.52473546e-05  [-0.0018575   0.00040969  0.00019675 ...  0.00237552 -0.00295546 \n",
      "                                                              8.16786058e-06  8.47441521e-06]                                      -0.00154114]                                                    \n",
      "                                                            [ 1.08688365e-05  1.13034675e-05 -1.00777946e-04 ...  1.14078573e-05  [-0.00216661  0.00098068  0.00075976 ...  0.00236665 -0.00305102 \n",
      "                                                              1.00237713e-05  9.90462763e-06]                                      -0.00220692]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.18262492e-05  9.72162390e-06  1.13093666e-05 ...  8.61848507e-06  [-0.00103874  0.00075809 -0.00115702 ...  0.00341696 -0.00227134 \n",
      "                                                              1.25326223e-05  1.30250620e-05]                                      -0.00223836]                                                    \n",
      "                                                            [ 9.81777636e-06  1.10783776e-05 -1.00760339e-04 ...  1.22454347e-05  [-0.00038945 -0.00043055 -0.00138027 ...  0.00371464 -0.00174567 \n",
      "                                                              9.82934956e-06  1.00933678e-05]                                      -0.0022493 ]                                                    \n",
      "                                                            [ 1.06721244e-05  1.07120003e-05  1.06450613e-05 ...  1.39535219e-05  [ 0.03378858 -0.10117167 -0.05113133 ...  0.0242857   0.03377073 \n",
      "                                                              9.40214541e-06  9.62353705e-06]]                                      0.0384216 ]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.7795989686971052 [[-1.74730114e-02 -3.24546659e-02 -5.10546072e-02 ... -1.76060809e-02\n",
      "                                      -2.67452694e-02  6.58826489e-03]                                   \n",
      "                                     [ 9.58233976e-03 -1.62162744e-02 -6.84895765e-02 ...  1.12334534e-01\n",
      "                                       2.52753581e-04  5.51158229e-02]                                   \n",
      "                                     [-3.31056479e-03  9.16246230e-02 -6.49262414e-02 ...  2.49338813e-02\n",
      "                                      -1.01210892e-01 -5.01351863e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-6.75133147e-02  2.60069633e-02 -3.86332398e-02 ...  1.03993150e-02\n",
      "                                      -1.38948580e-02 -1.40731258e-02]                                   \n",
      "                                     [ 3.10588587e-02  5.64155969e-02  6.67551336e-02 ... -7.74878644e-03\n",
      "                                      -1.59765849e-02  3.95754440e-02]                                   \n",
      "                                     [ 9.71781171e-02 -3.63104535e-01 -2.15314693e-01 ...  6.34396755e-02\n",
      "                                       9.46057993e-02  9.66111148e-02]]                                  \n",
      "Epoch 71, loss: 12.216040\n",
      "== W == -2.456598340046956\n",
      "enter of the function =  [[ 0.07557858  0.02405108  0.08536244 ... -0.05361751  0.13176825 [7 6 4 ... 6 4 3]\n",
      "                            0.09671032]                                                                     \n",
      "                          [ 0.10174493 -0.01059041  0.11876919 ... -0.06936647  0.09133504                  \n",
      "                            0.05382803]                                                                     \n",
      "                          [ 0.06546436  0.00651886  0.01057754 ... -0.17156055  0.08988182                  \n",
      "                            0.12619104]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.02153951  0.05471619 -0.0505791  ...  0.12981474 -0.02876155                  \n",
      "                           -0.08770553]                                                                     \n",
      "                          [ 0.00158197 -0.07421765 -0.04945509 ...  0.0208734   0.03025573                  \n",
      "                            0.05202333]                                                                     \n",
      "                          [ 0.01748217 -0.00973211  0.02924796 ...  0.08613307 -0.00322832                  \n",
      "                           -0.02294617]]                                                                    \n",
      "soft max = [[1.18378619e-05 1.12433352e-05 1.19542502e-05 ... 1.04031323e-05\n",
      "             1.25220702e-05 1.20906784e-05]                                 \n",
      "            [1.21517037e-05 1.08605182e-05 1.23603484e-05 ... 1.02405771e-05\n",
      "             1.20258619e-05 1.15831619e-05]                                 \n",
      "            [1.17187346e-05 1.10479326e-05 1.10928637e-05 ... 9.24574922e-06\n",
      "             1.20083983e-05 1.24524263e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.07422540e-05 1.15934541e-05 1.04347893e-05 ... 1.24976320e-05\n",
      "             1.06649525e-05 1.00544861e-05]                                 \n",
      "            [1.09935245e-05 1.01910185e-05 1.04465247e-05 ... 1.12076642e-05\n",
      "             1.13133131e-05 1.15622765e-05]                                 \n",
      "            [1.11697208e-05 1.08698439e-05 1.13019175e-05 ... 1.19634661e-05\n",
      "             1.09407695e-05 1.07271539e-05]]                                \n",
      "log =  102934.38880732388\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.437154311924875 [[ 1.18378619e-05  1.12433352e-05  1.19542502e-05 ... -1.00707979e-04 \n",
      "                                                   1.25220702e-05  1.20906784e-05]                                    \n",
      "                                                 [ 1.21517037e-05  1.08605182e-05  1.23603484e-05 ...  1.02405771e-05 \n",
      "                                                   1.20258619e-05  1.15831619e-05]                                    \n",
      "                                                 [ 1.17187346e-05  1.10479326e-05  1.10928637e-05 ...  9.24574922e-06 \n",
      "                                                   1.20083983e-05  1.24524263e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.07422540e-05  1.15934541e-05  1.04347893e-05 ...  1.24976320e-05 \n",
      "                                                   1.06649525e-05  1.00544861e-05]                                    \n",
      "                                                 [ 1.09935245e-05  1.01910185e-05  1.04465247e-05 ...  1.12076642e-05 \n",
      "                                                   1.13133131e-05  1.15622765e-05]                                    \n",
      "                                                 [ 1.11697208e-05  1.08698439e-05  1.13019175e-05 ...  1.19634661e-05 \n",
      "                                                   1.09407695e-05  1.07271539e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.437154311924875 [[ 1.18378619e-05  1.12433352e-05  1.19542502e-05 ... -1.00707979e-04 [[-0.00131541 -0.00033212 -0.00033943 ...  0.00265581 -0.00265165 \n",
      "                                                              1.25220702e-05  1.20906784e-05]                                      -0.00146906]                                                    \n",
      "                                                            [ 1.21517037e-05  1.08605182e-05  1.23603484e-05 ...  1.02405771e-05  [-0.00187704  0.00040762  0.00018398 ...  0.002428   -0.0029876  \n",
      "                                                              1.20258619e-05  1.15831619e-05]                                      -0.00157349]                                                    \n",
      "                                                            [ 1.17187346e-05  1.10479326e-05  1.10928637e-05 ...  9.24574922e-06  [-0.00218597  0.00097829  0.00074669 ...  0.00242191 -0.00308364 \n",
      "                                                              1.20083983e-05  1.24524263e-05]                                      -0.00224111]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.07422540e-05  1.15934541e-05  1.04347893e-05 ...  1.24976320e-05  [-0.00105639  0.00075951 -0.00117181 ...  0.00347245 -0.00230038 \n",
      "                                                              1.06649525e-05  1.00544861e-05]                                      -0.0022705 ]                                                    \n",
      "                                                            [ 1.09935245e-05  1.01910185e-05  1.04465247e-05 ...  1.12076642e-05  [-0.00040667 -0.00042982 -0.00139498 ...  0.00377225 -0.00177482 \n",
      "                                                              1.13133131e-05  1.15622765e-05]                                      -0.00228294]                                                    \n",
      "                                                            [ 1.11697208e-05  1.08698439e-05  1.13019175e-05 ...  1.19634661e-05  [ 0.03376466 -0.10125777 -0.05119324 ...  0.02431073  0.03376207 \n",
      "                                                              1.09407695e-05  1.07271539e-05]]                                      0.03841954]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.7953561298949084 [[-1.76607032e-02 -3.27825110e-02 -5.15684206e-02 ... -1.77560873e-02\n",
      "                                      -2.70389299e-02  6.63976572e-03]                                   \n",
      "                                     [ 9.65958813e-03 -1.63743402e-02 -6.91725048e-02 ...  1.13481635e-01\n",
      "                                       2.25726477e-04  5.56515698e-02]                                   \n",
      "                                     [-3.36533653e-03  9.25506760e-02 -6.55679062e-02 ...  2.52068867e-02\n",
      "                                      -1.02253511e-01 -5.06586073e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-6.81988353e-02  2.62746138e-02 -3.90311424e-02 ...  1.05374777e-02\n",
      "                                      -1.40565200e-02 -1.42362406e-02]                                   \n",
      "                                     [ 3.13655528e-02  5.69754474e-02  6.74088823e-02 ... -7.78912788e-03\n",
      "                                      -1.61538074e-02  3.99487054e-02]                                   \n",
      "                                     [ 9.84877841e-02 -3.67747297e-01 -2.17979153e-01 ...  6.43169293e-02\n",
      "                                       9.58895646e-02  9.79614419e-02]]                                  \n",
      "Epoch 72, loss: 12.232510\n",
      "== W == -2.49506559260081\n",
      "enter of the function =  [[-0.05875787 -0.03229522 -0.07888427 ...  0.19785463 -0.16877456 [5 1 2 ... 1 4 6]\n",
      "                           -0.13295573]                                                                     \n",
      "                          [ 0.05619091 -0.10694187 -0.07874122 ...  0.01483841 -0.00735579                  \n",
      "                            0.03494599]                                                                     \n",
      "                          [ 0.01315671 -0.04351467  0.0170774  ... -0.20937499  0.07488949                  \n",
      "                            0.17295915]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.01025372  0.08350278 -0.00215984 ... -0.08786893 -0.0006124                   \n",
      "                            0.06254727]                                                                     \n",
      "                          [-0.05680456 -0.03118487  0.02213641 ...  0.10552766 -0.08569886                  \n",
      "                           -0.06561159]                                                                     \n",
      "                          [-0.01203152  0.00906144 -0.10878389 ...  0.36208213 -0.03051049                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.16411484]]                                                                    \n",
      "soft max = [[1.03446458e-05 1.06220467e-05 1.01385265e-05 ... 1.33709117e-05\n",
      "             9.26693214e-06 9.60487917e-06]                                 \n",
      "            [1.16047889e-05 9.85801743e-06 1.01399769e-05 ... 1.11346888e-05\n",
      "             1.08902855e-05 1.13608466e-05]                                 \n",
      "            [1.11159794e-05 1.05035393e-05 1.11596473e-05 ... 8.89822622e-06\n",
      "             1.18238234e-05 1.30421457e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.08587719e-05 1.19261053e-05 1.09470181e-05 ... 1.00478433e-05\n",
      "             1.09639711e-05 1.16787882e-05]                                 \n",
      "            [1.03648719e-05 1.06338475e-05 1.12162470e-05 ... 1.21916904e-05\n",
      "             1.00696715e-05 1.02739890e-05]                                 \n",
      "            [1.08394843e-05 1.10705495e-05 9.83987542e-06 ... 1.57573840e-05\n",
      "             1.06410212e-05 9.31021427e-06]]                                \n",
      "log =  102940.99518110235\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.437888353455817 [[ 1.03446458e-05  1.06220467e-05  1.01385265e-05 ...  1.33709117e-05 \n",
      "                                                   9.26693214e-06  9.60487917e-06]                                    \n",
      "                                                 [ 1.16047889e-05 -1.01253094e-04  1.01399769e-05 ...  1.11346888e-05 \n",
      "                                                   1.08902855e-05  1.13608466e-05]                                    \n",
      "                                                 [ 1.11159794e-05  1.05035393e-05 -9.99514638e-05 ...  8.89822622e-06 \n",
      "                                                   1.18238234e-05  1.30421457e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.08587719e-05 -9.91850058e-05  1.09470181e-05 ...  1.00478433e-05 \n",
      "                                                   1.09639711e-05  1.16787882e-05]                                    \n",
      "                                                 [ 1.03648719e-05  1.06338475e-05  1.12162470e-05 ...  1.21916904e-05 \n",
      "                                                   1.00696715e-05  1.02739890e-05]                                    \n",
      "                                                 [ 1.08394843e-05  1.10705495e-05  9.83987542e-06 ...  1.57573840e-05 \n",
      "                                                   1.06410212e-05  9.31021427e-06]]                                   \n",
      "loss , grand (prediction), grad by W =  11.437888353455817 [[ 1.03446458e-05  1.06220467e-05  1.01385265e-05 ...  1.33709117e-05 [[-0.00133493 -0.00033442 -0.00035232 ...  0.0027072  -0.00268296 \n",
      "                                                              9.26693214e-06  9.60487917e-06]                                      -0.0015004 ]                                                    \n",
      "                                                            [ 1.16047889e-05 -1.01253094e-04  1.01399769e-05 ...  1.11346888e-05  [-0.00189687  0.00040552  0.00017101 ...  0.00248156 -0.0030202  \n",
      "                                                              1.08902855e-05  1.13608466e-05]                                      -0.00160632]                                                    \n",
      "                                                            [ 1.11159794e-05  1.05035393e-05 -9.99514638e-05 ...  8.89822622e-06  [-0.00220562  0.00097588  0.00073342 ...  0.00247829 -0.00311674 \n",
      "                                                              1.18238234e-05  1.30421457e-05]                                      -0.00227582]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.08587719e-05 -9.91850058e-05  1.09470181e-05 ...  1.00478433e-05  [-0.00107431  0.00076094 -0.00118681 ...  0.00352907 -0.00232987 \n",
      "                                                              1.09639711e-05  1.16787882e-05]                                      -0.00230313]                                                    \n",
      "                                                            [ 1.03648719e-05  1.06338475e-05  1.12162470e-05 ...  1.21916904e-05  [-0.00042416 -0.0004291  -0.00140989 ...  0.003831   -0.00180441 \n",
      "                                                              1.00696715e-05  1.02739890e-05]                                      -0.00231708]                                                    \n",
      "                                                            [ 1.08394843e-05  1.10705495e-05  9.83987542e-06 ...  1.57573840e-05  [ 0.0337394  -0.10134589 -0.05125685 ...  0.02433642  0.03375258 \n",
      "                                                              1.06410212e-05  9.31021427e-06]]                                      0.03841686]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.8114325246742361 [[-1.78504643e-02 -3.31136573e-02 -5.20874991e-02 ... -1.79070901e-02\n",
      "                                      -2.73358357e-02  6.69147281e-03]                                   \n",
      "                                     [ 9.73741359e-03 -1.65340074e-02 -6.98623901e-02 ...  1.14640731e-01\n",
      "                                       1.98107744e-04  5.61923506e-02]                                   \n",
      "                                     [-3.42084960e-03  9.34859657e-02 -6.62161184e-02 ...  2.54831747e-02\n",
      "                                      -1.03306882e-01 -5.11876045e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-6.88913876e-02  2.65449551e-02 -3.94331720e-02 ...  1.06775770e-02\n",
      "                                      -1.42200890e-02 -1.44013080e-02]                                   \n",
      "                                     [ 3.16751416e-02  5.75409037e-02  6.80690214e-02 ... -7.82929670e-03\n",
      "                                      -1.63330937e-02  4.03253631e-02]                                   \n",
      "                                     [ 9.98103085e-02 -3.72437348e-01 -2.20670877e-01 ...  6.52032059e-02\n",
      "                                       9.71860809e-02  9.93252517e-02]]                                  \n",
      "Epoch 73, loss: 12.249321\n",
      "== W == -2.534024800542153\n",
      "enter of the function =  [[-0.11377862 -0.13249691 -0.12745492 ...  0.192849   -0.1200015  [2 2 4 ... 2 5 9]\n",
      "                           -0.07867526]                                                                     \n",
      "                          [ 0.14627849 -0.04868723  0.08027441 ... -0.18551546  0.19167149                  \n",
      "                            0.19247906]                                                                     \n",
      "                          [ 0.03877179 -0.06159034 -0.02920201 ... -0.15011904  0.15304251                  \n",
      "                            0.2076238 ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.08309629 -0.16115942  0.05749833 ... -0.0155153   0.0440269                   \n",
      "                            0.0027841 ]                                                                     \n",
      "                          [ 0.02236668 -0.11042286 -0.01580002 ... -0.1665428   0.15055646                  \n",
      "                            0.14857882]                                                                     \n",
      "                          [-0.00474579  0.02979884 -0.11822048 ...  0.26670372 -0.02020244                  \n",
      "                           -0.13301553]]                                                                    \n",
      "soft max = [[9.78579473e-06 9.60432507e-06 9.65287227e-06 ... 1.32972791e-05\n",
      "             9.72508796e-06 1.01354095e-05]                                 \n",
      "            [1.26922164e-05 1.04439536e-05 1.18815271e-05 ... 9.10838101e-06\n",
      "             1.32816306e-05 1.32923609e-05]                                 \n",
      "            [1.13985052e-05 1.03100598e-05 1.06494519e-05 ... 9.43655902e-06\n",
      "             1.27783578e-05 1.34952024e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.19151027e-05 9.33294880e-06 1.16139710e-05 ... 1.07962099e-05\n",
      "             1.14585633e-05 1.09955929e-05]                                 \n",
      "            [1.12130369e-05 9.81868872e-06 1.07931364e-05 ... 9.28284100e-06\n",
      "             1.27466296e-05 1.27214463e-05]                                 \n",
      "            [1.09131081e-05 1.12966845e-05 9.74242403e-06 ... 1.43165206e-05\n",
      "             1.07457250e-05 9.59934545e-06]]                                \n",
      "log =  102947.79880493016\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.438644311658907 [[ 9.78579473e-06  9.60432507e-06 -1.01458239e-04 ...  1.32972791e-05 \n",
      "                                                   9.72508796e-06  1.01354095e-05]                                    \n",
      "                                                 [ 1.26922164e-05  1.04439536e-05 -9.92295841e-05 ...  9.10838101e-06 \n",
      "                                                   1.32816306e-05  1.32923609e-05]                                    \n",
      "                                                 [ 1.13985052e-05  1.03100598e-05  1.06494519e-05 ...  9.43655902e-06 \n",
      "                                                   1.27783578e-05  1.34952024e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.19151027e-05  9.33294880e-06 -9.94971401e-05 ...  1.07962099e-05 \n",
      "                                                   1.14585633e-05  1.09955929e-05]                                    \n",
      "                                                 [ 1.12130369e-05  9.81868872e-06  1.07931364e-05 ...  9.28284100e-06 \n",
      "                                                   1.27466296e-05  1.27214463e-05]                                    \n",
      "                                                 [ 1.09131081e-05  1.12966845e-05  9.74242403e-06 ...  1.43165206e-05 \n",
      "                                                   1.07457250e-05 -1.01511766e-04]]                                   \n",
      "loss , grand (prediction), grad by W =  11.438644311658907 [[ 9.78579473e-06  9.60432507e-06 -1.01458239e-04 ...  1.32972791e-05 [[-0.00135474 -0.00033675 -0.00036541 ...  0.00275965 -0.00271472 \n",
      "                                                              9.72508796e-06  1.01354095e-05]                                      -0.00153221]                                                    \n",
      "                                                            [ 1.26922164e-05  1.04439536e-05 -9.92295841e-05 ...  9.10838101e-06  [-0.00191698  0.00040339  0.00015786 ...  0.00253622 -0.00305326 \n",
      "                                                              1.32816306e-05  1.32923609e-05]                                      -0.00163965]                                                    \n",
      "                                                            [ 1.13985052e-05  1.03100598e-05  1.06494519e-05 ...  9.43655902e-06  [-0.00222555  0.00097343  0.00071995 ...  0.00253582 -0.00315031 \n",
      "                                                              1.27783578e-05  1.34952024e-05]                                      -0.00231103]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.19151027e-05  9.33294880e-06 -9.94971401e-05 ...  1.07962099e-05  [-0.00109251  0.00076235 -0.00120202 ...  0.00358684 -0.0023598  \n",
      "                                                              1.14585633e-05  1.09955929e-05]                                      -0.00233624]                                                    \n",
      "                                                            [ 1.12130369e-05  9.81868872e-06  1.07931364e-05 ...  9.28284100e-06  [-0.00044193 -0.0004284  -0.00142501 ...  0.00389095 -0.00183445 \n",
      "                                                              1.27466296e-05  1.27214463e-05]                                      -0.00235172]                                                    \n",
      "                                                            [ 1.09131081e-05  1.12966845e-05  9.74242403e-06 ...  1.43165206e-05  [ 0.03371274 -0.10143611 -0.05132221 ...  0.0243628   0.03374223 \n",
      "                                                              1.07457250e-05 -1.01511766e-04]]                                      0.03841352]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.827834654293399 [[-1.80423183e-02 -3.34481380e-02 -5.26118973e-02 ... -1.80590890e-02\n",
      "                                     -2.76360236e-02  6.74338354e-03]                                   \n",
      "                                    [ 9.81581906e-03 -1.66952923e-02 -7.05593038e-02 ...  1.15811954e-01\n",
      "                                      1.69886846e-04  5.67382108e-02]                                   \n",
      "                                    [-3.47711427e-03  9.44305841e-02 -6.68709455e-02 ...  2.57627893e-02\n",
      "                                     -1.04371118e-01 -5.17222388e-02]                                   \n",
      "                                    ...                                                                 \n",
      "                                    [-6.95910446e-02  2.68180140e-02 -3.98393718e-02 ...  1.08196435e-02\n",
      "                                     -1.43855886e-02 -1.45683524e-02]                                   \n",
      "                                    [ 3.19876514e-02  5.81120217e-02  6.87356127e-02 ... -7.86927963e-03\n",
      "                                     -1.65144688e-02  4.07054459e-02]                                   \n",
      "                                    [ 1.01145806e-01 -3.77175180e-01 -2.23390154e-01 ...  6.60986022e-02\n",
      "                                      9.84954676e-02  1.00702673e-01]]                                  \n",
      "Epoch 74, loss: 12.266479\n",
      "== W == -2.573476908276278\n",
      "enter of the function =  [[-0.00216334 -0.01362253  0.03612446 ... -0.0424943   0.0986687  [1 9 6 ... 4 7 8]\n",
      "                            0.0602402 ]                                                                     \n",
      "                          [ 0.1076583  -0.07847887 -0.02253611 ... -0.06794438  0.10982467                  \n",
      "                            0.15312819]                                                                     \n",
      "                          [-0.12381444 -0.04214006 -0.10318888 ...  0.18245993 -0.15165069                  \n",
      "                           -0.13633123]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.0720492  -0.05300857  0.08284363 ... -0.2273241   0.12034108                  \n",
      "                            0.20022052]                                                                     \n",
      "                          [ 0.13178882 -0.08902444  0.12126235 ... -0.16137997  0.23478088                  \n",
      "                            0.17319733]                                                                     \n",
      "                          [ 0.07524377 -0.14947932  0.01766837 ... -0.15506626  0.12208371                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.08967059]]                                                                    \n",
      "soft max = [[1.09354613e-05 1.08108650e-05 1.13622748e-05 ... 1.05031990e-05\n",
      "             1.20956136e-05 1.16396151e-05]                                 \n",
      "            [1.22048385e-05 1.01319653e-05 1.07149297e-05 ... 1.02392645e-05\n",
      "             1.22313074e-05 1.27726015e-05]                                 \n",
      "            [9.68288346e-06 1.05069203e-05 9.88467223e-06 ... 1.31527927e-05\n",
      "             9.41706516e-06 9.56244026e-06]                                 \n",
      "            ...                                                             \n",
      "            [1.17778821e-05 1.03933441e-05 1.19057063e-05 ... 8.73073953e-06\n",
      "             1.23606157e-05 1.33884808e-05]                                 \n",
      "            [1.25029298e-05 1.00256793e-05 1.23720083e-05 ... 9.32588817e-06\n",
      "             1.38592801e-05 1.30315261e-05]                                 \n",
      "            [1.18155676e-05 9.43753527e-06 1.11544949e-05 ... 9.38495538e-06\n",
      "             1.23821744e-05 1.19872642e-05]]                                \n",
      "log =  102954.80684132053\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.439422982368948 [[ 1.09354613e-05 -1.00300246e-04  1.13622748e-05 ...  1.05031990e-05 \n",
      "                                                   1.20956136e-05  1.16396151e-05]                                    \n",
      "                                                 [ 1.22048385e-05  1.01319653e-05  1.07149297e-05 ...  1.02392645e-05 \n",
      "                                                   1.22313074e-05 -9.83385096e-05]                                    \n",
      "                                                 [ 9.68288346e-06  1.05069203e-05  9.88467223e-06 ...  1.31527927e-05 \n",
      "                                                   9.41706516e-06  9.56244026e-06]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.17778821e-05  1.03933441e-05  1.19057063e-05 ...  8.73073953e-06 \n",
      "                                                   1.23606157e-05  1.33884808e-05]                                    \n",
      "                                                 [ 1.25029298e-05  1.00256793e-05  1.23720083e-05 ... -1.01785223e-04 \n",
      "                                                   1.38592801e-05  1.30315261e-05]                                    \n",
      "                                                 [ 1.18155676e-05  9.43753527e-06  1.11544949e-05 ...  9.38495538e-06 \n",
      "                                                  -9.87289367e-05  1.19872642e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.439422982368948 [[ 1.09354613e-05 -1.00300246e-04  1.13622748e-05 ...  1.05031990e-05 [[-0.00137483 -0.00033911 -0.00037868 ...  0.00281317 -0.00274693 \n",
      "                                                              1.20956136e-05  1.16396151e-05]                                      -0.0015645 ]                                                    \n",
      "                                                            [ 1.22048385e-05  1.01319653e-05  1.07149297e-05 ...  1.02392645e-05  [-0.00193737  0.00040123  0.00014451 ...  0.00259199 -0.0030868  \n",
      "                                                              1.22313074e-05 -9.83385096e-05]                                      -0.00167347]                                                    \n",
      "                                                            [ 9.68288346e-06  1.05069203e-05  9.88467223e-06 ...  1.31527927e-05  [-0.00224578  0.00097095  0.00070628 ...  0.00259451 -0.00318435 \n",
      "                                                              9.41706516e-06  9.56244026e-06]                                      -0.00234677]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.17778821e-05  1.03933441e-05  1.19057063e-05 ...  8.73073953e-06  [-0.00111098  0.00076376 -0.00121743 ...  0.00364578 -0.00239017 \n",
      "                                                              1.23606157e-05  1.33884808e-05]                                      -0.00236985]                                                    \n",
      "                                                            [ 1.25029298e-05  1.00256793e-05  1.23720083e-05 ... -1.01785223e-04  [-0.00045996 -0.00042771 -0.00144035 ...  0.0039521  -0.00186494 \n",
      "                                                              1.38592801e-05  1.30315261e-05]                                      -0.00238688]                                                    \n",
      "                                                            [ 1.18155676e-05  9.43753527e-06  1.11544949e-05 ...  9.38495538e-06  [ 0.03368463 -0.1015285  -0.05138938 ...  0.02438988  0.03373097 \n",
      "                                                             -9.87289367e-05  1.19872642e-05]]                                      0.03840947]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.8445691540361093 [[-1.82362889e-02 -3.37859869e-02 -5.31416703e-02 ... -1.82120834e-02\n",
      "                                      -2.79395310e-02  6.79549524e-03]                                   \n",
      "                                     [ 9.89480749e-03 -1.68582114e-02 -7.12633183e-02 ...  1.16995436e-01\n",
      "                                       1.41053097e-04  5.72891964e-02]                                   \n",
      "                                     [-3.53414093e-03  9.53846242e-02 -6.75324555e-02 ...  2.60457754e-02\n",
      "                                      -1.05446333e-01 -5.22625715e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-7.02978801e-02  2.70938177e-02 -4.02497857e-02 ...  1.09637083e-02\n",
      "                                      -1.45530424e-02 -1.47373983e-02]                                   \n",
      "                                     [ 3.23031087e-02  5.86888579e-02  6.94087186e-02 ... -7.90906296e-03\n",
      "                                      -1.66979580e-02  4.10889831e-02]                                   \n",
      "                                     [ 1.02494391e-01 -3.81961293e-01 -2.26137278e-01 ...  6.70032162e-02\n",
      "                                       9.98178446e-02  1.02093835e-01]]                                  \n",
      "Epoch 75, loss: 12.283992\n",
      "== W == -2.613422511485637\n",
      "enter of the function =  [[ 0.17828053 -0.05672603  0.13391735 ... -0.33159379  0.24563049 [7 5 3 ... 1 3 1]\n",
      "                            0.26514765]                                                                     \n",
      "                          [-0.01515677 -0.16617986 -0.02680108 ...  0.0885149  -0.00263141                  \n",
      "                           -0.0588456 ]                                                                     \n",
      "                          [-0.07732575 -0.0433236  -0.15734258 ...  0.30735456 -0.13140531                  \n",
      "                           -0.17385623]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.09764191 -0.01423389 -0.02055269 ... -0.00522414  0.23019467                  \n",
      "                            0.05310278]                                                                     \n",
      "                          [ 0.07606234  0.02268815 -0.09355376 ...  0.04439135  0.0749962                   \n",
      "                           -0.07495878]                                                                     \n",
      "                          [ 0.04622692  0.07126078  0.01599968 ... -0.15263161  0.11924209                  \n",
      "                            0.21383702]]                                                                    \n",
      "soft max = [[1.30906456e-05 1.03490150e-05 1.25225964e-05 ... 7.86186281e-06\n",
      "             1.40026677e-05 1.42786445e-05]                                 \n",
      "            [1.07882826e-05 9.27606577e-06 1.06633891e-05 ... 1.19667536e-05\n",
      "             1.09242595e-05 1.03271028e-05]                                 \n",
      "            [1.01380089e-05 1.04886506e-05 9.35840428e-06 ... 1.48942009e-05\n",
      "             9.60431104e-06 9.20513190e-06]                                 \n",
      "            ...                                                             \n",
      "            [1.20764742e-05 1.07982435e-05 1.07302267e-05 ... 1.08959726e-05\n",
      "             1.37881847e-05 1.15504010e-05]                                 \n",
      "            [1.18186609e-05 1.12043884e-05 9.97481700e-06 ... 1.14502175e-05\n",
      "             1.18060672e-05 1.01620337e-05]                                 \n",
      "            [1.14712545e-05 1.17620489e-05 1.11296983e-05 ... 9.40259541e-06\n",
      "             1.23401659e-05 1.35644770e-05]]                                \n",
      "log =  102962.02676137348\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.440225195708164 [[ 1.30906456e-05  1.03490150e-05  1.25225964e-05 ... -1.03249248e-04 \n",
      "                                                   1.40026677e-05  1.42786445e-05]                                    \n",
      "                                                 [ 1.07882826e-05  9.27606577e-06  1.06633891e-05 ...  1.19667536e-05 \n",
      "                                                   1.09242595e-05  1.03271028e-05]                                    \n",
      "                                                 [ 1.01380089e-05  1.04886506e-05  9.35840428e-06 ...  1.48942009e-05 \n",
      "                                                   9.60431104e-06  9.20513190e-06]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.20764742e-05 -1.00312868e-04  1.07302267e-05 ...  1.08959726e-05 \n",
      "                                                   1.37881847e-05  1.15504010e-05]                                    \n",
      "                                                 [ 1.18186609e-05  1.12043884e-05  9.97481700e-06 ...  1.14502175e-05 \n",
      "                                                   1.18060672e-05  1.01620337e-05]                                    \n",
      "                                                 [ 1.14712545e-05 -9.93490622e-05  1.11296983e-05 ...  9.40259541e-06 \n",
      "                                                   1.23401659e-05  1.35644770e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.440225195708164 [[ 1.30906456e-05  1.03490150e-05  1.25225964e-05 ... -1.03249248e-04 [[-0.0013952  -0.0003415  -0.00039215 ...  0.00286779 -0.00277961 \n",
      "                                                              1.40026677e-05  1.42786445e-05]                                      -0.00159727]                                                    \n",
      "                                                            [ 1.07882826e-05  9.27606577e-06  1.06633891e-05 ...  1.19667536e-05  [-0.00195806  0.00039904  0.00013096 ...  0.00264891 -0.00312081 \n",
      "                                                              1.09242595e-05  1.03271028e-05]                                      -0.00170779]                                                    \n",
      "                                                            [ 1.01380089e-05  1.04886506e-05  9.35840428e-06 ...  1.48942009e-05  [-0.0022663   0.00096843  0.00069241 ...  0.0026544  -0.00321889 \n",
      "                                                              9.60431104e-06  9.20513190e-06]                                      -0.00238303]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.20764742e-05 -1.00312868e-04  1.07302267e-05 ...  1.08959726e-05  [-0.00112973  0.00076516 -0.00123306 ...  0.00370593 -0.00242101 \n",
      "                                                              1.37881847e-05  1.15504010e-05]                                      -0.00240396]                                                    \n",
      "                                                            [ 1.18186609e-05  1.12043884e-05  9.97481700e-06 ...  1.14502175e-05  [-0.00047828 -0.00042703 -0.00145589 ...  0.00401449 -0.00189589 \n",
      "                                                              1.18060672e-05  1.01620337e-05]                                      -0.00242255]                                                    \n",
      "                                                            [ 1.14712545e-05 -9.93490622e-05  1.11296983e-05 ...  9.40259541e-06  [ 0.033655   -0.10162312 -0.05145843 ...  0.02441768  0.03371875 \n",
      "                                                              1.23401659e-05  1.35644770e-05]]                                      0.03840468]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.8616427960634434 [[-1.84324000e-02 -3.41272379e-02 -5.36768739e-02 ... -1.83660726e-02\n",
      "                                      -2.82463956e-02  6.84780515e-03]                                   \n",
      "                                     [ 9.97438182e-03 -1.70227813e-02 -7.19745064e-02 ...  1.18191310e-01\n",
      "                                       1.11595660e-04  5.78453537e-02]                                   \n",
      "                                     [-3.59194012e-03  9.63481800e-02 -6.82007172e-02 ...  2.63321782e-02\n",
      "                                      -1.06532640e-01 -5.28086649e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-7.10119687e-02  2.73723935e-02 -4.06644579e-02 ...  1.11098032e-02\n",
      "                                      -1.47224746e-02 -1.49084708e-02]                                   \n",
      "                                     [ 3.26215402e-02  5.92714694e-02  7.00884023e-02 ... -7.94863259e-03\n",
      "                                      -1.68835870e-02  4.14760042e-02]                                   \n",
      "                                     [ 1.03856181e-01 -3.86796191e-01 -2.28912544e-01 ...  6.79171472e-02\n",
      "                                       1.01153333e-01  1.03498868e-01]]                                  \n",
      "Epoch 76, loss: 12.301868\n",
      "== W == -2.6538618307395874\n",
      "enter of the function =  [[ 0.03979138 -0.01706249 -0.03568019 ... -0.13009105  0.04439828 [1 7 2 ... 3 5 2]\n",
      "                            0.12889432]                                                                     \n",
      "                          [-0.14265834 -0.06642539 -0.17221954 ...  0.2409664  -0.13119911                  \n",
      "                           -0.26987348]                                                                     \n",
      "                          [ 0.04783921 -0.01755747  0.05460793 ... -0.3438177   0.13907002                  \n",
      "                            0.29163555]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.04867818 -0.03577617  0.00649551 ... -0.00069679  0.02263421                  \n",
      "                           -0.00837945]                                                                     \n",
      "                          [ 0.0294938  -0.06904547 -0.06362601 ...  0.05078937  0.12422106                  \n",
      "                            0.07346019]                                                                     \n",
      "                          [-0.03019177  0.02876651 -0.0400489  ...  0.07576    -0.08232086                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.02968281]]                                                                    \n",
      "soft max = [[1.13910799e-05 1.07615189e-05 1.05630178e-05 ... 9.61138327e-06\n",
      "             1.14436785e-05 1.24526508e-05]                                 \n",
      "            [9.49135003e-06 1.02431973e-05 9.21488088e-06 ... 1.39294542e-05\n",
      "             9.60073916e-06 8.35755335e-06]                                 \n",
      "            [1.14831233e-05 1.07561935e-05 1.15611129e-05 ... 7.76185619e-06\n",
      "             1.25800123e-05 1.46534346e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.14927612e-05 1.05620039e-05 1.10180486e-05 ... 1.09390878e-05\n",
      "             1.11973082e-05 1.08553685e-05]                                 \n",
      "            [1.12743813e-05 1.02163945e-05 1.02719121e-05 ... 1.15170503e-05\n",
      "             1.23945922e-05 1.17811335e-05]                                 \n",
      "            [1.06211515e-05 1.12661844e-05 1.05169717e-05 ... 1.18082590e-05\n",
      "             1.00816642e-05 1.06265586e-05]]                                \n",
      "log =  102969.46636112985\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.441051817903316 [[ 1.13910799e-05 -1.00349592e-04  1.05630178e-05 ...  9.61138327e-06 \n",
      "                                                   1.14436785e-05  1.24526508e-05]                                    \n",
      "                                                 [ 9.49135003e-06  1.02431973e-05  9.21488088e-06 ... -9.71816569e-05 \n",
      "                                                   9.60073916e-06  8.35755335e-06]                                    \n",
      "                                                 [ 1.14831233e-05  1.07561935e-05 -9.95499982e-05 ...  7.76185619e-06 \n",
      "                                                   1.25800123e-05  1.46534346e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.14927612e-05  1.05620039e-05  1.10180486e-05 ...  1.09390878e-05 \n",
      "                                                   1.11973082e-05  1.08553685e-05]                                    \n",
      "                                                 [ 1.12743813e-05  1.02163945e-05  1.02719121e-05 ...  1.15170503e-05 \n",
      "                                                   1.23945922e-05  1.17811335e-05]                                    \n",
      "                                                 [ 1.06211515e-05  1.12661844e-05 -1.00594139e-04 ...  1.18082590e-05 \n",
      "                                                   1.00816642e-05  1.06265586e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.441051817903316 [[ 1.13910799e-05 -1.00349592e-04  1.05630178e-05 ...  9.61138327e-06 [[-0.00141586 -0.00034393 -0.00040581 ...  0.00292353 -0.00281275 \n",
      "                                                              1.14436785e-05  1.24526508e-05]                                      -0.00163053]                                                    \n",
      "                                                            [ 9.49135003e-06  1.02431973e-05  9.21488088e-06 ... -9.71816569e-05  [-0.00197905  0.00039682  0.00011721 ...  0.002707   -0.0031553  \n",
      "                                                              9.60073916e-06  8.35755335e-06]                                      -0.00174262]                                                    \n",
      "                                                            [ 1.14831233e-05  1.07561935e-05 -9.95499982e-05 ...  7.76185619e-06  [-0.00228711  0.00096589  0.00067833 ...  0.00271551 -0.00325391 \n",
      "                                                              1.25800123e-05  1.46534346e-05]                                      -0.00241982]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.14927612e-05  1.05620039e-05  1.10180486e-05 ...  1.09390878e-05  [-0.00114876  0.00076656 -0.0012489  ...  0.0037673  -0.0024523  \n",
      "                                                              1.11973082e-05  1.08553685e-05]                                      -0.00243857]                                                    \n",
      "                                                            [ 1.12743813e-05  1.02163945e-05  1.02719121e-05 ...  1.15170503e-05  [-0.00049687 -0.00042637 -0.00147165 ...  0.00407815 -0.0019273  \n",
      "                                                              1.23945922e-05  1.17811335e-05]                                      -0.00245875]                                                    \n",
      "                                                            [ 1.06211515e-05  1.12661844e-05 -1.00594139e-04 ...  1.18082590e-05  [ 0.03362378 -0.10172006 -0.05152943 ...  0.02444623  0.03370552 \n",
      "                                                              1.00816642e-05  1.06265586e-05]]                                      0.03839911]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.8790624923324278 [[-1.86306760e-02 -3.44719253e-02 -5.42175641e-02 ... -1.85210554e-02\n",
      "                                      -2.85566557e-02  6.90031046e-03]                                   \n",
      "                                     [ 1.00545450e-02 -1.71890187e-02 -7.26929419e-02 ...  1.19399713e-01\n",
      "                                       8.15035496e-05  5.84067293e-02]                                   \n",
      "                                     [-3.65052249e-03  9.73213461e-02 -6.88758003e-02 ...  2.66220439e-02\n",
      "                                      -1.07630155e-01 -5.33605819e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-7.17333856e-02  2.76537690e-02 -4.10834330e-02 ...  1.12579605e-02\n",
      "                                      -1.48939094e-02 -1.50815952e-02]                                   \n",
      "                                     [ 3.29429728e-02  5.98599137e-02  7.07747275e-02 ... -7.98797400e-03\n",
      "                                      -1.70713818e-02  4.18665387e-02]                                   \n",
      "                                     [ 1.05231293e-01 -3.91680384e-01 -2.31716254e-01 ...  6.88404955e-02\n",
      "                                       1.02502054e-01  1.04917903e-01]]                                  \n",
      "Epoch 77, loss: 12.320114\n",
      "== W == -2.6947946833083565\n",
      "enter of the function =  [[ 0.00588024 -0.0421222   0.04469173 ...  0.07263508 -0.00543241 [3 2 5 ... 1 4 7]\n",
      "                           -0.07657269]                                                                     \n",
      "                          [ 0.07732687 -0.06065095  0.05271948 ... -0.12756303  0.14500068                  \n",
      "                            0.11714349]                                                                     \n",
      "                          [-0.01151651  0.01643189  0.01958533 ... -0.13955984  0.01540142                  \n",
      "                            0.11494281]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.01728257  0.01633622 -0.0477762  ...  0.16093638 -0.04602655                  \n",
      "                           -0.03730009]                                                                     \n",
      "                          [-0.07028793 -0.07491376  0.04487834 ...  0.03992039 -0.06951749                  \n",
      "                            0.03704307]                                                                     \n",
      "                          [ 0.03854101 -0.01181883  0.0427244  ... -0.19705372  0.01296759                  \n",
      "                            0.10868566]]                                                                    \n",
      "soft max = [[1.10046625e-05 1.04888900e-05 1.14401664e-05 ... 1.17643514e-05\n",
      "             1.08808721e-05 1.01336961e-05]                                 \n",
      "            [1.18196769e-05 1.02963334e-05 1.15323748e-05 ... 9.62992821e-06\n",
      "             1.26472460e-05 1.22997913e-05]                                 \n",
      "            [1.08148728e-05 1.11213946e-05 1.11565206e-05 ... 9.51509010e-06\n",
      "             1.11099403e-05 1.22727531e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.07526930e-05 1.11203307e-05 1.04297533e-05 ... 1.28504031e-05\n",
      "             1.04480176e-05 1.05395908e-05]                                 \n",
      "            [1.01975844e-05 1.01505211e-05 1.14423016e-05 ... 1.13857115e-05\n",
      "             1.02054441e-05 1.13529984e-05]                                 \n",
      "            [1.13700171e-05 1.08116037e-05 1.14176820e-05 ... 8.98345978e-06\n",
      "             1.10829334e-05 1.21962004e-05]]                                \n",
      "log =  102977.13377902893\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.441903753225436 [[ 1.10046625e-05  1.04888900e-05  1.14401664e-05 ...  1.17643514e-05 \n",
      "                                                   1.08808721e-05  1.01336961e-05]                                    \n",
      "                                                 [ 1.18196769e-05  1.02963334e-05 -9.95787363e-05 ...  9.62992821e-06 \n",
      "                                                   1.26472460e-05  1.22997913e-05]                                    \n",
      "                                                 [ 1.08148728e-05  1.11213946e-05  1.11565206e-05 ...  9.51509010e-06 \n",
      "                                                   1.11099403e-05  1.22727531e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.07526930e-05 -9.99907804e-05  1.04297533e-05 ...  1.28504031e-05 \n",
      "                                                   1.04480176e-05  1.05395908e-05]                                    \n",
      "                                                 [ 1.01975844e-05  1.01505211e-05  1.14423016e-05 ...  1.13857115e-05 \n",
      "                                                   1.02054441e-05  1.13529984e-05]                                    \n",
      "                                                 [ 1.13700171e-05  1.08116037e-05  1.14176820e-05 ... -1.02127651e-04 \n",
      "                                                   1.10829334e-05  1.21962004e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.441903753225436 [[ 1.10046625e-05  1.04888900e-05  1.14401664e-05 ...  1.17643514e-05 [[-0.00143681 -0.00034638 -0.00041967 ...  0.00298043 -0.00284636 \n",
      "                                                              1.08808721e-05  1.01336961e-05]                                      -0.00166428]                                                    \n",
      "                                                            [ 1.18196769e-05  1.02963334e-05 -9.95787363e-05 ...  9.62992821e-06  [-0.00200033  0.00039457  0.00010327 ...  0.0027663  -0.00319027 \n",
      "                                                              1.26472460e-05  1.22997913e-05]                                      -0.00177795]                                                    \n",
      "                                                            [ 1.08148728e-05  1.11213946e-05  1.11565206e-05 ...  9.51509010e-06  [-0.00230822  0.00096331  0.00066406 ...  0.00277787 -0.00328943 \n",
      "                                                              1.11099403e-05  1.22727531e-05]                                      -0.00245714]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.07526930e-05 -9.99907804e-05  1.04297533e-05 ...  1.28504031e-05  [-0.00116807  0.00076794 -0.00126495 ...  0.00382994 -0.00248405 \n",
      "                                                              1.04480176e-05  1.05395908e-05]                                      -0.00247369]                                                    \n",
      "                                                            [ 1.01975844e-05  1.01505211e-05  1.14423016e-05 ...  1.13857115e-05  [-0.00051575 -0.00042572 -0.00148762 ...  0.00414311 -0.00195917 \n",
      "                                                              1.02054441e-05  1.13529984e-05]                                      -0.00249547]                                                    \n",
      "                                                            [ 1.13700171e-05  1.08116037e-05  1.14176820e-05 ... -1.02127651e-04  [ 0.03359091 -0.10181938 -0.05160245 ...  0.02447555  0.03369124 \n",
      "                                                              1.10829334e-05  1.21962004e-05]]                                      0.0383927 ]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.8968352975832794 [[-1.88311414e-02 -3.48200838e-02 -5.47637979e-02 ... -1.86770306e-02\n",
      "                                      -2.88703497e-02  6.95300824e-03]                                   \n",
      "                                     [ 1.01353000e-02 -1.73569408e-02 -7.34186992e-02 ...  1.20620780e-01\n",
      "                                       5.07656251e-05  5.89733704e-02]                                   \n",
      "                                     [-3.70989883e-03  9.83042184e-02 -6.95577750e-02 ...  2.69154195e-02\n",
      "                                      -1.08738996e-01 -5.39183859e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-7.24622071e-02  2.79379723e-02 -4.15067563e-02 ...  1.14082132e-02\n",
      "                                      -1.50673715e-02 -1.52567968e-02]                                   \n",
      "                                     [ 3.32674338e-02  6.04542492e-02  7.14677582e-02 ... -8.02707223e-03\n",
      "                                      -1.72613686e-02  4.22606166e-02]                                   \n",
      "                                     [ 1.06619844e-01 -3.96614389e-01 -2.34548711e-01 ...  6.97733628e-02\n",
      "                                       1.03864129e-01  1.06351074e-01]]                                  \n",
      "Epoch 78, loss: 12.338739\n",
      "== W == -2.7362204530460428\n",
      "enter of the function =  [[-0.03789398 -0.00066694 -0.0549595  ...  0.17601126 -0.10438062 [7 5 0 ... 8 7 1]\n",
      "                           -0.11324256]                                                                     \n",
      "                          [ 0.10232554 -0.01568809  0.04434122 ... -0.1158506   0.14353206                  \n",
      "                            0.01055062]                                                                     \n",
      "                          [ 0.00126618  0.08788754 -0.0009468  ...  0.01979456 -0.01053217                  \n",
      "                            0.00148223]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.03935642 -0.05828903 -0.02477182 ... -0.07422786  0.06202545                  \n",
      "                            0.07826332]                                                                     \n",
      "                          [ 0.05459982  0.15799905 -0.07178499 ... -0.12330392 -0.10374379                  \n",
      "                            0.00912145]                                                                     \n",
      "                          [-0.10126954  0.02195065  0.0361916  ...  0.28578444 -0.12227614                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.16212355]]                                                                    \n",
      "soft max = [[1.05267676e-05 1.09260336e-05 1.03486470e-05 ... 1.30374573e-05\n",
      "             9.84963761e-06 9.76273639e-06]                                 \n",
      "            [1.21113233e-05 1.07631386e-05 1.14290288e-05 ... 9.73730796e-06\n",
      "             1.26208139e-05 1.10492871e-05]                                 \n",
      "            [1.09471754e-05 1.19377163e-05 1.09229762e-05 ... 1.11518996e-05\n",
      "             1.08187757e-05 1.09495408e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.13721992e-05 1.03142482e-05 1.06658118e-05 ... 1.01511543e-05\n",
      "             1.16329401e-05 1.18233762e-05]                                 \n",
      "            [1.15468781e-05 1.28047262e-05 1.01759826e-05 ... 9.66500241e-06\n",
      "             9.85591217e-06 1.10335070e-05]                                 \n",
      "            [9.88032841e-06 1.11759700e-05 1.13362651e-05 ... 1.45501270e-05\n",
      "             9.67494108e-06 9.29699970e-06]]                                \n",
      "log =  102985.03751456425\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.442781946062695 [[ 1.05267676e-05  1.09260336e-05  1.03486470e-05 ... -9.80736538e-05 \n",
      "                                                   9.84963761e-06  9.76273639e-06]                                    \n",
      "                                                 [ 1.21113233e-05  1.07631386e-05  1.14290288e-05 ...  9.73730796e-06 \n",
      "                                                   1.26208139e-05  1.10492871e-05]                                    \n",
      "                                                 [-1.00163936e-04  1.19377163e-05  1.09229762e-05 ...  1.11518996e-05 \n",
      "                                                   1.08187757e-05  1.09495408e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.13721992e-05  1.03142482e-05  1.06658118e-05 ...  1.01511543e-05 \n",
      "                                                  -9.94781710e-05  1.18233762e-05]                                    \n",
      "                                                 [ 1.15468781e-05  1.28047262e-05  1.01759826e-05 ... -1.01446109e-04 \n",
      "                                                   9.85591217e-06  1.10335070e-05]                                    \n",
      "                                                 [ 9.88032841e-06 -9.99351411e-05  1.13362651e-05 ...  1.45501270e-05 \n",
      "                                                   9.67494108e-06  9.29699970e-06]]                                   \n",
      "loss , grand (prediction), grad by W =  11.442781946062695 [[ 1.05267676e-05  1.09260336e-05  1.03486470e-05 ... -9.80736538e-05 [[-1.45805447e-03 -3.48866529e-04 -4.33728582e-04 ...  3.03850408e-03 \n",
      "                                                              9.84963761e-06  9.76273639e-06]                                      -2.88043521e-03 -1.69852226e-03]                                    \n",
      "                                                            [ 1.21113233e-05  1.07631386e-05  1.14290288e-05 ...  9.73730796e-06  [-2.02190226e-03  3.92285522e-04  8.91199633e-05 ...  2.82681627e-03 \n",
      "                                                              1.26208139e-05  1.10492871e-05]                                      -3.22572808e-03 -1.81380476e-03]                                    \n",
      "                                                            [-1.00163936e-04  1.19377163e-05  1.09229762e-05 ...  1.11518996e-05  [-2.32963708e-03  9.60695417e-04  6.49573589e-04 ...  2.84151918e-03 \n",
      "                                                              1.08187757e-05  1.09495408e-05]                                      -3.32544176e-03 -2.49500126e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.13721992e-05  1.03142482e-05  1.06658118e-05 ...  1.01511543e-05  [-1.18767658e-03  7.69313186e-04 -1.28121477e-03 ...  3.89386783e-03 \n",
      "                                                             -9.94781710e-05  1.18233762e-05]                                      -2.51627494e-03 -2.50932798e-03]                                    \n",
      "                                                            [ 1.15468781e-05  1.28047262e-05  1.01759826e-05 ... -1.01446109e-04  [-5.34905339e-04 -4.25080266e-04 -1.50380657e-03 ...  4.20938994e-03 \n",
      "                                                              9.85591217e-06  1.10335070e-05]                                      -1.99151840e-03 -2.53271672e-03]                                    \n",
      "                                                            [ 9.88032841e-06 -9.99351411e-05  1.13362651e-05 ...  1.45501270e-05  [ 3.35563000e-02 -1.01921163e-01 -5.16775688e-02 ...  2.45056656e-02 \n",
      "                                                              9.67494108e-06  9.29699970e-06]]                                      3.36758315e-02  3.83854011e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.914968412397435 [[-1.90338209e-02 -3.51717485e-02 -5.53156326e-02 ... -1.88339966e-02\n",
      "                                     -2.91875168e-02  7.00589553e-03]                                   \n",
      "                                    [ 1.02166497e-02 -1.75265645e-02 -7.41518535e-02 ...  1.21854650e-01\n",
      "                                      1.93705967e-05  5.95453245e-02]                                   \n",
      "                                    [-3.77008006e-03  9.92968937e-02 -7.02467121e-02 ...  2.72123524e-02\n",
      "                                     -1.09859280e-01 -5.44821412e-02]                                   \n",
      "                                    ...                                                                 \n",
      "                                    [-7.31985099e-02  2.82250314e-02 -4.19344734e-02 ...  1.15605947e-02\n",
      "                                     -1.52428857e-02 -1.54341017e-02]                                   \n",
      "                                    [ 3.35949507e-02  6.10545345e-02  7.21675596e-02 ... -8.06591188e-03\n",
      "                                     -1.74535740e-02  4.26582681e-02]                                   \n",
      "                                    [ 1.08021951e-01 -4.01598726e-01 -2.37410222e-01 ...  7.07158520e-02\n",
      "                                      1.05239683e-01  1.07798511e-01]]                                  \n",
      "Epoch 79, loss: 12.357750\n",
      "== W == -2.7781380581956614\n",
      "enter of the function =  [[-0.06291836 -0.09911707 -0.07979409 ...  0.11946621 -0.10691732 [6 6 2 ... 6 6 3]\n",
      "                           -0.10638574]                                                                     \n",
      "                          [ 0.06885627  0.01480467  0.01007577 ... -0.07780326  0.07179129                  \n",
      "                            0.05324616]                                                                     \n",
      "                          [ 0.13723413 -0.13579846 -0.03493619 ... -0.05801005  0.16263863                  \n",
      "                            0.08169031]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.00314005 -0.09635726 -0.08506039 ...  0.10777683 -0.02934901                  \n",
      "                            0.01826065]                                                                     \n",
      "                          [ 0.07777162 -0.01363492  0.01653858 ... -0.11764948  0.10517129                  \n",
      "                            0.05893086]                                                                     \n",
      "                          [ 0.12356422 -0.07288917  0.14937138 ... -0.31755155  0.21339989                  \n",
      "                            0.21164148]]                                                                    \n",
      "soft max = [[1.02599641e-05 9.89520835e-06 1.00882725e-05 ... 1.23127328e-05\n",
      "             9.81832345e-06 9.82354413e-06]                                 \n",
      "            [1.17050922e-05 1.10892079e-05 1.10368919e-05 ... 1.01083766e-05\n",
      "             1.17394973e-05 1.15237931e-05]                                 \n",
      "            [1.25334596e-05 9.53881481e-06 1.05511147e-05 ... 1.03104470e-05\n",
      "             1.28559448e-05 1.18562839e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.08919900e-05 9.92255495e-06 1.00352843e-05 ... 1.21696425e-05\n",
      "             1.06102307e-05 1.11275982e-05]                                 \n",
      "            [1.18099137e-05 1.07782776e-05 1.11084523e-05 ... 9.71351511e-06\n",
      "             1.21379753e-05 1.15894890e-05]                                 \n",
      "            [1.23632940e-05 1.01581723e-05 1.26865082e-05 ... 7.95353237e-06\n",
      "             1.35253755e-05 1.35016133e-05]]                                \n",
      "log =  102993.18644824039\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.44368738313782 [[ 1.02599641e-05  9.89520835e-06  1.00882725e-05 ...  1.23127328e-05 \n",
      "                                                  9.81832345e-06  9.82354413e-06]                                    \n",
      "                                                [ 1.17050922e-05  1.10892079e-05  1.10368919e-05 ...  1.01083766e-05 \n",
      "                                                  1.17394973e-05  1.15237931e-05]                                    \n",
      "                                                [ 1.25334596e-05  9.53881481e-06 -1.00559996e-04 ...  1.03104470e-05 \n",
      "                                                  1.28559448e-05  1.18562839e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [ 1.08919900e-05  9.92255495e-06  1.00352843e-05 ...  1.21696425e-05 \n",
      "                                                  1.06102307e-05  1.11275982e-05]                                    \n",
      "                                                [ 1.18099137e-05  1.07782776e-05  1.11084523e-05 ...  9.71351511e-06 \n",
      "                                                  1.21379753e-05  1.15894890e-05]                                    \n",
      "                                                [ 1.23632940e-05  1.01581723e-05  1.26865082e-05 ...  7.95353237e-06 \n",
      "                                                  1.35253755e-05  1.35016133e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.44368738313782 [[ 1.02599641e-05  9.89520835e-06  1.00882725e-05 ...  1.23127328e-05 [[-1.47959202e-03 -3.51382688e-04 -4.47983441e-04 ...  3.09778361e-03 \n",
      "                                                             9.81832345e-06  9.82354413e-06]                                      -2.91499068e-03 -1.73326428e-03]                                    \n",
      "                                                           [ 1.17050922e-05  1.10892079e-05  1.10368919e-05 ...  1.01083766e-05  [-2.04377865e-03  3.89974810e-04  7.47708085e-05 ...  2.88859230e-03 \n",
      "                                                             1.17394973e-05  1.15237931e-05]                                      -3.26167856e-03 -1.85017326e-03]                                    \n",
      "                                                           [ 1.25334596e-05  9.53881481e-06 -1.00559996e-04 ...  1.03104470e-05  [-2.35135243e-03  9.58049112e-04  6.34883129e-04 ...  2.90647727e-03 \n",
      "                                                             1.28559448e-05  1.18562839e-05]                                      -3.36195623e-03 -2.53340492e-03]                                    \n",
      "                                                           ...                                                                   ...                                                                  \n",
      "                                                           [ 1.08919900e-05  9.92255495e-06  1.00352843e-05 ...  1.21696425e-05  [-1.20756744e-03  7.70676983e-04 -1.29769766e-03 ...  3.95911451e-03 \n",
      "                                                             1.06102307e-05  1.11275982e-05]                                      -2.54896793e-03 -2.54547965e-03]                                    \n",
      "                                                           [ 1.18099137e-05  1.07782776e-05  1.11084523e-05 ...  9.71351511e-06  [-5.54351200e-04 -4.24459528e-04 -1.52020866e-03 ...  4.27703115e-03 \n",
      "                                                             1.21379753e-05  1.15894890e-05]                                      -2.02433609e-03 -2.57050296e-03]                                    \n",
      "                                                           [ 1.23632940e-05  1.01581723e-05  1.26865082e-05 ...  7.95353237e-06  [ 3.35198850e-02 -1.02025503e-01 -5.17548573e-02 ...  2.45365907e-02 \n",
      "                                                             1.35253755e-05  1.35016133e-05]]                                      3.36592524e-02  3.83771728e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.9334691863286221 [[-1.92387396e-02 -3.55269546e-02 -5.58731262e-02 ... -1.89919515e-02\n",
      "                                      -2.95081963e-02  7.05896926e-03]                                   \n",
      "                                     [ 1.02985972e-02 -1.76979073e-02 -7.48924808e-02 ...  1.23101465e-01\n",
      "                                      -1.26929782e-05  6.01226397e-02]                                   \n",
      "                                     [-3.83107723e-03  1.00299470e-01 -7.09426835e-02 ...  2.75128911e-02\n",
      "                                      -1.10991127e-01 -5.50519126e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-7.39423717e-02  2.85149748e-02 -4.23666302e-02 ...  1.17151393e-02\n",
      "                                      -1.54204773e-02 -1.56135360e-02]                                   \n",
      "                                     [ 3.39255511e-02  6.16608291e-02  7.28741971e-02 ... -8.10447710e-03\n",
      "                                      -1.76480250e-02  4.30595236e-02]                                   \n",
      "                                     [ 1.09437734e-01 -4.06633925e-01 -2.40301100e-01 ...  7.16680671e-02\n",
      "                                       1.06628838e-01  1.09260350e-01]]                                  \n",
      "Epoch 80, loss: 12.377157\n",
      "== W == -2.820545916956256\n",
      "enter of the function =  [[-0.13492181 -0.06070892 -0.03081186 ...  0.16549157 -0.20918694 [6 1 0 ... 1 0 1]\n",
      "                           -0.15576   ]                                                                     \n",
      "                          [-0.3225812  -0.0313531  -0.2144714  ...  0.56964786 -0.44566086                  \n",
      "                           -0.47067536]                                                                     \n",
      "                          [-0.08048075 -0.03670405 -0.16985487 ...  0.14914647 -0.01654137                  \n",
      "                           -0.04139889]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.05336113 -0.06976178  0.05250886 ... -0.07578902 -0.10868633                  \n",
      "                            0.00951997]                                                                     \n",
      "                          [-0.29870046 -0.12178348 -0.25127673 ...  0.68920734 -0.4219579                   \n",
      "                           -0.46802767]                                                                     \n",
      "                          [ 0.04431187 -0.0686754  -0.15506682 ...  0.14397957  0.10032463                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.01330425]]                                                                    \n",
      "soft max = [[9.54076111e-06 1.02757438e-05 1.05875969e-05 ... 1.28840052e-05\n",
      "             8.85788595e-06 9.34400605e-06]                                 \n",
      "            [7.90830815e-06 1.05818680e-05 8.81120024e-06 ... 1.93007301e-05\n",
      "             6.99247246e-06 6.81972883e-06]                                 \n",
      "            [1.00745689e-05 1.05253962e-05 9.21322723e-06 ... 1.26751267e-05\n",
      "             1.07397703e-05 1.04760969e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.15173685e-05 1.01831387e-05 1.15075567e-05 ... 1.01219471e-05\n",
      "             9.79437984e-06 1.10233422e-05]                                 \n",
      "            [8.09943745e-06 9.66693776e-06 8.49279654e-06 ... 2.17519282e-05\n",
      "             7.16019466e-06 6.83780926e-06]                                 \n",
      "            [1.14136150e-05 1.01942076e-05 9.35048536e-06 ... 1.26098044e-05\n",
      "             1.20711668e-05 1.07745925e-05]]                                \n",
      "log =  103001.58986294552\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.444621095882836 [[ 9.54076111e-06  1.02757438e-05  1.05875969e-05 ...  1.28840052e-05 \n",
      "                                                   8.85788595e-06  9.34400605e-06]                                    \n",
      "                                                 [ 7.90830815e-06 -1.00529243e-04  8.81120024e-06 ...  1.93007301e-05 \n",
      "                                                   6.99247246e-06  6.81972883e-06]                                    \n",
      "                                                 [-1.01036542e-04  1.05253962e-05  9.21322723e-06 ...  1.26751267e-05 \n",
      "                                                   1.07397703e-05  1.04760969e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.15173685e-05 -1.00927972e-04  1.15075567e-05 ...  1.01219471e-05 \n",
      "                                                   9.79437984e-06  1.10233422e-05]                                    \n",
      "                                                 [-1.03011674e-04  9.66693776e-06  8.49279654e-06 ...  2.17519282e-05 \n",
      "                                                   7.16019466e-06  6.83780926e-06]                                    \n",
      "                                                 [ 1.14136150e-05 -1.00916904e-04  9.35048536e-06 ...  1.26098044e-05 \n",
      "                                                   1.20711668e-05  1.07745925e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.444621095882836 [[ 9.54076111e-06  1.02757438e-05  1.05875969e-05 ...  1.28840052e-05 [[-1.50142568e-03 -3.53930069e-04 -4.62437740e-04 ...  3.15829706e-03 \n",
      "                                                              8.85788595e-06  9.34400605e-06]                                      -2.95002537e-03 -1.76850992e-03]                                    \n",
      "                                                            [ 7.90830815e-06 -1.00529243e-04  8.81120024e-06 ...  1.93007301e-05  [-2.06595684e-03  3.87633523e-04  6.02184456e-05 ...  2.95165448e-03 \n",
      "                                                              6.99247246e-06  6.81972883e-06]                                      -3.29812347e-03 -1.88706468e-03]                                    \n",
      "                                                            [-1.01036542e-04  1.05253962e-05  9.21322723e-06 ...  1.26751267e-05  [-2.37337219e-03  9.55369268e-04  6.19984637e-04 ...  2.97277768e-03 \n",
      "                                                              1.07397703e-05  1.04760969e-05]                                      -3.39897533e-03 -2.57235608e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.15173685e-05 -1.00927972e-04  1.15075567e-05 ...  1.01219471e-05  [-1.22774923e-03  7.72029927e-04 -1.31439723e-03 ...  4.02571241e-03 \n",
      "                                                              9.79437984e-06  1.10233422e-05]                                      -2.58213610e-03 -2.58215327e-03]                                    \n",
      "                                                            [-1.03011674e-04  9.66693776e-06  8.49279654e-06 ...  2.17519282e-05  [-5.74085694e-04 -4.23854534e-04 -1.53682759e-03 ...  4.34606248e-03 \n",
      "                                                              7.16019466e-06  6.83780926e-06]                                      -2.05763130e-03 -2.60882894e-03]                                    \n",
      "                                                            [ 1.14136150e-05 -1.00916904e-04  9.35048536e-06 ...  1.26098044e-05  [ 3.34815808e-02 -1.02132480e-01 -5.18344002e-02 ...  2.45683527e-02 \n",
      "                                                              1.20711668e-05  1.07745925e-05]]                                      3.36414370e-02  3.83679554e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.9523451211093379 [[-1.94459230e-02 -3.58857380e-02 -5.64363373e-02 ... -1.91508932e-02\n",
      "                                      -2.98324281e-02  7.11222631e-03]                                   \n",
      "                                     [ 1.03811454e-02 -1.78709866e-02 -7.56406579e-02 ...  1.24361366e-01\n",
      "                                      -4.54366935e-05  6.07053644e-02]                                   \n",
      "                                     [-3.89290153e-03  1.01312045e-01 -7.16457615e-02 ...  2.78170848e-02\n",
      "                                      -1.12134658e-01 -5.56277658e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-7.46938711e-02  2.88078314e-02 -4.28032735e-02 ...  1.18718819e-02\n",
      "                                      -1.56001718e-02 -1.57951262e-02]                                   \n",
      "                                     [ 3.42592631e-02  6.22731928e-02  7.35877370e-02 ... -8.14275156e-03\n",
      "                                      -1.78447486e-02  4.34644139e-02]                                   \n",
      "                                     [ 1.10867310e-01 -4.11720520e-01 -2.43221660e-01 ...  7.26301137e-02\n",
      "                                       1.08031719e-01  1.10736726e-01]]                                  \n",
      "Epoch 81, loss: 12.396966\n",
      "== W == -2.863441910637754\n",
      "enter of the function =  [[-0.02934194 -0.00303187 -0.06658343 ...  0.02084145  0.04522368 [3 9 5 ... 2 1 2]\n",
      "                           -0.02972628]                                                                     \n",
      "                          [-0.12443846 -0.15098398 -0.10654324 ...  0.29358764 -0.15837796                  \n",
      "                           -0.14714745]                                                                     \n",
      "                          [-0.15878123 -0.09240595 -0.12061885 ...  0.40072956 -0.22768354                  \n",
      "                           -0.27642995]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.07899475 -0.0143534  -0.10989837 ...  0.06701212 -0.13491404                  \n",
      "                           -0.09252676]                                                                     \n",
      "                          [-0.00380139 -0.08600888  0.08035669 ... -0.13694289  0.07512674                  \n",
      "                            0.07221486]                                                                     \n",
      "                          [-0.01027964 -0.01617361 -0.08284337 ...  0.01446964 -0.08269043                  \n",
      "                           -0.06940528]]                                                                    \n",
      "soft max = [[1.05957652e-05 1.08782402e-05 1.02084206e-05 ... 1.11410647e-05\n",
      "             1.14160474e-05 1.05916937e-05]                                 \n",
      "            [9.63457213e-06 9.38218221e-06 9.80853693e-06 ... 1.46345328e-05\n",
      "             9.31306639e-06 9.41824633e-06]                                 \n",
      "            [9.30931142e-06 9.94818784e-06 9.67144282e-06 ... 1.62895845e-05\n",
      "             8.68947759e-06 8.27605500e-06]                                 \n",
      "            ...                                                             \n",
      "            [1.00825036e-05 1.07557765e-05 9.77568311e-06 ... 1.16675149e-05\n",
      "             9.53417119e-06 9.94698605e-06]                                 \n",
      "            [1.08698724e-05 1.00120311e-05 1.18242563e-05 ... 9.51484741e-06\n",
      "             1.17625775e-05 1.17283761e-05]                                 \n",
      "            [1.07996823e-05 1.07362165e-05 1.00437745e-05 ... 1.10703017e-05\n",
      "             1.00453107e-05 1.01796545e-05]]                                \n",
      "log =  103010.2574668652\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.445584162985021 [[ 1.05957652e-05  1.08782402e-05  1.02084206e-05 ...  1.11410647e-05 \n",
      "                                                   1.14160474e-05  1.05916937e-05]                                    \n",
      "                                                 [ 9.63457213e-06  9.38218221e-06  9.80853693e-06 ...  1.46345328e-05 \n",
      "                                                   9.31306639e-06 -1.01692865e-04]                                    \n",
      "                                                 [ 9.30931142e-06  9.94818784e-06  9.67144282e-06 ...  1.62895845e-05 \n",
      "                                                   8.68947759e-06  8.27605500e-06]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.00825036e-05  1.07557765e-05 -1.01335428e-04 ...  1.16675149e-05 \n",
      "                                                   9.53417119e-06  9.94698605e-06]                                    \n",
      "                                                 [ 1.08698724e-05 -1.01099080e-04  1.18242563e-05 ...  9.51484741e-06 \n",
      "                                                   1.17625775e-05  1.17283761e-05]                                    \n",
      "                                                 [ 1.07996823e-05  1.07362165e-05 -1.01067337e-04 ...  1.10703017e-05 \n",
      "                                                   1.00453107e-05  1.01796545e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.445584162985021 [[ 1.05957652e-05  1.08782402e-05  1.02084206e-05 ...  1.11410647e-05 [[-1.52355718e-03 -3.56508764e-04 -4.77092404e-04 ...  3.22007352e-03 \n",
      "                                                              1.14160474e-05  1.05916937e-05]                                      -2.98554265e-03 -1.80426325e-03]                                    \n",
      "                                                            [ 9.63457213e-06  9.38218221e-06  9.80853693e-06 ...  1.46345328e-05  [-2.08843859e-03  3.85261538e-04  4.54619068e-05 ...  3.01603324e-03 \n",
      "                                                              9.31306639e-06 -1.01692865e-04]                                      -3.33506618e-03 -1.92448316e-03]                                    \n",
      "                                                            [ 9.30931142e-06  9.94818784e-06  9.67144282e-06 ...  1.62895845e-05  [-2.39569823e-03  9.52655770e-04  6.04877132e-04 ...  3.04045200e-03 \n",
      "                                                              8.68947759e-06  8.27605500e-06]                                      -3.43650253e-03 -2.61185898e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.00825036e-05  1.07557765e-05 -1.01335428e-04 ...  1.16675149e-05  [-1.24822401e-03  7.73371348e-04 -1.33131423e-03 ...  4.09369362e-03 \n",
      "                                                              9.53417119e-06  9.94698605e-06]                                      -2.61578320e-03 -2.61935300e-03]                                    \n",
      "                                                            [ 1.08698724e-05 -1.01099080e-04  1.18242563e-05 ...  9.51484741e-06  [-5.94110900e-04 -4.23265878e-04 -1.55366410e-03 ...  4.41651675e-03 \n",
      "                                                              1.17625775e-05  1.17283761e-05]                                      -2.09140776e-03 -2.64769888e-03]                                    \n",
      "                                                            [ 1.07996823e-05  1.07362165e-05 -1.01067337e-04 ...  1.10703017e-05  [ 3.34413026e-02 -1.02242188e-01 -5.19162831e-02 ...  2.46009748e-02 \n",
      "                                                              1.00453107e-05  1.01796545e-05]]                                      3.36223199e-02  3.83576906e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.9716038739352497 [[-1.96553964e-02 -3.62481347e-02 -5.70053250e-02 ... -1.93108192e-02\n",
      "                                      -3.01602527e-02  7.16566348e-03]                                   \n",
      "                                     [ 1.04642973e-02 -1.80458201e-02 -7.63964623e-02 ...  1.25634496e-01\n",
      "                                      -7.88722951e-05  6.12935474e-02]                                   \n",
      "                                     [-3.95556427e-03  1.02334719e-01 -7.23560193e-02 ...  2.81249834e-02\n",
      "                                      -1.13289994e-01 -5.62097670e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-7.54530873e-02  2.91036300e-02 -4.32444502e-02 ...  1.20308578e-02\n",
      "                                      -1.57819949e-02 -1.59788990e-02]                                   \n",
      "                                     [ 3.45961149e-02  6.28916862e-02  7.43082461e-02 ... -8.18071845e-03\n",
      "                                      -1.80437724e-02  4.38729697e-02]                                   \n",
      "                                     [ 1.12310799e-01 -4.16859050e-01 -2.46172220e-01 ...  7.36020984e-02\n",
      "                                       1.09448451e-01  1.12227772e-01]]                                  \n",
      "Epoch 82, loss: 12.417188\n",
      "== W == -2.906823344213428\n",
      "enter of the function =  [[-0.0466615  -0.06597279 -0.26677743 ...  0.437741   -0.09530464 [1 2 2 ... 1 7 1]\n",
      "                           -0.23291545]                                                                     \n",
      "                          [ 0.09183409 -0.08735884  0.06369381 ... -0.06739197  0.12217357                  \n",
      "                            0.04593825]                                                                     \n",
      "                          [ 0.08242069 -0.18635864  0.06106422 ... -0.10149757  0.07918014                  \n",
      "                            0.0855797 ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.11318833 -0.10828142 -0.13950466 ...  0.35708647 -0.18548549                  \n",
      "                           -0.28253013]                                                                     \n",
      "                          [ 0.10888014 -0.03847188  0.09060987 ... -0.28369861  0.18364884                  \n",
      "                            0.20121104]                                                                     \n",
      "                          [-0.03474501  0.059023    0.03681632 ...  0.0062289   0.07259231                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.01348535]]                                                                    \n",
      "soft max = [[1.04062750e-05 1.02072443e-05 8.35026322e-06 ... 1.68915160e-05\n",
      "             9.91219527e-06 8.63786145e-06]                                 \n",
      "            [1.19520711e-05 9.99126941e-06 1.16204246e-05 ... 1.01927687e-05\n",
      "             1.23202476e-05 1.14159185e-05]                                 \n",
      "            [1.18400894e-05 9.04952119e-06 1.15899079e-05 ... 9.85099944e-06\n",
      "             1.18017830e-05 1.18775514e-05]                                 \n",
      "            ...                                                             \n",
      "            [9.73650437e-06 9.78439792e-06 9.48361739e-06 ... 1.55826320e-05\n",
      "             9.05742619e-06 8.21975466e-06]                                 \n",
      "            [1.21575530e-05 1.04918484e-05 1.19374480e-05 ... 8.21015561e-06\n",
      "             1.31014030e-05 1.33335248e-05]                                 \n",
      "            [1.05310230e-05 1.15662744e-05 1.13122568e-05 ... 1.09714823e-05\n",
      "             1.17242904e-05 1.10513858e-05]]                                \n",
      "log =  103019.19941807809\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.446577713119789 [[ 1.04062750e-05 -1.00903867e-04  8.35026322e-06 ...  1.68915160e-05 \n",
      "                                                   9.91219527e-06  8.63786145e-06]                                    \n",
      "                                                 [ 1.19520711e-05  9.99126941e-06 -9.94906865e-05 ...  1.01927687e-05 \n",
      "                                                   1.23202476e-05  1.14159185e-05]                                    \n",
      "                                                 [ 1.18400894e-05  9.04952119e-06 -9.95212033e-05 ...  9.85099944e-06 \n",
      "                                                   1.18017830e-05  1.18775514e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 9.73650437e-06 -1.01326713e-04  9.48361739e-06 ...  1.55826320e-05 \n",
      "                                                   9.05742619e-06  8.21975466e-06]                                    \n",
      "                                                 [ 1.21575530e-05  1.04918484e-05  1.19374480e-05 ... -1.02900956e-04 \n",
      "                                                   1.31014030e-05  1.33335248e-05]                                    \n",
      "                                                 [ 1.05310230e-05 -9.95448367e-05  1.13122568e-05 ...  1.09714823e-05 \n",
      "                                                   1.17242904e-05  1.10513858e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.446577713119789 [[ 1.04062750e-05 -1.00903867e-04  8.35026322e-06 ...  1.68915160e-05 [[-1.54598811e-03 -3.59118843e-04 -4.91948240e-04 ...  3.28314290e-03 \n",
      "                                                              9.91219527e-06  8.63786145e-06]                                      -3.02154572e-03 -1.84052816e-03]                                    \n",
      "                                                            [ 1.19520711e-05  9.99126941e-06 -9.94906865e-05 ...  1.01927687e-05  [-2.11122551e-03  3.82858751e-04  3.05003419e-05 ...  3.08175981e-03 \n",
      "                                                              1.23202476e-05  1.14159185e-05]                                      -3.37250986e-03 -1.96243262e-03]                                    \n",
      "                                                            [ 1.18400894e-05  9.04952119e-06 -9.95212033e-05 ...  9.85099944e-06  [-2.41833225e-03  9.49908522e-04  5.89559755e-04 ...  3.10953266e-03 \n",
      "                                                              1.18017830e-05  1.18775514e-05]                                      -3.47454108e-03 -2.65191769e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 9.73650437e-06 -1.01326713e-04  9.48361739e-06 ...  1.55826320e-05  [-1.26899369e-03  7.74700549e-04 -1.34844927e-03 ...  4.16309111e-03 \n",
      "                                                              9.05742619e-06  8.21975466e-06]                                      -2.64991280e-03 -2.65708281e-03]                                    \n",
      "                                                            [ 1.21575530e-05  1.04918484e-05  1.19374480e-05 ... -1.02900956e-04  [-6.14428769e-04 -4.22694167e-04 -1.57071881e-03 ...  4.48842768e-03 \n",
      "                                                              1.31014030e-05  1.33335248e-05]                                      -2.12566900e-03 -2.68711677e-03]                                    \n",
      "                                                            [ 1.05310230e-05 -9.95448367e-05  1.13122568e-05 ...  1.09714823e-05  [ 3.33989615e-02 -1.02354720e-01 -5.20005953e-02 ...  2.46344804e-02 \n",
      "                                                              1.17242904e-05  1.10513858e-05]]                                      3.36018323e-02  3.83463166e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  0.9912532608301609 [[-1.98671860e-02 -3.66141811e-02 -5.75801492e-02 ... -1.94717266e-02\n",
      "                                      -3.04917106e-02  7.21927748e-03]                                   \n",
      "                                     [ 1.05480558e-02 -1.82224257e-02 -7.71599723e-02 ...  1.26921001e-01\n",
      "                                      -1.13011680e-04  6.18872380e-02]                                   \n",
      "                                     [-4.01907689e-03  1.03367593e-01 -7.30735307e-02 ...  2.84366378e-02\n",
      "                                      -1.14457259e-01 -5.67979832e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-7.62201005e-02  2.94024000e-02 -4.36902079e-02 ...  1.21921033e-02\n",
      "                                      -1.59659726e-02 -1.61648815e-02]                                   \n",
      "                                     [ 3.49361350e-02  6.35163704e-02  7.50357919e-02 ... -8.21836047e-03\n",
      "                                      -1.82451242e-02  4.42852224e-02]                                   \n",
      "                                     [ 1.13768320e-01 -4.22050062e-01 -2.49153106e-01 ...  7.45841291e-02\n",
      "                                       1.10879158e-01  1.13733627e-01]]                                  \n",
      "Epoch 83, loss: 12.437831\n",
      "== W == -2.9506869040623216\n",
      "enter of the function =  [[-0.19482475 -0.07697783 -0.04899429 ...  0.49045351 -0.30711434 [1 5 4 ... 1 3 4]\n",
      "                           -0.28921953]                                                                     \n",
      "                          [-0.076003    0.00233413 -0.04021557 ...  0.19242321 -0.19328138                  \n",
      "                           -0.18749732]                                                                     \n",
      "                          [-0.01113768 -0.0069894  -0.06433143 ...  0.07132806 -0.0159493                   \n",
      "                           -0.13806643]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.1875865  -0.06029079 -0.0254256  ...  0.34268535 -0.23673645                  \n",
      "                           -0.17370528]                                                                     \n",
      "                          [-0.08854933 -0.27779422 -0.18458611 ...  0.29364757 -0.17224859                  \n",
      "                           -0.33038494]                                                                     \n",
      "                          [-0.0148193   0.01378105 -0.08137854 ...  0.13740847 -0.05167831                  \n",
      "                           -0.11890934]]                                                                    \n",
      "soft max = [[8.96646569e-06 1.00879183e-05 1.03742009e-05 ... 1.77923722e-05\n",
      "             8.01409601e-06 8.15879758e-06]                                 \n",
      "            [1.00977571e-05 1.09205951e-05 1.04656740e-05 ... 1.32069016e-05\n",
      "             8.98031494e-06 9.03240810e-06]                                 \n",
      "            [1.07744614e-05 1.08192497e-05 1.02163043e-05 ... 1.17006499e-05\n",
      "             1.07227434e-05 9.49010716e-06]                                 \n",
      "            ...                                                             \n",
      "            [9.03160262e-06 1.02576682e-05 1.06216113e-05 ... 1.53482534e-05\n",
      "             8.59843216e-06 9.15784647e-06]                                 \n",
      "            [9.97185870e-06 8.25254894e-06 9.05874167e-06 ... 1.46137651e-05\n",
      "             9.17119633e-06 7.82975636e-06]                                 \n",
      "            [1.07348669e-05 1.10463204e-05 1.00436219e-05 ... 1.24999519e-05\n",
      "             1.03463936e-05 9.67366249e-06]]                                \n",
      "log =  103028.42635098746\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.447602927887495 [[ 8.96646569e-06 -1.01023193e-04  1.03742009e-05 ...  1.77923722e-05 \n",
      "                                                   8.01409601e-06  8.15879758e-06]                                    \n",
      "                                                 [ 1.00977571e-05  1.09205951e-05  1.04656740e-05 ...  1.32069016e-05 \n",
      "                                                   8.98031494e-06  9.03240810e-06]                                    \n",
      "                                                 [ 1.07744614e-05  1.08192497e-05  1.02163043e-05 ...  1.17006499e-05 \n",
      "                                                   1.07227434e-05  9.49010716e-06]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 9.03160262e-06 -1.00853443e-04  1.06216113e-05 ...  1.53482534e-05 \n",
      "                                                   8.59843216e-06  9.15784647e-06]                                    \n",
      "                                                 [ 9.97185870e-06  8.25254894e-06  9.05874167e-06 ...  1.46137651e-05 \n",
      "                                                   9.17119633e-06  7.82975636e-06]                                    \n",
      "                                                 [ 1.07348669e-05  1.10463204e-05  1.00436219e-05 ...  1.24999519e-05 \n",
      "                                                   1.03463936e-05  9.67366249e-06]]                                   \n",
      "loss , grand (prediction), grad by W =  11.447602927887495 [[ 8.96646569e-06 -1.01023193e-04  1.03742009e-05 ...  1.77923722e-05 [[-1.56871990e-03 -3.61760355e-04 -5.07005929e-04 ...  3.34753590e-03 \n",
      "                                                              8.01409601e-06  8.15879758e-06]                                      -3.05803756e-03 -1.87730835e-03]                                    \n",
      "                                                            [ 1.00977571e-05  1.09205951e-05  1.04656740e-05 ...  1.32069016e-05  [-2.13431906e-03  3.80425077e-04  1.53330280e-05 ...  3.14886628e-03 \n",
      "                                                              8.98031494e-06  9.03240810e-06]                                      -3.41045744e-03 -2.00091681e-03]                                    \n",
      "                                                            [ 1.07744614e-05  1.08192497e-05  1.02163043e-05 ...  1.17006499e-05  [-2.44127580e-03  9.47127448e-04  5.74031777e-04 ...  3.18005296e-03 \n",
      "                                                              1.07227434e-05  9.49010716e-06]                                      -3.51309400e-03 -2.69253601e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 9.03160262e-06 -1.00853443e-04  1.06216113e-05 ...  1.53482534e-05  [-1.29006006e-03  7.76016808e-04 -1.36580281e-03 ...  4.23393876e-03 \n",
      "                                                              8.59843216e-06  9.15784647e-06]                                      -2.68452829e-03 -2.69534648e-03]                                    \n",
      "                                                            [ 9.97185870e-06  8.25254894e-06  9.05874167e-06 ...  1.46137651e-05  [-6.35041108e-04 -4.22140027e-04 -1.58799218e-03 ...  4.56182988e-03 \n",
      "                                                              9.17119633e-06  7.82975636e-06]                                      -2.16041837e-03 -2.72708638e-03]                                    \n",
      "                                                            [ 1.07348669e-05  1.10463204e-05  1.00436219e-05 ...  1.24999519e-05  [ 3.33544641e-02 -1.02470177e-01 -5.20874302e-02 ...  2.46688930e-02 \n",
      "                                                              1.03463936e-05  9.67366249e-06]]                                      3.35799010e-02  3.83337678e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.0113012600943614 [[-2.00813177e-02 -3.69839141e-02 -5.81608702e-02 ... -1.96336125e-02\n",
      "                                      -3.08268432e-02  7.27306497e-03]                                   \n",
      "                                     [ 1.06324242e-02 -1.84008214e-02 -7.79312670e-02 ...  1.28221029e-01\n",
      "                                      -1.47866895e-04  6.24864861e-02]                                   \n",
      "                                     [-4.08345098e-03  1.04410768e-01 -7.37983704e-02 ...  2.87520995e-02\n",
      "                                      -1.15636577e-01 -5.73924822e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-7.69949914e-02  2.97041710e-02 -4.41405945e-02 ...  1.23556553e-02\n",
      "                                      -1.61521315e-02 -1.63531012e-02]                                   \n",
      "                                     [ 3.52793520e-02  6.41473071e-02  7.57704427e-02 ... -8.25565979e-03\n",
      "                                      -1.84488321e-02  4.47012035e-02]                                   \n",
      "                                     [ 1.15239993e-01 -4.27294110e-01 -2.52164643e-01 ...  7.55763152e-02\n",
      "                                       1.12323968e-01  1.15254426e-01]]                                  \n",
      "Epoch 84, loss: 12.458904\n",
      "== W == -2.9950286126746577\n",
      "enter of the function =  [[ 0.07883114  0.09934361  0.04263617 ... -0.11399438  0.08997326 [6 3 2 ... 9 6 1]\n",
      "                            0.09682595]                                                                     \n",
      "                          [ 0.14205842 -0.10869946  0.07361043 ...  0.02976689  0.12978243                  \n",
      "                           -0.02913155]                                                                     \n",
      "                          [ 0.03495555 -0.07242623  0.005943   ... -0.03086032 -0.02803177                  \n",
      "                            0.0283341 ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.01706959 -0.04168358  0.01907334 ... -0.06333425 -0.03931346                  \n",
      "                           -0.00774139]                                                                     \n",
      "                          [ 0.06372957  0.0587959  -0.06346512 ... -0.07357131  0.06530192                  \n",
      "                           -0.04483574]                                                                     \n",
      "                          [ 0.04365622 -0.03624106  0.00527113 ... -0.07378562  0.05499091                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.01172157]]                                                                    \n",
      "soft max = [[1.17795366e-05 1.20236591e-05 1.13608003e-05 ... 9.71371025e-06\n",
      "             1.19115195e-05 1.19934257e-05]                                 \n",
      "            [1.25483743e-05 9.76528004e-06 1.17181992e-05 ... 1.12155318e-05\n",
      "             1.23952722e-05 1.05740316e-05]                                 \n",
      "            [1.12738767e-05 1.01260010e-05 1.09514920e-05 ... 1.05557673e-05\n",
      "             1.05856671e-05 1.11994738e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.10740251e-05 1.04421355e-05 1.10962369e-05 ... 1.02184861e-05\n",
      "             1.04669140e-05 1.08026482e-05]                                 \n",
      "            [1.16029835e-05 1.15458792e-05 1.02171489e-05 ... 1.01144125e-05\n",
      "             1.16212418e-05 1.04092721e-05]                                 \n",
      "            [1.13723949e-05 1.04991221e-05 1.09441365e-05 ... 1.01122451e-05\n",
      "             1.15020308e-05 1.10149592e-05]]                                \n",
      "log =  103037.94940476136\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.448661044973484 [[ 1.17795366e-05  1.20236591e-05  1.13608003e-05 ...  9.71371025e-06 \n",
      "                                                   1.19115195e-05  1.19934257e-05]                                    \n",
      "                                                 [ 1.25483743e-05  9.76528004e-06  1.17181992e-05 ...  1.12155318e-05 \n",
      "                                                   1.23952722e-05  1.05740316e-05]                                    \n",
      "                                                 [ 1.12738767e-05  1.01260010e-05 -1.00159619e-04 ...  1.05557673e-05 \n",
      "                                                   1.05856671e-05  1.11994738e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.10740251e-05  1.04421355e-05  1.10962369e-05 ...  1.02184861e-05 \n",
      "                                                   1.04669140e-05 -1.00308463e-04]                                    \n",
      "                                                 [ 1.16029835e-05  1.15458792e-05  1.02171489e-05 ...  1.01144125e-05 \n",
      "                                                   1.16212418e-05  1.04092721e-05]                                    \n",
      "                                                 [ 1.13723949e-05 -1.00611989e-04  1.09441365e-05 ...  1.01122451e-05 \n",
      "                                                   1.15020308e-05  1.10149592e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.448661044973484 [[ 1.17795366e-05  1.20236591e-05  1.13608003e-05 ...  9.71371025e-06 [[-1.59175380e-03 -3.64433330e-04 -5.22266020e-04 ...  3.41328407e-03 \n",
      "                                                              1.19115195e-05  1.19934257e-05]                                      -3.09502090e-03 -1.91460732e-03]                                    \n",
      "                                                            [ 1.25483743e-05  9.76528004e-06  1.17181992e-05 ...  1.12155318e-05  [-2.15772048e-03  3.77960450e-04 -4.06200785e-08 ...  3.21738563e-03 \n",
      "                                                              1.23952722e-05  1.05740316e-05]                                      -3.44891162e-03 -2.03993922e-03]                                    \n",
      "                                                            [ 1.12738767e-05  1.01260010e-05 -1.00159619e-04 ...  1.05557673e-05  [-2.46453025e-03  9.44312497e-04  5.58292610e-04 ...  3.25204711e-03 \n",
      "                                                              1.05856671e-05  1.11994738e-05]                                      -3.55216405e-03 -2.73371753e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.10740251e-05  1.04421355e-05  1.10962369e-05 ...  1.02184861e-05  [-1.31142473e-03  7.77319376e-04 -1.38337518e-03 ...  4.30627138e-03 \n",
      "                                                              1.04669140e-05 -1.00308463e-04]                                      -2.71963284e-03 -2.73414755e-03]                                    \n",
      "                                                            [ 1.16029835e-05  1.15458792e-05  1.02171489e-05 ...  1.01144125e-05  [-6.55949570e-04 -4.21604105e-04 -1.60548451e-03 ...  4.63675895e-03 \n",
      "                                                              1.16212418e-05  1.04092721e-05]                                      -2.19565900e-03 -2.76761125e-03]                                    \n",
      "                                                            [ 1.13723949e-05 -1.00611989e-04  1.09441365e-05 ...  1.01122451e-05  [ 3.33077122e-02 -1.02588660e-01 -5.21768858e-02 ...  2.47042359e-02 \n",
      "                                                              1.15020308e-05  1.10149592e-05]]                                      3.35564489e-02  3.83199746e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.0317560158393517 [[-2.02978181e-02 -3.73573709e-02 -5.87475489e-02 ... -1.97964732e-02\n",
      "                                      -3.11656920e-02  7.32702254e-03]                                   \n",
      "                                     [ 1.07174052e-02 -1.85810254e-02 -7.87104264e-02 ...  1.29534728e-01\n",
      "                                      -1.83450139e-04  6.30913418e-02]                                   \n",
      "                                     [-4.14869825e-03  1.05464347e-01 -7.45306138e-02 ...  2.90714210e-02\n",
      "                                      -1.16828074e-01 -5.79933324e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-7.77778419e-02  3.00089729e-02 -4.45956584e-02 ...  1.25215512e-02\n",
      "                                      -1.63404981e-02 -1.65435856e-02]                                   \n",
      "                                     [ 3.56257951e-02  6.47845588e-02  7.65122672e-02 ... -8.29259809e-03\n",
      "                                      -1.86549246e-02  4.51209446e-02]                                   \n",
      "                                     [ 1.16725937e-01 -4.32591753e-01 -2.55207163e-01 ...  7.65787673e-02\n",
      "                                       1.13783007e-01  1.16790308e-01]]                                  \n",
      "Epoch 85, loss: 12.480417\n",
      "== W == -3.039843780071762\n",
      "enter of the function =  [[ 0.11055724 -0.01680658  0.03014931 ... -0.12181899  0.13952156 [2 7 0 ... 4 0 9]\n",
      "                            0.07806334]                                                                     \n",
      "                          [ 0.12457107 -0.00896832 -0.01753497 ... -0.12276402  0.17543198                  \n",
      "                            0.15701243]                                                                     \n",
      "                          [ 0.18819154 -0.04468784  0.1203477  ... -0.41252162  0.25280908                  \n",
      "                            0.30430781]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.09465502 -0.02275251  0.02401952 ... -0.14356431  0.13290022                  \n",
      "                            0.09233436]                                                                     \n",
      "                          [ 0.09204298 -0.11920009  0.0954874  ... -0.25367101  0.16223036                  \n",
      "                            0.20247183]                                                                     \n",
      "                          [ 0.01615663  0.08970279 -0.06339756 ... -0.03821039  0.0400394                   \n",
      "                           -0.01067533]]                                                                    \n",
      "soft max = [[1.21493489e-05 1.06964485e-05 1.12106885e-05 ... 9.63015532e-06\n",
      "             1.25063924e-05 1.17609143e-05]                                 \n",
      "            [1.23208065e-05 1.07806194e-05 1.06886601e-05 ... 9.62105882e-06\n",
      "             1.29636634e-05 1.27270643e-05]                                 \n",
      "            [1.31301339e-05 1.04023372e-05 1.22688809e-05 ... 7.20083312e-06\n",
      "             1.40065829e-05 1.47468007e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.19576755e-05 1.06330368e-05 1.11421796e-05 ... 9.42300497e-06\n",
      "             1.24238569e-05 1.19299579e-05]                                 \n",
      "            [1.19264823e-05 9.65540877e-06 1.19676329e-05 ... 8.44054882e-06\n",
      "             1.27936469e-05 1.33189812e-05]                                 \n",
      "            [1.10549133e-05 1.18986046e-05 1.02095218e-05 ... 1.04699365e-05\n",
      "             1.13221134e-05 1.07622325e-05]]                                \n",
      "log =  103047.78025397191\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.449753361552435 [[ 1.21493489e-05  1.06964485e-05 -9.99004226e-05 ...  9.63015532e-06 \n",
      "                                                   1.25063924e-05  1.17609143e-05]                                    \n",
      "                                                 [ 1.23208065e-05  1.07806194e-05  1.06886601e-05 ... -1.01490052e-04 \n",
      "                                                   1.29636634e-05  1.27270643e-05]                                    \n",
      "                                                 [-9.79809772e-05  1.04023372e-05  1.22688809e-05 ...  7.20083312e-06 \n",
      "                                                   1.40065829e-05  1.47468007e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.19576755e-05  1.06330368e-05  1.11421796e-05 ...  9.42300497e-06 \n",
      "                                                   1.24238569e-05  1.19299579e-05]                                    \n",
      "                                                 [-9.91846289e-05  9.65540877e-06  1.19676329e-05 ...  8.44054882e-06 \n",
      "                                                   1.27936469e-05  1.33189812e-05]                                    \n",
      "                                                 [ 1.10549133e-05  1.18986046e-05  1.02095218e-05 ...  1.04699365e-05 \n",
      "                                                   1.13221134e-05 -1.00348879e-04]]                                   \n",
      "loss , grand (prediction), grad by W =  11.449753361552435 [[ 1.21493489e-05  1.06964485e-05 -9.99004226e-05 ...  9.63015532e-06 [[-1.61509085e-03 -3.67137769e-04 -5.37728909e-04 ...  3.48041978e-03 \n",
      "                                                              1.25063924e-05  1.17609143e-05]                                      -3.13249822e-03 -1.95242830e-03]                                    \n",
      "                                                            [ 1.23208065e-05  1.07806194e-05  1.06886601e-05 ... -1.01490052e-04  [-2.18143084e-03  3.75464826e-04 -1.56210381e-05 ...  3.28735171e-03 \n",
      "                                                              1.29636634e-05  1.27270643e-05]                                      -3.48787481e-03 -2.07950309e-03]                                    \n",
      "                                                            [-9.79809772e-05  1.04023372e-05  1.22688809e-05 ...  7.20083312e-06  [-2.48809676e-03  9.41463640e-04  5.42341821e-04 ...  3.32555023e-03 \n",
      "                                                              1.40065829e-05  1.47468007e-05]                                      -3.59175371e-03 -2.77546557e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.19576755e-05  1.06330368e-05  1.11421796e-05 ...  9.42300497e-06  [-1.33308915e-03  7.78607477e-04 -1.40116649e-03 ...  4.38012471e-03 \n",
      "                                                              1.24238569e-05  1.19299579e-05]                                      -2.75522937e-03 -2.77348932e-03]                                    \n",
      "                                                            [-9.91846289e-05  9.65540877e-06  1.19676329e-05 ...  8.44054882e-06  [-6.77155637e-04 -4.21087063e-04 -1.62319593e-03 ...  4.71325141e-03 \n",
      "                                                              1.27936469e-05  1.33189812e-05]                                      -2.23139378e-03 -2.80869467e-03]                                    \n",
      "                                                            [ 1.10549133e-05  1.18986046e-05  1.02095218e-05 ...  1.04699365e-05  [ 3.32586028e-02 -1.02710279e-01 -5.22690648e-02 ...  2.47405324e-02 \n",
      "                                                              1.13221134e-05 -1.00348879e-04]]                                      3.35313942e-02  3.83048628e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.052625841612118 [[-2.05167138e-02 -3.77345889e-02 -5.93402471e-02 ... -1.99603051e-02\n",
      "                                     -3.15082991e-02  7.38114669e-03]                                   \n",
      "                                    [ 1.08030020e-02 -1.87630560e-02 -7.94975310e-02 ...  1.30862249e-01\n",
      "                                     -2.19773756e-04  6.37018558e-02]                                   \n",
      "                                    [-4.21483054e-03  1.06528433e-01 -7.52703370e-02 ...  2.93946557e-02\n",
      "                                     -1.18031876e-01 -5.86006029e-02]                                   \n",
      "                                    ...                                                                 \n",
      "                                    [-7.85687346e-02  3.03168358e-02 -4.50554488e-02 ...  1.26898294e-02\n",
      "                                     -1.65310994e-02 -1.67363630e-02]                                   \n",
      "                                    [ 3.59754936e-02  6.54281884e-02  7.72613350e-02 ... -8.32915648e-03\n",
      "                                     -1.88634304e-02  4.55444780e-02]                                   \n",
      "                                    [ 1.18226274e-01 -4.37943557e-01 -2.58281004e-01 ...  7.75915973e-02\n",
      "                                      1.15256402e-01  1.18341411e-01]]                                  \n",
      "Epoch 86, loss: 12.502379\n",
      "== W == -3.08512695166824\n",
      "enter of the function =  [[ 0.05027787 -0.19695315  0.11682159 ... -0.17019322  0.12505463 [7 3 8 ... 4 3 8]\n",
      "                            0.18286832]                                                                     \n",
      "                          [-0.01765082  0.01721082 -0.04355138 ...  0.08555159 -0.0726844                   \n",
      "                            0.01100967]                                                                     \n",
      "                          [ 0.07585233 -0.05146941  0.02255386 ... -0.1563962   0.10350477                  \n",
      "                            0.10480818]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.03547571 -0.10125785  0.01494361 ... -0.14409744  0.08067948                  \n",
      "                            0.10499317]                                                                     \n",
      "                          [-0.21893929 -0.0946831  -0.23033674 ...  0.69293405 -0.49255585                  \n",
      "                           -0.49701254]                                                                     \n",
      "                          [ 0.06618957 -0.05571822  0.04360015 ... -0.26124746  0.13408924                  \n",
      "                            0.21980134]]                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max = [[1.14289526e-05 8.92555780e-06 1.22153525e-05 ... 9.16762949e-06\n",
      "             1.23163371e-05 1.30493756e-05]                                 \n",
      "            [1.06783801e-05 1.10572109e-05 1.04053552e-05 ... 1.18392888e-05\n",
      "             1.01065888e-05 1.09888556e-05]                                 \n",
      "            [1.17250116e-05 1.03232906e-05 1.11164481e-05 ... 9.29499208e-06\n",
      "             1.20537612e-05 1.20694824e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.12610254e-05 9.82189546e-06 1.10321703e-05 ... 9.41001481e-06\n",
      "             1.17817468e-05 1.20717154e-05]                                 \n",
      "            [8.73146072e-06 9.88668466e-06 8.63250934e-06 ... 2.17324388e-05\n",
      "             6.64135572e-06 6.61182310e-06]                                 \n",
      "            [1.16122612e-05 1.02795219e-05 1.13528876e-05 ... 8.36975414e-06\n",
      "             1.24281146e-05 1.35403390e-05]]                                \n",
      "log =  103057.93114164761\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.450881237960846 [[ 1.14289526e-05  8.92555780e-06  1.22153525e-05 ... -1.01943482e-04 \n",
      "                                                   1.23163371e-05  1.30493756e-05]                                    \n",
      "                                                 [ 1.06783801e-05  1.10572109e-05  1.04053552e-05 ...  1.18392888e-05 \n",
      "                                                   1.01065888e-05  1.09888556e-05]                                    \n",
      "                                                 [ 1.17250116e-05  1.03232906e-05  1.11164481e-05 ...  9.29499208e-06 \n",
      "                                                  -9.90573499e-05  1.20694824e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.12610254e-05  9.82189546e-06  1.10321703e-05 ...  9.41001481e-06 \n",
      "                                                   1.17817468e-05  1.20717154e-05]                                    \n",
      "                                                 [ 8.73146072e-06  9.88668466e-06  8.63250934e-06 ...  2.17324388e-05 \n",
      "                                                   6.64135572e-06  6.61182310e-06]                                    \n",
      "                                                 [ 1.16122612e-05  1.02795219e-05  1.13528876e-05 ...  8.36975414e-06 \n",
      "                                                  -9.86829965e-05  1.35403390e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.450881237960846 [[ 1.14289526e-05  8.92555780e-06  1.22153525e-05 ... -1.01943482e-04 [[-1.63873190e-03 -3.69873650e-04 -5.53394838e-04 ...  3.54897630e-03 \n",
      "                                                              1.23163371e-05  1.30493756e-05]                                      -3.17047173e-03 -1.99077428e-03]                                    \n",
      "                                                            [ 1.06783801e-05  1.10572109e-05  1.04053552e-05 ...  1.18392888e-05  [-2.20545098e-03  3.72938187e-04 -3.14085003e-05 ...  3.35879928e-03 \n",
      "                                                              1.01065888e-05  1.09888556e-05]                                      -3.52734911e-03 -2.11961138e-03]                                    \n",
      "                                                            [ 1.17250116e-05  1.03232906e-05  1.11164481e-05 ...  9.29499208e-06  [-2.51197628e-03  9.38580878e-04  5.26179142e-04 ...  3.40059838e-03 \n",
      "                                                             -9.90573499e-05  1.20694824e-05]                                      -3.63186516e-03 -2.81778314e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.12610254e-05  9.82189546e-06  1.10321703e-05 ...  9.41001481e-06  [-1.35505457e-03  7.79880303e-04 -1.41917672e-03 ...  4.45553549e-03 \n",
      "                                                              1.17817468e-05  1.20717154e-05]                                      -2.79132056e-03 -2.81337481e-03]                                    \n",
      "                                                            [ 8.73146072e-06  9.88668466e-06  8.63250934e-06 ...  2.17324388e-05  [-6.98660604e-04 -4.20589587e-04 -1.64112640e-03 ...  4.79134479e-03 \n",
      "                                                              6.64135572e-06  6.61182310e-06]                                      -2.26762532e-03 -2.85033962e-03]                                    \n",
      "                                                            [ 1.16122612e-05  1.02795219e-05  1.13528876e-05 ...  8.36975414e-06  [ 3.32070274e-02 -1.02835146e-01 -5.23640747e-02 ...  2.47778056e-02 \n",
      "                                                             -9.86829965e-05  1.35403390e-05]]                                      3.35046504e-02  3.82883538e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.073919224112356 [[-2.07380319e-02 -3.81156062e-02 -5.99390268e-02 ... -2.01251040e-02\n",
      "                                     -3.18547071e-02  7.43543387e-03]                                   \n",
      "                                    [ 1.08892178e-02 -1.89469319e-02 -8.02926626e-02 ...  1.32203745e-01\n",
      "                                     -2.56850242e-04  6.43180793e-02]                                   \n",
      "                                    [-4.28185981e-03  1.07603132e-01 -7.60176170e-02 ...  2.97218577e-02\n",
      "                                     -1.19248113e-01 -5.92143636e-02]                                   \n",
      "                                    ...                                                                 \n",
      "                                    [-7.93677528e-02  3.06277902e-02 -4.55200149e-02 ...  1.28605290e-02\n",
      "                                     -1.67239627e-02 -1.69314615e-02]                                   \n",
      "                                    [ 3.63284770e-02  6.60782594e-02  7.80177164e-02 ... -8.36531553e-03\n",
      "                                     -1.90743787e-02  4.59718358e-02]                                   \n",
      "                                    [ 1.19741123e-01 -4.43350095e-01 -2.61386504e-01 ...  7.86149186e-02\n",
      "                                      1.16744279e-01  1.19907874e-01]]                                  \n",
      "Epoch 87, loss: 12.524800\n",
      "== W == -3.130871852277653\n",
      "enter of the function =  [[ 0.06687693 -0.01264357 -0.03494218 ... -0.07634388  0.0175288  [3 2 9 ... 3 5 4]\n",
      "                            0.0817065 ]                                                                     \n",
      "                          [-0.11704509  0.00845273 -0.22745604 ...  0.15271369 -0.0921532                   \n",
      "                            0.00388685]                                                                     \n",
      "                          [-0.05009149 -0.11826818 -0.12375844 ...  0.17828488 -0.08476654                  \n",
      "                           -0.13257554]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.01290439 -0.06411255 -0.25687684 ...  0.2178953  -0.11690833                  \n",
      "                           -0.13200559]                                                                     \n",
      "                          [ 0.02848578 -0.10280273  0.14984389 ... -0.00432214  0.02833827                  \n",
      "                            0.05156207]                                                                     \n",
      "                          [ 0.00099059 -0.08865609 -0.06074432 ...  0.02793205  0.03354673                  \n",
      "                            0.05951065]]                                                                    \n",
      "soft max = [[1.16100271e-05 1.07225460e-05 1.04860942e-05 ... 1.00608164e-05\n",
      "             1.10510008e-05 1.17834817e-05]                                 \n",
      "            [9.65955042e-06 1.09511550e-05 8.64979966e-06 ... 1.26506157e-05\n",
      "             9.90301244e-06 1.09012673e-05]                                 \n",
      "            [1.03284343e-05 9.64774318e-06 9.59491962e-06 ... 1.29782786e-05\n",
      "             9.97643349e-06 9.51069215e-06]                                 \n",
      "            ...                                                             \n",
      "            [1.07197497e-05 1.01846293e-05 8.39902271e-06 ... 1.35026707e-05\n",
      "             9.66087150e-06 9.51611437e-06]                                 \n",
      "            [1.11727522e-05 9.79810959e-06 1.26143630e-05 ... 1.08121452e-05\n",
      "             1.11711042e-05 1.14335757e-05]                                 \n",
      "            [1.08697401e-05 9.93770501e-06 1.02189912e-05 ... 1.11665672e-05\n",
      "             1.12294403e-05 1.15248186e-05]]                                \n",
      "log =  103068.4149149752\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.452046101663912 [[ 1.16100271e-05  1.07225460e-05  1.04860942e-05 ...  1.00608164e-05 \n",
      "                                                   1.10510008e-05  1.17834817e-05]                                    \n",
      "                                                 [ 9.65955042e-06  1.09511550e-05 -1.02461311e-04 ...  1.26506157e-05 \n",
      "                                                   9.90301244e-06  1.09012673e-05]                                    \n",
      "                                                 [ 1.03284343e-05  9.64774318e-06  9.59491962e-06 ...  1.29782786e-05 \n",
      "                                                   9.97643349e-06 -1.01600419e-04]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.07197497e-05  1.01846293e-05  8.39902271e-06 ...  1.35026707e-05 \n",
      "                                                   9.66087150e-06  9.51611437e-06]                                    \n",
      "                                                 [ 1.11727522e-05  9.79810959e-06  1.26143630e-05 ...  1.08121452e-05 \n",
      "                                                   1.11711042e-05  1.14335757e-05]                                    \n",
      "                                                 [ 1.08697401e-05  9.93770501e-06  1.02189912e-05 ...  1.11665672e-05 \n",
      "                                                   1.12294403e-05  1.15248186e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.452046101663912 [[ 1.16100271e-05  1.07225460e-05  1.04860942e-05 ...  1.00608164e-05 [[-1.66267756e-03 -3.72640924e-04 -5.69263871e-04 ...  3.61898776e-03 \n",
      "                                                              1.10510008e-05  1.17834817e-05]                                      -3.20894331e-03 -2.02964796e-03]                                    \n",
      "                                                            [ 9.65955042e-06  1.09511550e-05 -1.02461311e-04 ...  1.26506157e-05  [-2.22978152e-03  3.70380537e-04 -4.74031058e-05 ...  3.43176404e-03 \n",
      "                                                              9.90301244e-06  1.09012673e-05]                                      -3.56733631e-03 -2.16026676e-03]                                    \n",
      "                                                            [ 1.03284343e-05  9.64774318e-06  9.59491962e-06 ...  1.29782786e-05  [-2.53616954e-03  9.35664235e-04  5.09804485e-04 ...  3.47722859e-03 \n",
      "                                                              9.97643349e-06 -1.01600419e-04]                                      -3.67250026e-03 -2.86067293e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.07197497e-05  1.01846293e-05  8.39902271e-06 ...  1.35026707e-05  [-1.37732205e-03  7.81137018e-04 -1.43740560e-03 ...  4.53254146e-03 \n",
      "                                                              9.66087150e-06  9.51611437e-06]                                      -2.82790878e-03 -2.85380674e-03]                                    \n",
      "                                                            [ 1.11727522e-05  9.79810959e-06  1.26143630e-05 ...  1.08121452e-05  [-7.20465563e-04 -4.20112380e-04 -1.65927565e-03 ...  4.87107761e-03 \n",
      "                                                              1.11711042e-05  1.14335757e-05]                                      -2.30435596e-03 -2.89254877e-03]                                    \n",
      "                                                            [ 1.08697401e-05  9.93770501e-06  1.02189912e-05 ...  1.11665672e-05  [ 3.31528720e-02 -1.02963380e-01 -5.24620286e-02 ...  2.48160780e-02 \n",
      "                                                              1.12294403e-05  1.15248186e-05]]                                      3.34761256e-02  3.82703636e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.0956448270062782 [[-2.09617995e-02 -3.85004610e-02 -6.05439511e-02 ... -2.02908653e-02\n",
      "                                      -3.22049589e-02  7.48988047e-03]                                   \n",
      "                                     [ 1.09760554e-02 -1.91326719e-02 -8.10959033e-02 ...  1.33559370e-01\n",
      "                                      -2.94692235e-04  6.49400640e-02]                                   \n",
      "                                     [-4.34979817e-03  1.08688549e-01 -7.67725314e-02 ...  3.00530823e-02\n",
      "                                      -1.20476912e-01 -5.98346851e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-8.01749809e-02  3.09418669e-02 -4.59894068e-02 ...  1.30336896e-02\n",
      "                                      -1.69191155e-02 -1.71289099e-02]                                   \n",
      "                                     [ 3.66847751e-02  6.67348361e-02  7.87814823e-02 ... -8.40105524e-03\n",
      "                                      -1.92877987e-02  4.64030508e-02]                                   \n",
      "                                     [ 1.21270604e-01 -4.48811948e-01 -2.64524010e-01 ...  7.96488459e-02\n",
      "                                       1.18246769e-01  1.21489836e-01]]                                  \n",
      "Epoch 88, loss: 12.547691\n",
      "== W == -3.1770713259334853\n",
      "enter of the function =  [[-0.06760087 -0.00412949 -0.0865263  ...  0.11114898 -0.13682352 [2 3 0 ... 9 1 1]\n",
      "                           -0.15199249]                                                                     \n",
      "                          [ 0.09869395  0.07582424  0.0561804  ... -0.02813303  0.07421476                  \n",
      "                            0.01306063]                                                                     \n",
      "                          [ 0.18372239 -0.11297584  0.07635899 ... -0.16083714  0.19761634                  \n",
      "                            0.09708883]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.02899573  0.01298109 -0.03446687 ...  0.06726593 -0.00750155                  \n",
      "                            0.01299226]                                                                     \n",
      "                          [-0.10851645 -0.06122771 -0.12047874 ...  0.38857256 -0.16568077                  \n",
      "                           -0.30248931]                                                                     \n",
      "                          [-0.01685103 -0.05322221 -0.05223684 ...  0.05496109 -0.09000501                  \n",
      "                           -0.0212261 ]]                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max = [[1.01398855e-05 1.08043421e-05 9.94978844e-06 ... 1.21244802e-05\n",
      "             9.46171889e-06 9.31927743e-06]                                 \n",
      "            [1.19744060e-05 1.17036624e-05 1.14760009e-05 ... 1.05480874e-05\n",
      "             1.16848408e-05 1.09916755e-05]                                 \n",
      "            [1.30371109e-05 9.69007092e-06 1.17099227e-05 ... 9.23721506e-06\n",
      "             1.32195120e-05 1.19552010e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.11682320e-05 1.09908013e-05 1.04814886e-05 ... 1.16039263e-05\n",
      "             1.07679705e-05 1.09909241e-05]                                 \n",
      "            [9.73337926e-06 1.02047151e-05 9.61763931e-06 ... 1.60009829e-05\n",
      "             9.19258163e-06 8.01719230e-06]                                 \n",
      "            [1.06677648e-05 1.02867368e-05 1.02968781e-05 ... 1.14620167e-05\n",
      "             9.91523612e-06 1.06211945e-05]]                                \n",
      "log =  103079.2450639175\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.453249451546387 [[ 1.01398855e-05  1.08043421e-05 -1.01161323e-04 ...  1.21244802e-05 \n",
      "                                                   9.46171889e-06  9.31927743e-06]                                    \n",
      "                                                 [ 1.19744060e-05  1.17036624e-05  1.14760009e-05 ...  1.05480874e-05 \n",
      "                                                   1.16848408e-05  1.09916755e-05]                                    \n",
      "                                                 [-9.80740002e-05  9.69007092e-06  1.17099227e-05 ...  9.23721506e-06 \n",
      "                                                   1.32195120e-05  1.19552010e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11682320e-05  1.09908013e-05  1.04814886e-05 ...  1.16039263e-05 \n",
      "                                                   1.07679705e-05 -1.00120187e-04]                                    \n",
      "                                                 [ 9.73337926e-06 -1.00906396e-04  9.61763931e-06 ...  1.60009829e-05 \n",
      "                                                   9.19258163e-06  8.01719230e-06]                                    \n",
      "                                                 [ 1.06677648e-05 -1.00824374e-04  1.02968781e-05 ...  1.14620167e-05 \n",
      "                                                   9.91523612e-06  1.06211945e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.453249451546387 [[ 1.01398855e-05  1.08043421e-05 -1.01161323e-04 ...  1.21244802e-05 [[-1.68692819e-03 -3.75439509e-04 -5.85335890e-04 ...  3.69048921e-03 \n",
      "                                                              9.46171889e-06  9.31927743e-06]                                      -3.24791450e-03 -2.06905170e-03]                                    \n",
      "                                                            [ 1.19744060e-05  1.17036624e-05  1.14760009e-05 ...  1.05480874e-05  [-2.25442281e-03  3.67791911e-04 -6.36047649e-05 ...  3.50628266e-03 \n",
      "                                                              1.16848408e-05  1.09916755e-05]                                      -3.60783783e-03 -2.20147154e-03]                                    \n",
      "                                                            [-9.80740002e-05  9.69007092e-06  1.17099227e-05 ...  9.23721506e-06  [-2.56067700e-03  9.32713770e-04  4.93217954e-04 ...  3.55547886e-03 \n",
      "                                                              1.32195120e-05  1.19552010e-05]                                      -3.71366047e-03 -2.90413728e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.11682320e-05  1.09908013e-05  1.04814886e-05 ...  1.16039263e-05  [-1.39989240e-03  7.82376751e-04 -1.45585268e-03 ...  4.61118134e-03 \n",
      "                                                              1.07679705e-05 -1.00120187e-04]                                      -2.86499612e-03 -2.89478751e-03]                                    \n",
      "                                                            [ 9.73337926e-06 -1.00906396e-04  9.61763931e-06 ...  1.60009829e-05  [-7.42571385e-04 -4.19656170e-04 -1.67764321e-03 ...  4.95248945e-03 \n",
      "                                                              9.19258163e-06  8.01719230e-06]                                      -2.34158772e-03 -2.93532444e-03]                                    \n",
      "                                                            [ 1.06677648e-05 -1.00824374e-04  1.02968781e-05 ...  1.14620167e-05  [ 3.30960166e-02 -1.03095104e-01 -5.25630450e-02 ...  2.48553718e-02 \n",
      "                                                              9.91523612e-06  1.06211945e-05]]                                      3.34457224e-02  3.82508032e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.117811494840885 [[-2.11880443e-02 -3.88891920e-02 -6.11550832e-02 ... -2.04575840e-02\n",
      "                                     -3.25590979e-02  7.54448280e-03]                                   \n",
      "                                    [ 1.10635182e-02 -1.93202948e-02 -8.19073363e-02 ...  1.34929282e-01\n",
      "                                     -3.33312521e-04  6.55678620e-02]                                   \n",
      "                                    [-4.41865785e-03  1.09784791e-01 -7.75351586e-02 ...  3.03883854e-02\n",
      "                                     -1.21718406e-01 -6.04616387e-02]                                   \n",
      "                                    ...                                                                 \n",
      "                                    [-8.09905039e-02  3.12590970e-02 -4.64636750e-02 ...  1.32093519e-02\n",
      "                                     -1.71165858e-02 -1.73287370e-02]                                   \n",
      "                                    [ 3.70444182e-02  6.73979833e-02  7.95527044e-02 ... -8.43635502e-03\n",
      "                                     -1.95037203e-02  4.68381558e-02]                                   \n",
      "                                    [ 1.22814839e-01 -4.54329701e-01 -2.67693871e-01 ...  8.06934951e-02\n",
      "                                      1.19763998e-01  1.23087438e-01]]                                  \n",
      "Epoch 89, loss: 12.571061\n",
      "== W == -3.2237172711643303\n",
      "enter of the function =  [[ 0.10900538 -0.00610024 -0.12990712 ... -0.11118297  0.14267885 [2 3 3 ... 8 0 1]\n",
      "                            0.10738133]                                                                     \n",
      "                          [ 0.01502738  0.00213566 -0.02305042 ...  0.06270391 -0.07878368                  \n",
      "                           -0.05655948]                                                                     \n",
      "                          [ 0.19843327 -0.046748    0.08911298 ... -0.19456774  0.21285636                  \n",
      "                            0.19379369]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.03155499 -0.05490233  0.08596743 ... -0.25235025  0.09770401                  \n",
      "                            0.17564485]                                                                     \n",
      "                          [-0.01193748 -0.1924336  -0.04847052 ... -0.04266596 -0.0010984                   \n",
      "                            0.07588751]                                                                     \n",
      "                          [ 0.093196   -0.09507001  0.1234343  ... -0.12271731  0.19800833                  \n",
      "                            0.14376868]]                                                                    \n",
      "soft max = [[1.20870182e-05 1.07728209e-05 9.51833084e-06 ... 9.69823245e-06\n",
      "             1.25009603e-05 1.20674042e-05]                                 \n",
      "            [1.10028464e-05 1.08619111e-05 1.05917585e-05 ... 1.15401300e-05\n",
      "             1.00175939e-05 1.02427192e-05]                                 \n",
      "            [1.32177403e-05 1.03437102e-05 1.18489541e-05 ... 8.92234564e-06\n",
      "             1.34097624e-05 1.31565576e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.11862082e-05 1.02597071e-05 1.18117411e-05 ... 8.42140242e-06\n",
      "             1.19511873e-05 1.29199350e-05]                                 \n",
      "            [1.07101205e-05 8.94140752e-06 1.03259082e-05 ... 1.03860199e-05\n",
      "             1.08268398e-05 1.16932778e-05]                                 \n",
      "            [1.18974325e-05 9.85576547e-06 1.22626851e-05 ... 9.58701245e-06\n",
      "             1.32121248e-05 1.25145917e-05]]                                \n",
      "log =  103090.4357630434\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.454492862560377 [[ 1.20870182e-05  1.07728209e-05 -1.01592780e-04 ...  9.69823245e-06 \n",
      "                                                   1.25009603e-05  1.20674042e-05]                                    \n",
      "                                                 [ 1.10028464e-05  1.08619111e-05  1.05917585e-05 ...  1.15401300e-05 \n",
      "                                                   1.00175939e-05  1.02427192e-05]                                    \n",
      "                                                 [ 1.32177403e-05  1.03437102e-05  1.18489541e-05 ...  8.92234564e-06 \n",
      "                                                   1.34097624e-05  1.31565576e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.11862082e-05  1.02597071e-05  1.18117411e-05 ...  8.42140242e-06 \n",
      "                                                  -9.91599239e-05  1.29199350e-05]                                    \n",
      "                                                 [-1.00400991e-04  8.94140752e-06  1.03259082e-05 ...  1.03860199e-05 \n",
      "                                                   1.08268398e-05  1.16932778e-05]                                    \n",
      "                                                 [ 1.18974325e-05 -1.01255346e-04  1.22626851e-05 ...  9.58701245e-06 \n",
      "                                                   1.32121248e-05  1.25145917e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.454492862560377 [[ 1.20870182e-05  1.07728209e-05 -1.01592780e-04 ...  9.69823245e-06 [[-1.71148388e-03 -3.78269293e-04 -6.01610572e-04 ...  3.76351664e-03 \n",
      "                                                              1.25009603e-05  1.20674042e-05]                                      -3.28738647e-03 -2.10898753e-03]                                    \n",
      "                                                            [ 1.10028464e-05  1.08619111e-05  1.05917585e-05 ...  1.15401300e-05  [-2.27937491e-03  3.65172371e-04 -8.00131834e-05 ...  3.58239274e-03 \n",
      "                                                              1.00175939e-05  1.02427192e-05]                                      -3.64885472e-03 -2.24322768e-03]                                    \n",
      "                                                            [ 1.32177403e-05  1.03437102e-05  1.18489541e-05 ...  8.92234564e-06  [-2.58549886e-03  9.29729574e-04  4.76419866e-04 ...  3.63538820e-03 \n",
      "                                                              1.34097624e-05  1.31565576e-05]                                      -3.75534688e-03 -2.94817812e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.11862082e-05  1.02597071e-05  1.18117411e-05 ...  8.42140242e-06  [-1.42276620e-03  7.83598599e-04 -1.47451728e-03 ...  4.69149492e-03 \n",
      "                                                             -9.91599239e-05  1.29199350e-05]                                      -2.90258431e-03 -2.93631914e-03]                                    \n",
      "                                                            [-1.00400991e-04  8.94140752e-06  1.03259082e-05 ...  1.03860199e-05  [-7.64978699e-04 -4.19221705e-04 -1.69622838e-03 ...  5.03562091e-03 \n",
      "                                                              1.08268398e-05  1.16932778e-05]                                      -2.37932225e-03 -2.97866856e-03]                                    \n",
      "                                                            [ 1.18974325e-05 -1.01255346e-04  1.22626851e-05 ...  9.58701245e-06  [ 3.30363347e-02 -1.03230447e-01 -5.26672483e-02 ...  2.48957083e-02 \n",
      "                                                              1.32121248e-05  1.25145917e-05]]                                      3.34133376e-02  3.82295774e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.1404282570628932 [[-2.14167940e-02 -3.92818383e-02 -6.17724874e-02 ... -2.06252550e-02\n",
      "                                      -3.29171680e-02  7.59923711e-03]                                   \n",
      "                                     [ 1.11516091e-02 -1.95098198e-02 -8.27270458e-02 ...  1.36313637e-01\n",
      "                                      -3.72724024e-04  6.62015259e-02]                                   \n",
      "                                     [-4.48845120e-03  1.10891967e-01 -7.83055780e-02 ...  3.07278240e-02\n",
      "                                      -1.22972727e-01 -6.10952964e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-8.18144079e-02  3.15795117e-02 -4.69428702e-02 ...  1.33875573e-02\n",
      "                                      -1.73164016e-02 -1.75309723e-02]                                   \n",
      "                                     [ 3.74074367e-02  6.80677666e-02  8.03314550e-02 ... -8.47119367e-03\n",
      "                                      -1.97221734e-02  4.72771841e-02]                                   \n",
      "                                     [ 1.24373947e-01 -4.59903949e-01 -2.70896440e-01 ...  8.17489838e-02\n",
      "                                       1.21296095e-01  1.24700821e-01]]                                  \n",
      "Epoch 90, loss: 12.594921\n",
      "== W == -3.270800571325549\n",
      "enter of the function =  [[ 0.11058011 -0.02805075  0.11081684 ... -0.34570408  0.20814597 [1 9 6 ... 2 7 9]\n",
      "                            0.25707133]                                                                     \n",
      "                          [ 0.21585016 -0.1531378   0.05174301 ... -0.24023681  0.26028847                  \n",
      "                            0.20341602]                                                                     \n",
      "                          [ 0.09048058 -0.04585327  0.05097443 ... -0.31664937  0.13877193                  \n",
      "                            0.24594832]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.01492507 -0.01244673 -0.00196696 ... -0.06660723  0.00124248                  \n",
      "                            0.05121512]                                                                     \n",
      "                          [-0.01811243  0.0310629  -0.06603761 ... -0.00687761 -0.02329345                  \n",
      "                            0.00653773]                                                                     \n",
      "                          [ 0.14626652  0.0288534   0.10481702 ... -0.23510724  0.18452858                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.21684168]]                                                                    \n",
      "soft max = [[1.20941010e-05 1.05285114e-05 1.20969644e-05 ... 7.66323043e-06\n",
      "             1.33335534e-05 1.40021239e-05]                                 \n",
      "            [1.34366744e-05 9.29056992e-06 1.14030483e-05 ... 8.51560953e-06\n",
      "             1.40472433e-05 1.32706352e-05]                                 \n",
      "            [1.18534418e-05 1.03427359e-05 1.13942875e-05 ... 7.88914947e-06\n",
      "             1.24399073e-05 1.38472412e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.09908463e-05 1.06940869e-05 1.08067479e-05 ... 1.01302953e-05\n",
      "             1.08414872e-05 1.13970303e-05]                                 \n",
      "            [1.06336688e-05 1.11696536e-05 1.01360674e-05 ... 1.07538098e-05\n",
      "             1.05787180e-05 1.08990478e-05]                                 \n",
      "            [1.25334895e-05 1.11450015e-05 1.20246020e-05 ... 8.55940311e-06\n",
      "             1.30223392e-05 1.34500036e-05]]                                \n",
      "log =  103102.00191690387\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.455777990767096 [[ 1.20941010e-05 -1.00582600e-04  1.20969644e-05 ...  7.66323043e-06 \n",
      "                                                   1.33335534e-05  1.40021239e-05]                                    \n",
      "                                                 [ 1.34366744e-05  9.29056992e-06  1.14030483e-05 ...  8.51560953e-06 \n",
      "                                                   1.40472433e-05 -9.78404759e-05]                                    \n",
      "                                                 [ 1.18534418e-05  1.03427359e-05  1.13942875e-05 ...  7.88914947e-06 \n",
      "                                                   1.24399073e-05  1.38472412e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.09908463e-05  1.06940869e-05 -1.00304363e-04 ...  1.01302953e-05 \n",
      "                                                   1.08414872e-05  1.13970303e-05]                                    \n",
      "                                                 [ 1.06336688e-05  1.11696536e-05  1.01360674e-05 ... -1.00357301e-04 \n",
      "                                                   1.05787180e-05  1.08990478e-05]                                    \n",
      "                                                 [ 1.25334895e-05  1.11450015e-05  1.20246020e-05 ...  8.55940311e-06 \n",
      "                                                   1.30223392e-05 -9.76611075e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.455777990767096 [[ 1.20941010e-05 -1.00582600e-04  1.20969644e-05 ...  7.66323043e-06 [[-1.73634440e-03 -3.81130128e-04 -6.18087379e-04 ...  3.83810695e-03 \n",
      "                                                              1.33335534e-05  1.40021239e-05]                                      -3.32736001e-03 -2.14945710e-03]                                    \n",
      "                                                            [ 1.34366744e-05  9.29056992e-06  1.14030483e-05 ...  8.51560953e-06  [-2.30463761e-03  3.62522012e-04 -9.66278458e-05 ...  3.66013290e-03 \n",
      "                                                              1.40472433e-05 -9.78404759e-05]                                      -3.69038757e-03 -2.28553672e-03]                                    \n",
      "                                                            [ 1.18534418e-05  1.03427359e-05  1.13942875e-05 ...  7.88914947e-06  [-2.61063500e-03  9.26711771e-04  4.59410762e-04 ...  3.71699661e-03 \n",
      "                                                              1.24399073e-05  1.38472412e-05]                                      -3.79756014e-03 -2.99279697e-03]                                    \n",
      "                                                            ...                                                                   ...                                                                  \n",
      "                                                            [ 1.09908463e-05  1.06940869e-05 -1.00304363e-04 ...  1.01302953e-05  [-1.44594377e-03  7.84801622e-04 -1.49339843e-03 ...  4.77352305e-03 \n",
      "                                                              1.08414872e-05  1.13970303e-05]                                      -2.94067469e-03 -2.97840326e-03]                                    \n",
      "                                                            [ 1.06336688e-05  1.11696536e-05  1.01360674e-05 ... -1.00357301e-04  [-7.87687871e-04 -4.18809761e-04 -1.71503021e-03 ...  5.12051367e-03 \n",
      "                                                              1.05787180e-05  1.08990478e-05]                                      -2.41756085e-03 -3.02258264e-03]                                    \n",
      "                                                            [ 1.25334895e-05  1.11450015e-05  1.20246020e-05 ...  8.55940311e-06  [ 3.29736931e-02 -1.03369546e-01 -5.27747695e-02 ...  2.49371078e-02 \n",
      "                                                              1.30223392e-05 -9.76611075e-05]]                                      3.33788614e-02  3.82065850e-02]]                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.163504332146809 [[-2.16480768e-02 -3.96784394e-02 -6.23962284e-02 ... -2.07938724e-02\n",
      "                                     -3.32792136e-02  7.65413960e-03]                                   \n",
      "                                    [ 1.12403315e-02 -1.97012663e-02 -8.35551163e-02 ...  1.37712598e-01\n",
      "                                     -4.12939812e-04  6.68411089e-02]                                   \n",
      "                                    [-4.55919070e-03  1.12010183e-01 -7.90838696e-02 ...  3.10714562e-02\n",
      "                                     -1.24240008e-01 -6.17357312e-02]                                   \n",
      "                                    ...                                                                 \n",
      "                                    [-8.26467796e-02  3.19031428e-02 -4.74270441e-02 ...  1.35683478e-02\n",
      "                                     -1.75185914e-02 -1.77356452e-02]                                   \n",
      "                                    [ 3.77738613e-02  6.87442520e-02  8.11178072e-02 ... -8.50554940e-03\n",
      "                                     -1.99431883e-02  4.77201693e-02]                                   \n",
      "                                    [ 1.25948050e-01 -4.65535293e-01 -2.74132077e-01 ...  8.28154307e-02\n",
      "                                      1.22843189e-01  1.26330125e-01]]                                  \n",
      "Epoch 91, loss: 12.619282\n",
      "== W == -3.3183110195486556\n",
      "enter of the function =  [[-2.85475308e-01 -2.37838932e-04 -2.27889806e-01 ...  6.84478584e-01 [5 4 2 ... 4 5 3]\n",
      "                           -2.98002332e-01 -4.34285680e-01]                                                     \n",
      "                          [ 7.22006079e-02 -5.97200491e-02 -5.02273413e-02 ... -8.86437347e-02                  \n",
      "                            6.85474481e-02  8.55754662e-02]                                                     \n",
      "                          [-1.65187793e-02 -9.49280333e-02 -2.12730268e-01 ...  1.51933272e-01                  \n",
      "                           -8.25805229e-02 -1.28136669e-01]                                                     \n",
      "                          ...                                                                                   \n",
      "                          [-2.16885948e-02 -3.99791874e-02 -1.10425131e-01 ...  1.80837982e-02                  \n",
      "                           -1.65598303e-02 -3.42977560e-02]                                                     \n",
      "                          [ 1.23761738e-01 -5.45136228e-02  6.24539406e-02 ... -3.29975527e-01                  \n",
      "                            2.11541407e-01  2.68831329e-01]                                                     \n",
      "                          [-1.19729120e-01 -3.23376239e-02 -2.34673476e-01 ...  1.34368693e-01                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -2.21989995e-01 -1.59346974e-01]]                                                    \n",
      "soft max = [[8.13059283e-06 1.08143214e-05 8.61254053e-06 ... 2.14470634e-05\n",
      "             8.02937600e-06 7.00639615e-06]                                 \n",
      "            [1.16267649e-05 1.01898191e-05 1.02870087e-05 ... 9.89931352e-06\n",
      "             1.15843679e-05 1.17833158e-05]                                 \n",
      "            [1.06396796e-05 9.83729834e-06 8.74409731e-06 ... 1.25917573e-05\n",
      "             9.95951749e-06 9.51597989e-06]                                 \n",
      "            ...                                                             \n",
      "            [1.05848163e-05 1.03929736e-05 9.68602396e-06 ... 1.10142837e-05\n",
      "             1.06392428e-05 1.04521886e-05]                                 \n",
      "            [1.22419782e-05 1.02430100e-05 1.15139931e-05 ... 7.77671196e-06\n",
      "             1.33651498e-05 1.41531962e-05]                                 \n",
      "            [9.59632323e-06 1.04726964e-05 8.55431362e-06 ... 1.23725195e-05\n",
      "             8.66350307e-06 9.22357009e-06]]                                \n",
      "log =  103113.95920932728\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.457106578814143 [[ 8.13059283e-06  1.08143214e-05  8.61254053e-06 ...  2.14470634e-05 \n",
      "                                                   8.02937600e-06  7.00639615e-06]                                    \n",
      "                                                 [ 1.16267649e-05  1.01898191e-05  1.02870087e-05 ...  9.89931352e-06 \n",
      "                                                   1.15843679e-05  1.17833158e-05]                                    \n",
      "                                                 [ 1.06396796e-05  9.83729834e-06 -1.02367014e-04 ...  1.25917573e-05 \n",
      "                                                   9.95951749e-06  9.51597989e-06]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.05848163e-05  1.03929736e-05  9.68602396e-06 ...  1.10142837e-05 \n",
      "                                                   1.06392428e-05  1.04521886e-05]                                    \n",
      "                                                 [ 1.22419782e-05  1.02430100e-05  1.15139931e-05 ...  7.77671196e-06 \n",
      "                                                   1.33651498e-05  1.41531962e-05]                                    \n",
      "                                                 [ 9.59632323e-06  1.04726964e-05  8.55431362e-06 ...  1.23725195e-05 \n",
      "                                                   8.66350307e-06  9.22357009e-06]]                                   \n",
      "loss , grand (prediction), grad by W =  11.457106578814143 [[ 8.13059283e-06  1.08143214e-05  8.61254053e-06 ...  2.14470634e-05 [[-0.00176151 -0.00038402 -0.00063477 ...  0.0039143  -0.00336784 \n",
      "                                                              8.02937600e-06  7.00639615e-06]                                      -0.00219046]                                                    \n",
      "                                                            [ 1.16267649e-05  1.01898191e-05  1.02870087e-05 ...  9.89931352e-06  [-0.00233021  0.00035984 -0.00011345 ...  0.00373954 -0.00373244 \n",
      "                                                              1.15843679e-05  1.17833158e-05]                                      -0.0023284 ]                                                    \n",
      "                                                            [ 1.06396796e-05  9.83729834e-06 -1.02367014e-04 ...  1.25917573e-05  [-0.00263609  0.00092366  0.00044219 ...  0.00380035 -0.0038403  \n",
      "                                                              9.95951749e-06  9.51597989e-06]                                      -0.00303799]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.05848163e-05  1.03929736e-05  9.68602396e-06 ...  1.10142837e-05  [-0.00146943  0.00078598 -0.00151249 ...  0.00485731 -0.00297927 \n",
      "                                                              1.06392428e-05  1.04521886e-05]                                      -0.00302104]                                                    \n",
      "                                                            [ 1.22419782e-05  1.02430100e-05  1.15139931e-05 ...  7.77671196e-06  [-0.0008107  -0.00041842 -0.00173405 ...  0.00520721 -0.0024563  \n",
      "                                                              1.33651498e-05  1.41531962e-05]                                      -0.00306707]                                                    \n",
      "                                                            [ 9.59632323e-06  1.04726964e-05  8.55431362e-06 ...  1.23725195e-05  [ 0.03290795 -0.10351254 -0.05288575 ...  0.02497959  0.03334218 \n",
      "                                                              8.66350307e-06  9.22357009e-06]]                                      0.03818172]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.1870491318370129 [[-2.18819210e-02 -4.00790351e-02 -6.30263715e-02 ... -2.09634300e-02\n",
      "                                      -3.36452793e-02  7.70918643e-03]                                   \n",
      "                                     [ 1.13296884e-02 -1.98946537e-02 -8.43916338e-02 ...  1.39126325e-01\n",
      "                                      -4.53973086e-04  6.74866646e-02]                                   \n",
      "                                     [-4.63088895e-03  1.13139552e-01 -7.98701142e-02 ...  3.14193407e-02\n",
      "                                      -1.25520384e-01 -6.23830164e-02]                                   \n",
      "                                     ...                                                                 \n",
      "                                     [-8.34877069e-02  3.22300223e-02 -4.79162485e-02 ...  1.37517665e-02\n",
      "                                      -1.77231841e-02 -1.79427857e-02]                                   \n",
      "                                     [ 3.81437230e-02  6.94275064e-02  8.19118350e-02 ... -8.53939976e-03\n",
      "                                      -2.01667958e-02  4.81671451e-02]                                   \n",
      "                                     [ 1.27537268e-01 -4.71224341e-01 -2.77401145e-01 ...  8.38929561e-02\n",
      "                                       1.24405410e-01  1.27975492e-01]]                                  \n",
      "Epoch 92, loss: 12.644156\n",
      "== W == -3.3662372378237073\n",
      "enter of the function =  [[-0.13358193 -0.03858815 -0.11492443 ...  0.1842346  -0.23675355 [2 9 2 ... 8 1 9]\n",
      "                           -0.17436102]                                                                     \n",
      "                          [ 0.00813594 -0.05411209 -0.03673963 ...  0.16619524 -0.10113179                  \n",
      "                           -0.09668626]                                                                     \n",
      "                          [-0.05741439 -0.1293892  -0.13843568 ...  0.25600861 -0.15618979                  \n",
      "                           -0.20801131]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.09863589 -0.36365956 -0.13997741 ...  0.22727621 -0.17360393                  \n",
      "                           -0.24227791]                                                                     \n",
      "                          [-0.12492686  0.01776224 -0.10884567 ...  0.30931503 -0.18270966                  \n",
      "                           -0.26032859]                                                                     \n",
      "                          [-0.08853565 -0.02770845 -0.09752013 ...  0.08080862 -0.04060959                  \n",
      "                           -0.02473139]]                                                                    \n",
      "soft max = [[9.45418272e-06 1.03963112e-05 9.63222999e-06 ... 1.29912206e-05\n",
      "             8.52740963e-06 9.07640487e-06]                                 \n",
      "            [1.08935964e-05 1.02361657e-05 1.04155467e-05 ... 1.27589684e-05\n",
      "             9.76600423e-06 9.80951598e-06]                                 \n",
      "            [1.02024185e-05 9.49390475e-06 9.40840574e-06 ... 1.39579300e-05\n",
      "             9.24284185e-06 8.77606283e-06]                                 \n",
      "            ...                                                             \n",
      "            [9.79040967e-06 7.51108280e-06 9.39391162e-06 ... 1.35625918e-05\n",
      "             9.08327909e-06 8.48043104e-06]                                 \n",
      "            [9.53636450e-06 1.09989677e-05 9.69096031e-06 ... 1.47221657e-05\n",
      "             9.00094462e-06 8.32872681e-06]                                 \n",
      "            [9.88979619e-06 1.05100374e-05 9.80133952e-06 ... 1.17147392e-05\n",
      "             1.03753168e-05 1.05413730e-05]]                                \n",
      "log =  103126.3241570551\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.458480461895011 [[ 9.45418272e-06  1.03963112e-05 -1.01478881e-04 ...  1.29912206e-05 \n",
      "                                                   8.52740963e-06  9.07640487e-06]                                    \n",
      "                                                 [ 1.08935964e-05  1.02361657e-05  1.04155467e-05 ...  1.27589684e-05 \n",
      "                                                   9.76600423e-06 -1.01301595e-04]                                    \n",
      "                                                 [ 1.02024185e-05  9.49390475e-06 -1.01702705e-04 ...  1.39579300e-05 \n",
      "                                                   9.24284185e-06  8.77606283e-06]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 9.79040967e-06  7.51108280e-06  9.39391162e-06 ...  1.35625918e-05 \n",
      "                                                  -1.02027832e-04  8.48043104e-06]                                    \n",
      "                                                 [ 9.53636450e-06 -1.00112143e-04  9.69096031e-06 ...  1.47221657e-05 \n",
      "                                                   9.00094462e-06  8.32872681e-06]                                    \n",
      "                                                 [ 9.88979619e-06  1.05100374e-05  9.80133952e-06 ...  1.17147392e-05 \n",
      "                                                   1.03753168e-05 -1.00569738e-04]]                                   \n",
      "loss , grand (prediction), grad by W =  11.458480461895011 [[ 9.45418272e-06  1.03963112e-05 -1.01478881e-04 ...  1.29912206e-05 [[-0.00178698 -0.00038694 -0.00065164 ...  0.00399213 -0.00340881 \n",
      "                                                              8.52740963e-06  9.07640487e-06]                                      -0.002232  ]                                                    \n",
      "                                                            [ 1.08935964e-05  1.02361657e-05  1.04155467e-05 ...  1.27589684e-05  [-0.00235609  0.00035713 -0.00013047 ...  0.00382066 -0.003775   \n",
      "                                                              9.76600423e-06 -1.01301595e-04]                                      -0.00237182]                                                    \n",
      "                                                            [ 1.02024185e-05  9.49390475e-06 -1.01702705e-04 ...  1.39579300e-05  [-0.00266185  0.00092058  0.00042476 ...  0.00388548 -0.00388357 \n",
      "                                                              9.24284185e-06  8.77606283e-06]                                      -0.00308377]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 9.79040967e-06  7.51108280e-06  9.39391162e-06 ...  1.35625918e-05  [-0.00149321  0.00078715 -0.00153181 ...  0.00494289 -0.00301837 \n",
      "                                                             -1.02027832e-04  8.48043104e-06]                                      -0.00306423]                                                    \n",
      "                                                            [ 9.53636450e-06 -1.00112143e-04  9.69096031e-06 ...  1.47221657e-05  [-0.00083401 -0.00041806 -0.00175328 ...  0.00529576 -0.00249555 \n",
      "                                                              9.00094462e-06  8.32872681e-06]                                      -0.00311212]                                                    \n",
      "                                                            [ 9.88979619e-06  1.05100374e-05  9.80133952e-06 ...  1.17147392e-05  [ 0.03283896 -0.10365959 -0.05300032 ...  0.02502317  0.03330316 \n",
      "                                                              1.03753168e-05 -1.00569738e-04]]                                      0.03815486]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.2110722655091117 [[-0.02211836 -0.04048367 -0.06366298 ... -0.02113392 -0.03401541\n",
      "                                       0.00776437]                                                   \n",
      "                                     [ 0.01141968 -0.02009    -0.08523668 ...  0.14055498 -0.00049584\n",
      "                                       0.06813825]                                                   \n",
      "                                     [-0.00470356  0.11428018 -0.08066439 ...  0.03177154 -0.12681399\n",
      "                                      -0.06303723]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.08433728  0.03256018 -0.04841054 ...  0.01393786 -0.01793021\n",
      "                                      -0.01815242]                                                   \n",
      "                                     [ 0.03851705  0.0701176   0.08271361 ... -0.00857272 -0.02039303\n",
      "                                       0.04861815]                                                   \n",
      "                                     [ 0.12914172 -0.47697171 -0.28070401 ...  0.08498168  0.12598289\n",
      "                                       0.12963706]]                                                  \n",
      "Epoch 93, loss: 12.669553\n",
      "== W == -3.4145665896784725\n",
      "enter of the function =  [[ 0.1114807  -0.00716115  0.10250604 ... -0.11870437  0.05583331 [3 3 8 ... 2 1 1]\n",
      "                            0.07463688]                                                                     \n",
      "                          [-0.08437003 -0.02777451 -0.15133183 ...  0.27576305 -0.21676184                  \n",
      "                           -0.20537773]                                                                     \n",
      "                          [ 0.05874654  0.04102366 -0.03113888 ... -0.16904267  0.12834284                  \n",
      "                            0.12954032]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.23491022  0.09195416 -0.16192398 ...  0.65436558 -0.25645957                  \n",
      "                           -0.44205626]                                                                     \n",
      "                          [ 0.10523872 -0.08526073  0.02442727 ... -0.1805242   0.1013602                   \n",
      "                            0.07447018]                                                                     \n",
      "                          [-0.08869529 -0.0800802  -0.16479898 ...  0.28491038 -0.22901679                  \n",
      "                           -0.24167991]]                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max = [[1.20661820e-05 1.07162878e-05 1.19583766e-05 ... 9.58521288e-06\n",
      "             1.14130709e-05 1.16297078e-05]                                 \n",
      "            [9.92002987e-06 1.04976503e-05 9.27751875e-06 ... 1.42205630e-05\n",
      "             8.68992338e-06 8.78941562e-06]                                 \n",
      "            [1.14463683e-05 1.12452928e-05 1.04623917e-05 ... 9.11465253e-06\n",
      "             1.22713687e-05 1.22860723e-05]                                 \n",
      "            ...                                                             \n",
      "            [8.53363783e-06 1.18328566e-05 9.17976843e-06 ... 2.07654705e-05\n",
      "             8.35171067e-06 6.93700203e-06]                                 \n",
      "            [1.19910997e-05 9.91119801e-06 1.10602017e-05 ... 9.01060086e-06\n",
      "             1.19446820e-05 1.16277693e-05]                                 \n",
      "            [9.87721581e-06 9.96267649e-06 9.15341450e-06 ... 1.43512399e-05\n",
      "             8.58407871e-06 8.47606283e-06]]                                \n",
      "log =  103139.11416819214\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.459901574243572 [[ 1.20661820e-05  1.07162878e-05  1.19583766e-05 ...  9.58521288e-06 \n",
      "                                                   1.14130709e-05  1.16297078e-05]                                    \n",
      "                                                 [ 9.92002987e-06  1.04976503e-05  9.27751875e-06 ...  1.42205630e-05 \n",
      "                                                   8.68992338e-06  8.78941562e-06]                                    \n",
      "                                                 [ 1.14463683e-05  1.12452928e-05  1.04623917e-05 ...  9.11465253e-06 \n",
      "                                                  -9.88397424e-05  1.22860723e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 8.53363783e-06  1.18328566e-05 -1.01931343e-04 ...  2.07654705e-05 \n",
      "                                                   8.35171067e-06  6.93700203e-06]                                    \n",
      "                                                 [ 1.19910997e-05 -1.01199913e-04  1.10602017e-05 ...  9.01060086e-06 \n",
      "                                                   1.19446820e-05  1.16277693e-05]                                    \n",
      "                                                 [ 9.87721581e-06 -1.01148435e-04  9.15341450e-06 ...  1.43512399e-05 \n",
      "                                                   8.58407871e-06  8.47606283e-06]]                                   \n",
      "loss , grand (prediction), grad by W =  11.459901574243572 [[ 1.20661820e-05  1.07162878e-05  1.19583766e-05 ...  9.58521288e-06 [[-0.00181275 -0.0003899  -0.00066872 ...  0.00407164 -0.00345029 \n",
      "                                                              1.14130709e-05  1.16297078e-05]                                      -0.00227408]                                                    \n",
      "                                                            [ 9.92002987e-06  1.04976503e-05  9.27751875e-06 ...  1.42205630e-05  [-0.00238228  0.00035439 -0.0001477  ...  0.00390354 -0.00381808 \n",
      "                                                              8.68992338e-06  8.78941562e-06]                                      -0.00241579]                                                    \n",
      "                                                            [ 1.14463683e-05  1.12452928e-05  1.04623917e-05 ...  9.11465253e-06  [-0.00268792  0.00091746  0.00040713 ...  0.00397243 -0.00392736 \n",
      "                                                             -9.88397424e-05  1.22860723e-05]                                      -0.00313013]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 8.53363783e-06  1.18328566e-05 -1.01931343e-04 ...  2.07654705e-05  [-0.0015173   0.00078829 -0.00155133 ...  0.00503032 -0.00305797 \n",
      "                                                              8.35171067e-06  6.93700203e-06]                                      -0.00310798]                                                    \n",
      "                                                            [ 1.19910997e-05 -1.01199913e-04  1.10602017e-05 ...  9.01060086e-06  [-0.00085763 -0.00041772 -0.00177272 ...  0.00538619 -0.00253531 \n",
      "                                                              1.19446820e-05  1.16277693e-05]                                      -0.00315775]                                                    \n",
      "                                                            [ 9.87721581e-06 -1.01148435e-04  9.15341450e-06 ...  1.43512399e-05  [ 0.03276656 -0.10381084 -0.05311865 ...  0.02506787  0.03326168 \n",
      "                                                              8.58407871e-06  8.47606283e-06]]                                      0.03812589]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.2355835446562633 [[-0.02235741 -0.04089237 -0.06430613 ... -0.02130534 -0.03438965\n",
      "                                       0.0078197 ]                                                   \n",
      "                                     [ 0.01151032 -0.02028733 -0.08609036 ...  0.14199874 -0.00053855\n",
      "                                       0.06879591]                                                   \n",
      "                                     [-0.00477721  0.11543219 -0.08146679 ...  0.03212811 -0.12812097\n",
      "                                      -0.06369844]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.08519558  0.03289366 -0.04890996 ...  0.01412666 -0.01813969\n",
      "                                      -0.01836459]                                                   \n",
      "                                     [ 0.03889388  0.07081459  0.08352322 ... -0.00860549 -0.02062191\n",
      "                                       0.04907321]                                                   \n",
      "                                     [ 0.13076153 -0.48277802 -0.28404106 ...  0.08608173  0.12757575\n",
      "                                       0.13131498]]                                                  \n",
      "Epoch 94, loss: 12.695485\n",
      "== W == -3.463285085860201\n",
      "enter of the function =  [[-0.07045278 -0.07897279 -0.1877228  ...  0.33311457 -0.14166187 [8 1 2 ... 5 6 1]\n",
      "                           -0.1362598 ]                                                                     \n",
      "                          [ 0.18064922 -0.17085952  0.16859343 ... -0.29754628  0.32262446                  \n",
      "                            0.22564635]                                                                     \n",
      "                          [ 0.04504435 -0.02929427 -0.07949307 ... -0.23529913  0.20310993                  \n",
      "                            0.15179952]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.28149093 -0.07922792 -0.21384519 ...  0.85585438 -0.49586168                  \n",
      "                           -0.61193826]                                                                     \n",
      "                          [ 0.01733482  0.05841236 -0.08600657 ... -0.01207168  0.05242141                  \n",
      "                           -0.00420613]                                                                     \n",
      "                          [ 0.08072216 -0.07282622  0.07572117 ...  0.09608498  0.04097784                  \n",
      "                           -0.09509241]]                                                                    \n",
      "soft max = [[1.00474081e-05 9.96216779e-06 8.93561236e-06 ... 1.50425380e-05\n",
      "             9.35682118e-06 9.40750413e-06]                                 \n",
      "            [1.29153523e-05 9.08757376e-06 1.27605823e-05 ... 8.00623971e-06\n",
      "             1.48855648e-05 1.35097796e-05]                                 \n",
      "            [1.12775253e-05 1.04695727e-05 9.95698593e-06 ... 8.52044316e-06\n",
      "             1.32087225e-05 1.25480717e-05]                                 \n",
      "            ...                                                             \n",
      "            [8.13582011e-06 9.95962647e-06 8.70521517e-06 ... 2.53713824e-05\n",
      "             6.56600648e-06 5.84641836e-06]                                 \n",
      "            [1.09693202e-05 1.14292956e-05 9.89234189e-06 ... 1.06514476e-05\n",
      "             1.13610279e-05 1.07355574e-05]                                 \n",
      "            [1.16871465e-05 1.00235894e-05 1.16288451e-05 ... 1.18680802e-05\n",
      "             1.12317582e-05 9.80286879e-06]]                                \n",
      "log =  103152.3476060067\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.461371956222967 [[ 1.00474081e-05  9.96216779e-06  8.93561236e-06 ...  1.50425380e-05 \n",
      "                                                  -1.01754290e-04  9.40750413e-06]                                    \n",
      "                                                 [ 1.29153523e-05 -1.02023537e-04  1.27605823e-05 ...  8.00623971e-06 \n",
      "                                                   1.48855648e-05  1.35097796e-05]                                    \n",
      "                                                 [ 1.12775253e-05  1.04695727e-05 -1.01154125e-04 ...  8.52044316e-06 \n",
      "                                                   1.32087225e-05  1.25480717e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 8.13582011e-06  9.95962647e-06  8.70521517e-06 ...  2.53713824e-05 \n",
      "                                                   6.56600648e-06  5.84641836e-06]                                    \n",
      "                                                 [ 1.09693202e-05  1.14292956e-05  9.89234189e-06 ...  1.06514476e-05 \n",
      "                                                   1.13610279e-05  1.07355574e-05]                                    \n",
      "                                                 [ 1.16871465e-05 -1.01087522e-04  1.16288451e-05 ...  1.18680802e-05 \n",
      "                                                   1.12317582e-05  9.80286879e-06]]                                   \n",
      "loss , grand (prediction), grad by W =  11.461371956222967 [[ 1.00474081e-05  9.96216779e-06  8.93561236e-06 ...  1.50425380e-05 [[-0.00183882 -0.00039288 -0.000686   ...  0.00415287 -0.00349227 \n",
      "                                                             -1.01754290e-04  9.40750413e-06]                                      -0.00231669]                                                    \n",
      "                                                            [ 1.29153523e-05 -1.02023537e-04  1.27605823e-05 ...  8.00623971e-06  [-0.00240878  0.00035162 -0.00016513 ...  0.0039882  -0.00386167 \n",
      "                                                              1.48855648e-05  1.35097796e-05]                                      -0.00246032]                                                    \n",
      "                                                            [ 1.12775253e-05  1.04695727e-05 -1.01154125e-04 ...  8.52044316e-06  [-0.00271431  0.00091431  0.00038928 ...  0.00406126 -0.00397168 \n",
      "                                                              1.32087225e-05  1.25480717e-05]                                      -0.00317707]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 8.13582011e-06  9.95962647e-06  8.70521517e-06 ...  2.53713824e-05  [-0.00154169  0.00078941 -0.00157106 ...  0.00511964 -0.00309807 \n",
      "                                                              6.56600648e-06  5.84641836e-06]                                      -0.00315228]                                                    \n",
      "                                                            [ 1.09693202e-05  1.14292956e-05  9.89234189e-06 ...  1.06514476e-05  [-0.00088154 -0.0004174  -0.00179238 ...  0.00547857 -0.00257557 \n",
      "                                                              1.13610279e-05  1.07355574e-05]                                      -0.00320395]                                                    \n",
      "                                                            [ 1.16871465e-05 -1.01087522e-04  1.16288451e-05 ...  1.18680802e-05  [ 0.0326906  -0.10396645 -0.0532409  ...  0.0251137   0.03321759 \n",
      "                                                              1.12317582e-05  9.80286879e-06]]                                      0.03809467]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.2605929875066773 [[-0.02259911 -0.04130519 -0.06495588 ... -0.02147768 -0.03476805\n",
      "                                       0.00787515]                                                   \n",
      "                                     [ 0.0116016  -0.02048666 -0.08695274 ...  0.14345776 -0.00058211\n",
      "                                       0.06945971]                                                   \n",
      "                                     [-0.00485186  0.11659569 -0.08227739 ...  0.03248911 -0.12944145\n",
      "                                      -0.06436672]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.08606271  0.03323048 -0.04941457 ...  0.01431823 -0.01835167\n",
      "                                      -0.01857932]                                                   \n",
      "                                     [ 0.03927425  0.07151856  0.08434072 ... -0.00863768 -0.02085348\n",
      "                                       0.04953236]                                                   \n",
      "                                     [ 0.13239681 -0.48864391 -0.28741265 ...  0.08719323  0.12918412\n",
      "                                       0.13300939]]                                                  \n",
      "Epoch 95, loss: 12.721965\n",
      "== W == -3.512377282360618\n",
      "enter of the function =  [[-0.30691622 -0.06023093 -0.2172427  ...  0.63095561 -0.45246489 [9 6 1 ... 3 1 2]\n",
      "                           -0.45220533]                                                                     \n",
      "                          [-0.07266938  0.02793293 -0.0338607  ... -0.10255019  0.05889368                  \n",
      "                            0.09143039]                                                                     \n",
      "                          [ 0.14737424 -0.01860348  0.0792102  ... -0.28079814  0.24004209                  \n",
      "                            0.2223334 ]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.14384556 -0.0563011   0.05511654 ... -0.17980448  0.19340115                  \n",
      "                            0.13279826]                                                                     \n",
      "                          [-0.04477327 -0.02729834 -0.11385311 ...  0.06773335 -0.07823049                  \n",
      "                           -0.08075676]                                                                     \n",
      "                          [ 0.04540381 -0.0219777  -0.00762272 ... -0.14119793  0.0936907                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.1617665 ]]                                                                    \n",
      "soft max = [[7.92201387e-06 1.01384055e-05 8.66523437e-06 ... 2.02370944e-05\n",
      "             6.84895973e-06 6.85073768e-06]                                 \n",
      "            [1.00130805e-05 1.10728326e-05 1.04093138e-05 ... 9.71830754e-06\n",
      "             1.14210181e-05 1.17987318e-05]                                 \n",
      "            [1.24776109e-05 1.05693488e-05 1.16554265e-05 ... 8.13164737e-06\n",
      "             1.36891529e-05 1.34488697e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.24336590e-05 1.01783261e-05 1.13779607e-05 ... 8.99579448e-06\n",
      "             1.30653387e-05 1.22970565e-05]                                 \n",
      "            [1.02963390e-05 1.04778482e-05 9.60908069e-06 ... 1.15224237e-05\n",
      "             9.95755123e-06 9.93242752e-06]                                 \n",
      "            [1.12679846e-05 1.05337456e-05 1.06860479e-05 ... 9.34988214e-06\n",
      "             1.18254309e-05 1.26584905e-05]]                                \n",
      "log =  103166.04385868656\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.462893762076284 [[ 7.92201387e-06  1.01384055e-05  8.66523437e-06 ...  2.02370944e-05 \n",
      "                                                   6.84895973e-06 -1.04260373e-04]                                    \n",
      "                                                 [ 1.00130805e-05  1.10728326e-05  1.04093138e-05 ...  9.71830754e-06 \n",
      "                                                   1.14210181e-05  1.17987318e-05]                                    \n",
      "                                                 [ 1.24776109e-05 -1.00541762e-04  1.16554265e-05 ...  8.13164737e-06 \n",
      "                                                   1.36891529e-05  1.34488697e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.24336590e-05  1.01783261e-05  1.13779607e-05 ...  8.99579448e-06 \n",
      "                                                   1.30653387e-05  1.22970565e-05]                                    \n",
      "                                                 [ 1.02963390e-05 -1.00633263e-04  9.60908069e-06 ...  1.15224237e-05 \n",
      "                                                   9.95755123e-06  9.93242752e-06]                                    \n",
      "                                                 [ 1.12679846e-05  1.05337456e-05 -1.00425063e-04 ...  9.34988214e-06 \n",
      "                                                   1.18254309e-05  1.26584905e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.462893762076284 [[ 7.92201387e-06  1.01384055e-05  8.66523437e-06 ...  2.02370944e-05 [[-0.00186519 -0.00039589 -0.00070347 ...  0.00423586 -0.00353474 \n",
      "                                                              6.84895973e-06 -1.04260373e-04]                                      -0.00235984]                                                    \n",
      "                                                            [ 1.00130805e-05  1.10728326e-05  1.04093138e-05 ...  9.71830754e-06  [-0.00243558  0.00034881 -0.00018276 ...  0.00407471 -0.00390578 \n",
      "                                                              1.14210181e-05  1.17987318e-05]                                      -0.0025054 ]                                                    \n",
      "                                                            [ 1.24776109e-05 -1.00541762e-04  1.16554265e-05 ...  8.13164737e-06  [-0.002741    0.00091113  0.00037124 ...  0.004152   -0.00401652 \n",
      "                                                              1.36891529e-05  1.34488697e-05]                                      -0.00322458]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.24336590e-05  1.01783261e-05  1.13779607e-05 ...  8.99579448e-06  [-0.00156638  0.0007905  -0.001591   ...  0.00521089 -0.00313868 \n",
      "                                                              1.30653387e-05  1.22970565e-05]                                      -0.00319714]                                                    \n",
      "                                                            [ 1.02963390e-05 -1.00633263e-04  9.60908069e-06 ...  1.15224237e-05  [-0.00090575 -0.00041712 -0.00181224 ...  0.00557293 -0.00261633 \n",
      "                                                              9.95755123e-06  9.93242752e-06]                                      -0.00325072]                                                    \n",
      "                                                            [ 1.12679846e-05  1.05337456e-05 -1.00425063e-04 ...  9.34988214e-06  [ 0.03261088 -0.10412661 -0.05336723 ...  0.02516066  0.03317075 \n",
      "                                                              1.18254309e-05  1.26584905e-05]]                                      0.03806107]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.2861108237790733 [[-0.02284349 -0.04172218 -0.0656123  ... -0.02165092 -0.03515066\n",
      "                                       0.00793074]                                                   \n",
      "                                     [ 0.01169353 -0.02068801 -0.08782392 ...  0.14493222 -0.00062655\n",
      "                                       0.07012971]                                                   \n",
      "                                     [-0.00492753  0.11777079 -0.08309627 ...  0.03285462 -0.13077558\n",
      "                                      -0.06504216]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.08693876  0.03357067 -0.04992443 ...  0.01451261 -0.01856617\n",
      "                                      -0.01879663]                                                   \n",
      "                                     [ 0.03965817  0.07222957  0.0851662  ... -0.00866928 -0.02108778\n",
      "                                       0.04999564]                                                   \n",
      "                                     [ 0.13404768 -0.49457002 -0.29081919 ...  0.0883163   0.13080814\n",
      "                                       0.13472043]]                                                  \n",
      "Epoch 96, loss: 12.749005\n",
      "== W == -3.561826170051176\n",
      "enter of the function =  [[-1.33829668e-01 -6.56970840e-02 -2.08103850e-01 ...  4.81360530e-01 [0 2 3 ... 6 1 4]\n",
      "                           -2.11506068e-01 -4.24735221e-01]                                                     \n",
      "                          [ 7.98212921e-02  6.51093051e-03  2.28921854e-02 ... -2.59597366e-01                  \n",
      "                            1.79445833e-01  1.97063548e-01]                                                     \n",
      "                          [ 1.42623640e-04  1.32977380e-02 -4.28141781e-02 ... -6.39097341e-02                  \n",
      "                            3.02963586e-02  3.89831580e-02]                                                     \n",
      "                          ...                                                                                   \n",
      "                          [ 1.66272269e-01 -6.79116316e-02  4.94915331e-02 ... -2.98713932e-01                  \n",
      "                            1.41694823e-01  2.51631255e-01]                                                     \n",
      "                          [-8.78628348e-02 -1.77141417e-01 -2.06706980e-01 ...  2.14502636e-01                  \n",
      "                           -1.94451900e-01 -1.68623010e-01]                                                     \n",
      "                          [-2.30369026e-01 -1.39203478e-01 -1.30301104e-01 ...  1.70367151e-01                  \n",
      "                           -1.06995481e-01 -3.86981137e-02]]                                                    \n",
      "soft max = [[9.40721218e-06 1.00704887e-05 8.73381669e-06 ... 1.74034219e-05\n",
      "             8.70415283e-06 7.03270278e-06]                                 \n",
      "            [1.16479198e-05 1.08245559e-05 1.10033360e-05 ... 8.29546476e-06\n",
      "             1.28681099e-05 1.30968254e-05]                                 \n",
      "            [1.07558408e-05 1.08982699e-05 1.03035875e-05 ... 1.00885042e-05\n",
      "             1.10851090e-05 1.11818226e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.26997027e-05 1.00482118e-05 1.12999449e-05 ... 7.97723919e-06\n",
      "             1.23913809e-05 1.38313475e-05]                                 \n",
      "            [9.84972447e-06 9.00846688e-06 8.74602522e-06 ... 1.33272252e-05\n",
      "             8.85386792e-06 9.08553244e-06]                                 \n",
      "            [8.54150559e-06 9.35679520e-06 9.44046476e-06 ... 1.27518131e-05\n",
      "             9.66306451e-06 1.03460852e-05]]                                \n",
      "log =  103180.22341573736\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.464469268415263 [[-1.01703899e-04  1.00704887e-05  8.73381669e-06 ...  1.74034219e-05 \n",
      "                                                   8.70415283e-06  7.03270278e-06]                                    \n",
      "                                                 [ 1.16479198e-05  1.08245559e-05 -1.00107775e-04 ...  8.29546476e-06 \n",
      "                                                   1.28681099e-05  1.30968254e-05]                                    \n",
      "                                                 [ 1.07558408e-05  1.08982699e-05  1.03035875e-05 ...  1.00885042e-05 \n",
      "                                                   1.10851090e-05  1.11818226e-05]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.26997027e-05  1.00482118e-05  1.12999449e-05 ...  7.97723919e-06 \n",
      "                                                   1.23913809e-05  1.38313475e-05]                                    \n",
      "                                                 [ 9.84972447e-06 -1.02102644e-04  8.74602522e-06 ...  1.33272252e-05 \n",
      "                                                   8.85386792e-06  9.08553244e-06]                                    \n",
      "                                                 [ 8.54150559e-06  9.35679520e-06  9.44046476e-06 ...  1.27518131e-05 \n",
      "                                                   9.66306451e-06  1.03460852e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.464469268415263 [[-1.01703899e-04  1.00704887e-05  8.73381669e-06 ...  1.74034219e-05 [[-0.00189185 -0.00039893 -0.00072113 ...  0.00432066 -0.00357772 \n",
      "                                                              8.70415283e-06  7.03270278e-06]                                      -0.00240352]                                                    \n",
      "                                                            [ 1.16479198e-05  1.08245559e-05 -1.00107775e-04 ...  8.29546476e-06  [-0.00246268  0.00034598 -0.00020059 ...  0.0041631  -0.00395039 \n",
      "                                                              1.28681099e-05  1.30968254e-05]                                      -0.00255103]                                                    \n",
      "                                                            [ 1.07558408e-05  1.08982699e-05  1.03035875e-05 ...  1.00885042e-05  [-0.002768    0.00090791  0.00035299 ...  0.0042447  -0.00406187 \n",
      "                                                              1.10851090e-05  1.11818226e-05]                                      -0.00327267]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.26997027e-05  1.00482118e-05  1.12999449e-05 ...  7.97723919e-06  [-0.00159137  0.00079157 -0.00161114 ...  0.00530412 -0.00317978 \n",
      "                                                              1.23913809e-05  1.38313475e-05]                                      -0.00324254]                                                    \n",
      "                                                            [ 9.84972447e-06 -1.02102644e-04  8.74602522e-06 ...  1.33272252e-05  [-0.00093026 -0.00041686 -0.0018323  ...  0.00566933 -0.00265759 \n",
      "                                                              8.85386792e-06  9.08553244e-06]                                      -0.00329806]                                                    \n",
      "                                                            [ 8.54150559e-06  9.35679520e-06  9.44046476e-06 ...  1.27518131e-05  [ 0.03252724 -0.10429149 -0.05349782 ...  0.02520878  0.033121   \n",
      "                                                              9.66306451e-06  1.03460852e-05]]                                      0.03802493]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.3121474995834919 [[-0.02309058 -0.04214336 -0.06627545 ... -0.02182507 -0.03553751\n",
      "                                       0.00798645]                                                   \n",
      "                                     [ 0.01178611 -0.0208914  -0.08870398 ...  0.14642229 -0.00067187\n",
      "                                       0.07080595]                                                   \n",
      "                                     [-0.00500421  0.11895761 -0.08392352 ...  0.03322468 -0.1321235 \n",
      "                                      -0.06572483]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.08782381  0.03391429 -0.05043958 ...  0.01470985 -0.01878322\n",
      "                                      -0.01901657]                                                   \n",
      "                                     [ 0.0400457   0.0729477   0.08599974 ... -0.00870024 -0.02132482\n",
      "                                       0.05046309]                                                   \n",
      "                                     [ 0.13571427 -0.50055698 -0.29426105 ...  0.08945106  0.13244793\n",
      "                                       0.13644825]]                                                  \n",
      "Epoch 97, loss: 12.776617\n",
      "== W == -3.611613055112563\n",
      "enter of the function =  [[-0.00498446  0.00902728 -0.13229657 ...  0.20822445 -0.11409999 [4 5 7 ... 1 1 1]\n",
      "                           -0.13532091]                                                                     \n",
      "                          [-0.21534961  0.08967142 -0.04380606 ...  0.24307963 -0.23431382                  \n",
      "                           -0.10763106]                                                                     \n",
      "                          [ 0.02527125 -0.00950697  0.05951923 ... -0.12493556 -0.01348063                  \n",
      "                            0.07251811]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [-0.01935501 -0.08059084  0.06364493 ... -0.07692205 -0.00975081                  \n",
      "                           -0.0031629 ]                                                                     \n",
      "                          [ 0.15208801 -0.09254892  0.09391279 ... -0.27849004  0.28522238                  \n",
      "                            0.23375708]                                                                     \n",
      "                          [ 0.06050082 -0.0004808  -0.02997335 ... -0.02305552  0.08969144                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           -0.0052318 ]]                                                                    \n",
      "soft max = [[1.06868565e-05 1.08376520e-05 9.40933723e-06 ... 1.32265152e-05\n",
      "             9.58212229e-06 9.38092322e-06]                                 \n",
      "            [8.65943497e-06 1.17478531e-05 1.02799257e-05 ... 1.36956562e-05\n",
      "             8.49676298e-06 9.64430929e-06]                                 \n",
      "            [1.10151361e-05 1.06386343e-05 1.13989167e-05 ... 9.47885497e-06\n",
      "             1.05964438e-05 1.15480570e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.05343788e-05 9.90865131e-06 1.14460423e-05 ... 9.94507084e-06\n",
      "             1.06360404e-05 1.07063411e-05]                                 \n",
      "            [1.25044815e-05 9.79086853e-06 1.17977859e-05 ... 8.12957825e-06\n",
      "             1.42851633e-05 1.35685711e-05]                                 \n",
      "            [1.14101112e-05 1.07350950e-05 1.04231129e-05 ... 1.04954682e-05\n",
      "             1.17480883e-05 1.06842136e-05]]                                \n",
      "log =  103194.90795180385\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.46610088353376 [[ 1.06868565e-05  1.08376520e-05  9.40933723e-06 ...  1.32265152e-05 \n",
      "                                                  9.58212229e-06  9.38092322e-06]                                    \n",
      "                                                [ 8.65943497e-06  1.17478531e-05  1.02799257e-05 ...  1.36956562e-05 \n",
      "                                                  8.49676298e-06  9.64430929e-06]                                    \n",
      "                                                [ 1.10151361e-05  1.06386343e-05  1.13989167e-05 ... -1.01632256e-04 \n",
      "                                                  1.05964438e-05  1.15480570e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [ 1.05343788e-05 -1.01202460e-04  1.14460423e-05 ...  9.94507084e-06 \n",
      "                                                  1.06360404e-05  1.07063411e-05]                                    \n",
      "                                                [ 1.25044815e-05 -1.01320243e-04  1.17977859e-05 ...  8.12957825e-06 \n",
      "                                                  1.42851633e-05  1.35685711e-05]                                    \n",
      "                                                [ 1.14101112e-05 -1.00376016e-04  1.04231129e-05 ...  1.04954682e-05 \n",
      "                                                  1.17480883e-05  1.06842136e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.46610088353376 [[ 1.06868565e-05  1.08376520e-05  9.40933723e-06 ...  1.32265152e-05 [[-0.00191881 -0.000402   -0.00073898 ...  0.0044073  -0.00362118 \n",
      "                                                             9.58212229e-06  9.38092322e-06]                                      -0.00244773]                                                    \n",
      "                                                           [ 8.65943497e-06  1.17478531e-05  1.02799257e-05 ...  1.36956562e-05  [-0.00249008  0.00034312 -0.00021861 ...  0.00425341 -0.00399551 \n",
      "                                                             8.49676298e-06  9.64430929e-06]                                      -0.00259721]                                                    \n",
      "                                                           [ 1.10151361e-05  1.06386343e-05  1.13989167e-05 ... -1.01632256e-04  [-0.0027953   0.00090467  0.00033454 ...  0.00433941 -0.00410775 \n",
      "                                                             1.05964438e-05  1.15480570e-05]                                      -0.00332133]                                                    \n",
      "                                                           ...                                                                   ...                                                              \n",
      "                                                           [ 1.05343788e-05 -1.01202460e-04  1.14460423e-05 ...  9.94507084e-06  [-0.00161665  0.00079261 -0.00163149 ...  0.00539939 -0.00322139 \n",
      "                                                             1.06360404e-05  1.07063411e-05]                                      -0.0032885 ]                                                    \n",
      "                                                           [ 1.25044815e-05 -1.01320243e-04  1.17977859e-05 ...  8.12957825e-06  [-0.00095507 -0.00041663 -0.00185257 ...  0.00576781 -0.00269936 \n",
      "                                                             1.42851633e-05  1.35685711e-05]                                      -0.00334596]                                                    \n",
      "                                                           [ 1.14101112e-05 -1.00376016e-04  1.04231129e-05 ...  1.04954682e-05  [ 0.03243947 -0.10446129 -0.05363287 ...  0.02525805  0.03306816 \n",
      "                                                             1.17480883e-05  1.06842136e-05]]                                      0.0379861 ]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.3387136824755963 [[-0.0233404  -0.04256878 -0.06694542 ... -0.02200012 -0.03592866\n",
      "                                       0.00804228]                                                   \n",
      "                                     [ 0.01187934 -0.02109686 -0.08959303 ...  0.14792815 -0.0007181 \n",
      "                                       0.0714885 ]                                                   \n",
      "                                     [-0.00508193  0.12015626 -0.08475922 ...  0.03359938 -0.13348536\n",
      "                                      -0.0664148 ]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.08871796  0.03426134 -0.05096009 ...  0.01490999 -0.01900285\n",
      "                                      -0.01923916]                                                   \n",
      "                                     [ 0.04043685  0.07367301  0.08684142 ... -0.00873055 -0.02156464\n",
      "                                       0.05093474]                                                   \n",
      "                                     [ 0.13739668 -0.50660547 -0.29773864 ...  0.09059766  0.13410362\n",
      "                                       0.13819298]]                                                  \n",
      "Epoch 98, loss: 12.804815\n",
      "== W == -3.6617174293482564\n",
      "enter of the function =  [[ 0.01225291  0.06706542 -0.00187393 ... -0.04992622  0.04840213 [6 7 1 ... 7 5 9]\n",
      "                            0.06629855]                                                                     \n",
      "                          [ 0.10762716 -0.05101316  0.05681549 ... -0.31063989  0.22863442                  \n",
      "                            0.25030107]                                                                     \n",
      "                          [ 0.04597267 -0.14450992 -0.15296234 ...  0.20641429  0.027685                    \n",
      "                           -0.17044809]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.11737926 -0.01262005  0.18066941 ... -0.02750078  0.09292614                  \n",
      "                            0.09634027]                                                                     \n",
      "                          [ 0.00444614 -0.06644485 -0.01092368 ...  0.11009284 -0.08077665                  \n",
      "                           -0.09134586]                                                                     \n",
      "                          [ 0.08667333  0.0460833  -0.00887827 ... -0.16731548  0.14994154                  \n",
      "                            0.19171033]]                                                                    \n",
      "soft max = [[1.08578727e-05 1.14696329e-05 1.07055637e-05 ... 1.02033009e-05\n",
      "             1.12575570e-05 1.14608405e-05]                                 \n",
      "            [1.19444253e-05 1.01922165e-05 1.13526704e-05 ... 7.86165873e-06\n",
      "             1.34808738e-05 1.37761464e-05]                                 \n",
      "            [1.12302404e-05 9.28246915e-06 9.20434052e-06 ... 1.31846305e-05\n",
      "             1.10267320e-05 9.04479470e-06]                                 \n",
      "            ...                                                             \n",
      "            [1.20614784e-05 1.05911363e-05 1.28495259e-05 ... 1.04346993e-05\n",
      "             1.17701144e-05 1.18103678e-05]                                 \n",
      "            [1.07734379e-05 1.00361408e-05 1.06091181e-05 ... 1.19739128e-05\n",
      "             9.89333066e-06 9.78931659e-06]                                 \n",
      "            [1.16967478e-05 1.12314829e-05 1.06308403e-05 ... 9.07317288e-06\n",
      "             1.24606919e-05 1.29921826e-05]]                                \n",
      "log =  103210.12041880166\n",
      "=N_SUMPLES= 9000\n",
      "loss , grand (prediction) =  11.467791157644628 [[ 1.08578727e-05  1.14696329e-05  1.07055637e-05 ...  1.02033009e-05 \n",
      "                                                   1.12575570e-05  1.14608405e-05]                                    \n",
      "                                                 [ 1.19444253e-05  1.01922165e-05  1.13526704e-05 ... -1.03249452e-04 \n",
      "                                                   1.34808738e-05  1.37761464e-05]                                    \n",
      "                                                 [ 1.12302404e-05 -1.01828642e-04  9.20434052e-06 ...  1.31846305e-05 \n",
      "                                                   1.10267320e-05  9.04479470e-06]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 1.20614784e-05  1.05911363e-05  1.28495259e-05 ... -1.00676412e-04 \n",
      "                                                   1.17701144e-05  1.18103678e-05]                                    \n",
      "                                                 [ 1.07734379e-05  1.00361408e-05  1.06091181e-05 ...  1.19739128e-05 \n",
      "                                                   9.89333066e-06  9.78931659e-06]                                    \n",
      "                                                 [ 1.16967478e-05  1.12314829e-05  1.06308403e-05 ...  9.07317288e-06 \n",
      "                                                   1.24606919e-05 -9.81189285e-05]]                                   \n",
      "loss , grand (prediction), grad by W =  11.467791157644628 [[ 1.08578727e-05  1.14696329e-05  1.07055637e-05 ...  1.02033009e-05 [[-0.00194606 -0.0004051  -0.00075703 ...  0.00449584 -0.00366513 \n",
      "                                                              1.12575570e-05  1.14608405e-05]                                      -0.00249247]                                                    \n",
      "                                                            [ 1.19444253e-05  1.01922165e-05  1.13526704e-05 ... -1.03249452e-04  [-0.00251777  0.00034023 -0.00023682 ...  0.00434571 -0.00404112 \n",
      "                                                              1.34808738e-05  1.37761464e-05]                                      -0.00264393]                                                    \n",
      "                                                            [ 1.12302404e-05 -1.01828642e-04  9.20434052e-06 ...  1.31846305e-05  [-0.0028229   0.00090139  0.0003159  ...  0.00443618 -0.00415413 \n",
      "                                                              1.10267320e-05  9.04479470e-06]                                      -0.00337056]                                                    \n",
      "                                                            ...                                                                   ...                                                              \n",
      "                                                            [ 1.20614784e-05  1.05911363e-05  1.28495259e-05 ... -1.00676412e-04  [-0.00164223  0.00079362 -0.00165203 ...  0.00549673 -0.00326349 \n",
      "                                                              1.17701144e-05  1.18103678e-05]                                      -0.003335  ]                                                    \n",
      "                                                            [ 1.07734379e-05  1.00361408e-05  1.06091181e-05 ...  1.19739128e-05  [-0.00098016 -0.00041643 -0.00187303 ...  0.00586843 -0.00274161 \n",
      "                                                              9.89333066e-06  9.78931659e-06]                                      -0.00339442]                                                    \n",
      "                                                            [ 1.16967478e-05  1.12314829e-05  1.06308403e-05 ...  9.07317288e-06  [ 0.03234736 -0.10463621 -0.05377257 ...  0.02530847  0.03301205 \n",
      "                                                              1.24606919e-05 -9.81189285e-05]]                                      0.03794439]]                                                   \n",
      "enter of L2  (3073, 10) 10.0\n",
      "L2 loss, grad =  1.3658202666734025 [[-0.02359299 -0.04299849 -0.06762226 ... -0.02217605 -0.03632416\n",
      "                                       0.00809822]                                                   \n",
      "                                     [ 0.01197323 -0.02130439 -0.09049114 ...  0.14944996 -0.00076523\n",
      "                                       0.07217741]                                                   \n",
      "                                     [-0.00516071  0.12136687 -0.08560347 ...  0.03397876 -0.13486129\n",
      "                                      -0.06711216]                                                   \n",
      "                                     ...                                                             \n",
      "                                     [-0.0896213   0.03461188 -0.05148601 ...  0.01511308 -0.01922509\n",
      "                                      -0.01946444]                                                   \n",
      "                                     [ 0.04083167  0.07440557  0.08769131 ... -0.00876018 -0.02180728\n",
      "                                       0.05141063]                                                   \n",
      "                                     [ 0.13909504 -0.51271613 -0.30125236 ...  0.09175622  0.13577533\n",
      "                                       0.13995477]]                                                  \n",
      "Epoch 99, loss: 12.833611\n",
      "== W == -3.712116829365282\n",
      "Accuracy after training for 100 epochs:  0.1\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.16.4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
