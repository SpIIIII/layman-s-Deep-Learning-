{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fmt_items(lines,max_lines=0):\n",
    "    max_width=max([len(line)for line in lines])\n",
    "    empty =' '*max_width\n",
    "    lines = [line.ljust(max_width)for line in lines]\n",
    "    lines += [empty]*(max_lines - len(lines))\n",
    "    return lines\n",
    "    \n",
    "def pp (*list):\n",
    "    lines = [ str(item).split('\\n') for item in list]\n",
    "    max_lines=max([len(item)for  item in lines])\n",
    "    lines = [fmt_items(item,max_lines=max_lines)for item in lines]\n",
    "    lines_t= np.array(lines).T\n",
    "    print('\\n'.join([' '.join(line) for  line in lines_t]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_check at (0,)  num = 6.000000003503203 anal = 6.0\n",
      "Gradient check passed!\n",
      "grad_check at (0,)  num = 1.0000000028043132 anal = 1.0\n",
      "grad_check at (1,)  num = 1.0000000005838672 anal = 1.0\n",
      "Gradient check passed!\n",
      "grad_check at (0, 0)  num = 1.0000000028043132 anal = 1.0\n",
      "grad_check at (0, 1)  num = 1.0000000005838672 anal = 1.0\n",
      "grad_check at (1, 0)  num = 0.9999999983634211 anal = 1.0\n",
      "grad_check at (1, 1)  num = 0.9999999983634211 anal = 1.0\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(array_sum(np.array([5.0, 2.0]))[0]-array_sum(np.array([1.0, 2.0]))[0])/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max = [2.06106005e-09 4.53978686e-05 9.99954600e-01]\n",
      "[2.06106005e-09 4.53978686e-05 9.99954600e-01]\n",
      "soft max = [1. 0. 0.]\n",
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import linear_classifer \n",
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "print(probs)\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "print(probs)\n",
    "# assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max = [4.50940412e-05 6.69254912e-03 9.93262357e-01]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-240e325b8705>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[1;34m(probs, target_index)\u001b[0m\n\u001b[0;32m     37\u001b[0m     '''\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# log = - np.log(probs[target_index])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mpp\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'log = '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "    \n",
    "linear_classifer.cross_entropy_loss(probs, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = linear_classifer.softmax(np.array([1, 0, 0]))\n",
    "linear_classifer.cross_entropy_loss(probs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [1. 0. 0.] 0\n",
      "soft max = [0.57611688 0.21194156 0.21194156]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6b65fea27065>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloss1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mloss2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss,grad = '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36msoftmax_with_cross_entropy\u001b[1;34m(predictions, target_index)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0msoftmax_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmax_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[1;34m(probs, target_index)\u001b[0m\n\u001b[0;32m     37\u001b[0m     '''\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# log = - np.log(probs[target_index])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mpp\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'log = '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "loss, grad= linear_classifer.softmax_with_cross_entropy(np.array([1., 0., 0.]), 0)\n",
    "loss1, grad1= linear_classifer.softmax_with_cross_entropy(np.array([3, 0, 0]), 0)\n",
    "loss2, grad2 = linear_classifer.softmax_with_cross_entropy(np.array([-1, 0, 0]), 0)\n",
    "print((loss1-loss2)/4)\n",
    "print('loss,grad = ',loss, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [1. 0. 0.] 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2837c8be80ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO Implement combined function orsoftmax and cross entropy and produces gradient #-2.0491237621641485\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcheck_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\gradient_check.py\u001b[0m in \u001b[0;36mcheck_gradient\u001b[1;34m(f, x, delta, tol)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0morig_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mfx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalytic_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Functions shouldn't modify input variables\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-2837c8be80ff>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO Implement combined function orsoftmax and cross entropy and produces gradient #-2.0491237621641485\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcheck_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36msoftmax_with_cross_entropy\u001b[1;34m(predictions, target_index)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0msoftmax_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmax_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\layman-s-Deep-Learning-\\assignments\\assignment1\\linear_classifer.py\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[1;34m(probs, target_index)\u001b[0m\n\u001b[0;32m     37\u001b[0m     '''\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;31m# pp ('log = ',log)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "# TODO Implement combined function orsoftmax and cross entropy and produces gradient #-2.0491237621641485\n",
    "\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([ 1,0,0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[ 1.  2. -1.  1.]]\n",
      "targets =  [[2]]\n",
      "enter of the function =  [[ 1.  2. -1.  1.]] [[2]]\n",
      "soft max = [[0.20603191 0.56005279 0.02788339 0.20603191]]\n",
      "log =  3.5797242232074917\n",
      "NNN SAMPLES!!! =  1\n",
      "loss , grand (prediction) =  3.5797242232074917 [[ 0.20603191  0.56005279 -0.97211661  0.20603191]] \n",
      "                                                                                                    \n",
      "enter of the function =  [[ 1.00002  2.      -1.       1.     ]] [[2]]\n",
      "soft max = [[0.20603518 0.56005049 0.02788327 0.20603106]]\n",
      "log =  3.5797283438783922\n",
      "NNN SAMPLES!!! =  1\n",
      "loss , grand (prediction) =  3.5797283438783922 [[ 0.20603518  0.56005049 -0.97211673  0.20603106]] \n",
      "                                                                                                    \n",
      "enter of the function =  [[ 0.99998  2.      -1.       1.     ]] [[2]]\n",
      "soft max = [[0.20602864 0.5600551  0.0278835  0.20603276]]\n",
      "log =  3.5797201026020242\n",
      "NNN SAMPLES!!! =  1\n",
      "loss , grand (prediction) =  3.5797201026020242 [[ 0.20602864  0.5600551  -0.9721165   0.20603276]] \n",
      "                                                                                                    \n",
      "grad_check at (0, 0)  num = 0.20603190920009948 anal = 0.20603190919001857\n",
      "enter of the function =  [[ 1.       2.00002 -1.       1.     ]] [[2]]\n",
      "soft max = [[0.2060296  0.56005772 0.02788307 0.2060296 ]]\n",
      "log =  3.579735424312667\n",
      "NNN SAMPLES!!! =  1\n",
      "loss , grand (prediction) =  3.579735424312667 [[ 0.2060296   0.56005772 -0.97211693  0.2060296 ]] \n",
      "                                                                                                   \n",
      "enter of the function =  [[ 1.       1.99998 -1.       1.     ]] [[2]]\n",
      "soft max = [[0.20603422 0.56004787 0.0278837  0.20603422]]\n",
      "log =  3.5797130222008735\n",
      "NNN SAMPLES!!! =  1\n",
      "loss , grand (prediction) =  3.5797130222008735 [[ 0.20603422  0.56004787 -0.9721163   0.20603422]] \n",
      "                                                                                                    \n",
      "grad_check at (0, 1)  num = 0.5600527948401712 anal = 0.5600527948339517\n",
      "enter of the function =  [[ 1.       2.      -0.99998  1.     ]] [[2]]\n",
      "soft max = [[0.20603179 0.56005248 0.02788393 0.20603179]]\n",
      "log =  3.5797047808806486\n",
      "NNN SAMPLES!!! =  1\n",
      "loss , grand (prediction) =  3.5797047808806486 [[ 0.20603179  0.56005248 -0.97211607  0.20603179]] \n",
      "                                                                                                    \n",
      "enter of the function =  [[ 1.       2.      -1.00002  1.     ]] [[2]]\n",
      "soft max = [[0.20603202 0.56005311 0.02788284 0.20603202]]\n",
      "log =  3.5797436655451773\n",
      "NNN SAMPLES!!! =  1\n",
      "loss , grand (prediction) =  3.5797436655451773 [[ 0.20603202  0.56005311 -0.97211716  0.20603202]] \n",
      "                                                                                                    \n",
      "grad_check at (0, 2)  num = -0.9721166132181657 anal = -0.9721166132139888\n",
      "enter of the function =  [[ 1.       2.      -1.       1.00002]] [[2]]\n",
      "soft max = [[0.20603106 0.56005049 0.02788327 0.20603518]]\n",
      "log =  3.5797283438783922\n",
      "NNN SAMPLES!!! =  1\n",
      "loss , grand (prediction) =  3.5797283438783922 [[ 0.20603106  0.56005049 -0.97211673  0.20603518]] \n",
      "                                                                                                    \n",
      "enter of the function =  [[ 1.       2.      -1.       0.99998]] [[2]]\n",
      "soft max = [[0.20603276 0.5600551  0.0278835  0.20602864]]\n",
      "log =  3.5797201026020242\n",
      "NNN SAMPLES!!! =  1\n",
      "loss , grand (prediction) =  3.5797201026020242 [[ 0.20603276  0.5600551  -0.9721165   0.20602864]] \n",
      "                                                                                                    \n",
      "grad_check at (0, 3)  num = 0.20603190920009948 anal = 0.20603190919001857\n",
      "Gradient check passed!\n",
      "predictions = [[ 2. -1. -1.  1.]\n",
      " [ 0.  1.  1.  1.]\n",
      " [ 1.  2. -1.  2.]]\n",
      "targets =  [[3]\n",
      " [3]\n",
      " [2]]\n",
      "enter of the function =  [[ 2. -1. -1.  1.]  [[3] \n",
      "                          [ 0.  1.  1.  1.]   [3] \n",
      "                          [ 1.  2. -1.  2.]]  [2]]\n",
      "soft max = [[0.19515646 0.00971627 0.00971627 0.07179405] \n",
      "            [0.02641156 0.07179405 0.07179405 0.07179405] \n",
      "            [0.07179405 0.19515646 0.00971627 0.19515646]]\n",
      "log =  9.901861007689417\n",
      "NNN SAMPLES!!! =  3\n",
      "loss , grand (prediction) =  9.901861007689417 [[ 0.06505215  0.00323876 -0.33009458 -0.30940198]  \n",
      "                                                [ 0.00880385  0.02393135 -0.30940198 -0.30940198]  \n",
      "                                                [ 0.02393135  0.06505215 -0.33009458 -0.26828118]] \n",
      "enter of the function =  [[ 2.00002 -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.1951596  0.00971623 0.00971623 0.07179377] \n",
      "            [0.02641145 0.07179377 0.07179377 0.07179377] \n",
      "            [0.07179377 0.1951557  0.00971623 0.1951557 ]]\n",
      "log =  9.901872717171424\n",
      "NNN SAMPLES!!! =  3\n",
      "loss , grand (prediction) =  9.901872717171424 [[ 0.0650532   0.00323874 -0.33009459 -0.30940208]  \n",
      "                                                [ 0.00880382  0.02393126 -0.30940208 -0.30940208]  \n",
      "                                                [ 0.02393126  0.0650519  -0.33009459 -0.26828143]] \n",
      "enter of the function =  [[ 1.99998 -1.      -1.       1.     ]  [[3] \n",
      "                          [ 0.       1.       1.       1.     ]   [3] \n",
      "                          [ 1.       2.      -1.       2.     ]]  [2]]\n",
      "soft max = [[0.19515332 0.00971631 0.00971631 0.07179433] \n",
      "            [0.02641166 0.07179433 0.07179433 0.07179433] \n",
      "            [0.07179433 0.19515722 0.00971631 0.19515722]]\n",
      "log =  9.901849298395895\n",
      "NNN SAMPLES!!! =  3\n",
      "loss , grand (prediction) =  9.901849298395895 [[ 0.06505111  0.00323877 -0.33009456 -0.30940189]  \n",
      "                                                [ 0.00880389  0.02393144 -0.30940189 -0.30940189]  \n",
      "                                                [ 0.02393144  0.06505241 -0.33009456 -0.26828093]] \n",
      "grad_check at (0, 0)  num = 0.5854693882145767 anal = 0.0650521542396471\n",
      "Gradients are different at (0, 0). Analytic: 0.06505, Numeric: 0.58547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "pp('predictions =',predictions)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "pp('targets = ',target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "print('predictions =',predictions)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "print('targets = ',target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of linear_softmax [[-1. -1.  1.]  [[ 1.  2.]  [1 1]\n",
      "                         [ 0.  1.  1.]]  [-1.  1.]       \n",
      "                                         [ 1.  2.]]      \n",
      "soft max = [[0.11245721 0.01521943] \n",
      "            [0.0413707  0.83095266]]\n",
      "log =  4.370364905207625\n",
      "loss , grand (prediction), grad by W =  4.370364905207625 [[0.11245721 0.01521943]  [[-0.11245721 -0.01521943]  \n",
      "                                                           [0.0413707  0.83095266]]  [-0.07108652  0.81573323]  \n",
      "                                                                                     [ 0.15382791  0.84617209]] \n",
      "[[-0.11245721 -0.01521943] \n",
      " [-0.07108652  0.81573323] \n",
      " [ 0.15382791  0.84617209]]\n",
      "enter of linear_softmax [[-1. -1.  1.]  [[ 1.  2.]  [1 1]\n",
      "                         [ 0.  1.  1.]]  [-1.  1.]       \n",
      "                                         [ 1.  2.]]      \n",
      "soft max = [[0.11245721 0.01521943] \n",
      "            [0.0413707  0.83095266]]\n",
      "log =  4.370364905207625\n",
      "loss , grand (prediction), grad by W =  4.370364905207625 [[0.11245721 0.01521943]  [[-0.11245721 -0.01521943]  \n",
      "                                                           [0.0413707  0.83095266]]  [-0.07108652  0.81573323]  \n",
      "                                                                                     [ 0.15382791  0.84617209]] \n",
      "enter of linear_softmax [[-1. -1.  1.]  [[ 1.00002  2.     ]  [1 1]\n",
      "                         [ 0.  1.  1.]]  [-1.       1.     ]       \n",
      "                                         [ 1.       2.     ]]      \n",
      "soft max = [[0.11245522 0.01521946] \n",
      "            [0.04137079 0.83095453]]\n",
      "log =  4.370360406959003\n",
      "loss , grand (prediction), grad by W =  4.370360406959003 [[0.11245522 0.01521946]  [[-0.11245522 -0.01521946]  \n",
      "                                                           [0.04137079 0.83095453]]  [-0.07108443  0.81573507]  \n",
      "                                                                                     [ 0.15382601  0.84617399]] \n",
      "enter of linear_softmax [[-1. -1.  1.]  [[ 0.99998  2.     ]  [1 1]\n",
      "                         [ 0.  1.  1.]]  [-1.       1.     ]       \n",
      "                                         [ 1.       2.     ]]      \n",
      "soft max = [[0.11245921 0.01521939] \n",
      "            [0.0413706  0.83095079]]\n",
      "log =  4.370369403536097\n",
      "loss , grand (prediction), grad by W =  4.370369403536097 [[0.11245921 0.01521939]  [[-0.11245921 -0.01521939]  \n",
      "                                                           [0.0413706  0.83095079]]  [-0.07108861  0.8157314 ]  \n",
      "                                                                                     [ 0.15382981  0.84617019]] \n",
      "grad_check at (0, 0)  num = -0.22491442734917652 anal = -0.11245721367093255\n",
      "Gradients are different at (0, 0). Analytic: -0.11246, Numeric: -0.22491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "pp(dW)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of L2  [[ 1.  2.]  0.01\n",
      "              [-1.  1.]      \n",
      "              [ 1.  2.]]     \n",
      "L2 loss, grad =  0.12 [[ 0.01  0.02] \n",
      "                       [-0.01  0.01] \n",
      "                       [ 0.01  0.02]]\n",
      "enter of L2  [[ 1.  2.]  0.01\n",
      "              [-1.  1.]      \n",
      "              [ 1.  2.]]     \n",
      "L2 loss, grad =  0.12 [[ 0.01  0.02] \n",
      "                       [-0.01  0.01] \n",
      "                       [ 0.01  0.02]]\n",
      "enter of L2  [[ 1.0000002  2.       ]  0.01\n",
      "              [-1.         1.       ]      \n",
      "              [ 1.         2.       ]]     \n",
      "L2 loss, grad =  0.12000000400000041 [[ 0.01  0.02] \n",
      "                                      [-0.01  0.01] \n",
      "                                      [ 0.01  0.02]]\n",
      "enter of L2  [[ 0.9999998  2.       ]  0.01\n",
      "              [-1.         1.       ]      \n",
      "              [ 1.         2.       ]]     \n",
      "L2 loss, grad =  0.1199999960000004 [[ 0.01  0.02] \n",
      "                                     [-0.01  0.01] \n",
      "                                     [ 0.01  0.02]]\n",
      "grad_check at (0, 0)  num = 0.020000000024167353 anal = 0.01\n",
      "Gradients are different at (0, 0). Analytic: 0.01000, Numeric: 0.02000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter of the function =  [[ 0.00465178 -0.0065898   0.01138863 ...  0.00539442  0.02523579 [5 2 2 ... 1 2 4]\n",
      "                           -0.0100008 ]                                                                     \n",
      "                          [-0.00176964  0.00404971  0.00156948 ... -0.00666434 -0.00115764                  \n",
      "                            0.00126712]                                                                     \n",
      "                          [-0.00240734  0.00169291  0.01001695 ... -0.00460786 -0.00425175                  \n",
      "                            0.00338654]                                                                     \n",
      "                          ...                                                                               \n",
      "                          [ 0.00169839 -0.00535952  0.02034442 ...  0.00763008  0.01760759                  \n",
      "                           -0.00889147]                                                                     \n",
      "                          [ 0.00161934  0.00548216 -0.00185972 ...  0.00388415  0.01365765                  \n",
      "                           -0.00044193]                                                                     \n",
      "                          [-0.0285787   0.01798424 -0.01144401 ... -0.00628576 -0.05089424                  \n",
      "                            0.00536954]]                                                                    \n",
      "soft max = [[1.11670533e-05 1.10422210e-05 1.12425381e-05 ... 1.11753495e-05\n",
      "             1.13992981e-05 1.10046202e-05]                                 \n",
      "            [1.10955747e-05 1.11603320e-05 1.11326862e-05 ... 1.10413979e-05\n",
      "             1.11023673e-05 1.11293206e-05]                                 \n",
      "            [1.10885014e-05 1.11340603e-05 1.12271275e-05 ... 1.10641277e-05\n",
      "             1.10680684e-05 1.11529333e-05]                                 \n",
      "            ...                                                             \n",
      "            [1.11341213e-05 1.10558144e-05 1.13436761e-05 ... 1.12003617e-05\n",
      "             1.13126729e-05 1.10168346e-05]                                 \n",
      "            [1.11332412e-05 1.11763300e-05 1.10945752e-05 ... 1.11584844e-05\n",
      "             1.12680766e-05 1.11103163e-05]                                 \n",
      "            [1.08020647e-05 1.13169345e-05 1.09887496e-05 ... 1.10455787e-05\n",
      "             1.05636805e-05 1.11750715e-05]]                                \n",
      "log =  102666.1607642957\n",
      "loss , grand (prediction) =  102666.1607642957 [[ 1.11670533e-05  1.10422210e-05  1.12425381e-05 ...  1.11753495e-05 \n",
      "                                                  1.13992981e-05  1.10046202e-05]                                    \n",
      "                                                [ 1.10955747e-05  1.11603320e-05 -9.99988867e-01 ...  1.10413979e-05 \n",
      "                                                  1.11023673e-05  1.11293206e-05]                                    \n",
      "                                                [ 1.10885014e-05  1.11340603e-05 -9.99988773e-01 ...  1.10641277e-05 \n",
      "                                                  1.10680684e-05  1.11529333e-05]                                    \n",
      "                                                ...                                                                  \n",
      "                                                [ 1.11341213e-05 -9.99988944e-01  1.13436761e-05 ...  1.12003617e-05 \n",
      "                                                  1.13126729e-05  1.10168346e-05]                                    \n",
      "                                                [ 1.11332412e-05  1.11763300e-05 -9.99988905e-01 ...  1.11584844e-05 \n",
      "                                                  1.12680766e-05  1.11103163e-05]                                    \n",
      "                                                [ 1.08020647e-05  1.13169345e-05  1.09887496e-05 ...  1.10455787e-05 \n",
      "                                                  1.05636805e-05  1.11750715e-05]]                                   \n",
      "Epoch 0, loss: 102666.160764\n",
      "enter of the function =  [[ -2.59504292 -12.89288808 -28.99616728 ...  -9.74129154  -1.13502429 [2 2 6 ... 4 2 2]\n",
      "                            -4.9949002 ]                                                                         \n",
      "                          [-11.55115704 -21.71633019 -13.45774971 ...  -2.860797   -10.19794941                  \n",
      "                            -7.4409339 ]                                                                         \n",
      "                          [ -3.67204887  -0.83179805 -11.35399126 ... -27.68221136   6.33068009                  \n",
      "                             5.42458751]                                                                         \n",
      "                          ...                                                                                    \n",
      "                          [ -2.1246899    3.97615267 -11.84562447 ... -32.20600658   3.94137502                  \n",
      "                             7.50460946]                                                                         \n",
      "                          [  3.23075547  -1.432866   -13.65156549 ... -24.56588002   7.57375675                  \n",
      "                             3.26498417]                                                                         \n",
      "                          [-11.27661246 -21.86338468 -17.28868067 ...   0.10748467 -13.53741478                  \n",
      "                           -12.80942998]]                                                                        \n",
      "soft max = [[2.20732457e-64 7.43993035e-69 7.55098436e-76 ... 1.73896105e-67\n",
      "             9.50482739e-64 2.00272548e-65]                                 \n",
      "            [2.84626451e-68 1.09546032e-72 4.22914507e-69 ... 1.69219601e-64\n",
      "             1.10145272e-67 1.73509191e-66]                                 \n",
      "            [7.51845248e-65 1.28716349e-63 3.46659628e-68 ... 2.80961866e-75\n",
      "             1.66057482e-60 6.71038504e-61]                                 \n",
      "            ...                                                             \n",
      "            [3.53295346e-64 1.57652253e-61 2.12026272e-68 ... 3.04781130e-77\n",
      "             1.52263721e-61 5.37142477e-60]                                 \n",
      "            [7.48132102e-62 7.05656293e-64 3.48401053e-69 ... 6.33946902e-74\n",
      "             5.75598673e-60 7.74182991e-62]                                 \n",
      "            [3.74548836e-68 9.45652766e-73 9.17276787e-71 ... 3.29275197e-63\n",
      "             3.90530069e-69 8.08749942e-69]]                                \n",
      "log =  1418103.0433875346\n",
      "loss , grand (prediction) =  1418103.0433875346 [[ 2.20732457e-64  7.43993035e-69 -1.00000000e+00 ...  1.73896105e-67 \n",
      "                                                   9.50482739e-64  2.00272548e-65]                                    \n",
      "                                                 [ 2.84626451e-68  1.09546032e-72 -1.00000000e+00 ...  1.69219601e-64 \n",
      "                                                   1.10145272e-67  1.73509191e-66]                                    \n",
      "                                                 [ 7.51845248e-65  1.28716349e-63  3.46659628e-68 ...  2.80961866e-75 \n",
      "                                                   1.66057482e-60  6.71038504e-61]                                    \n",
      "                                                 ...                                                                  \n",
      "                                                 [ 3.53295346e-64  1.57652253e-61  2.12026272e-68 ...  3.04781130e-77 \n",
      "                                                   1.52263721e-61  5.37142477e-60]                                    \n",
      "                                                 [ 7.48132102e-62  7.05656293e-64 -1.00000000e+00 ...  6.33946902e-74 \n",
      "                                                   5.75598673e-60  7.74182991e-62]                                    \n",
      "                                                 [ 3.74548836e-68  9.45652766e-73 -1.00000000e+00 ...  3.29275197e-63 \n",
      "                                                   3.90530069e-69  8.08749942e-69]]                                   \n",
      "Epoch 1, loss: 1418103.043388\n",
      "enter of the function =  [[  3.0111742  -20.76848089 -45.82400298 ... -24.39461031  -8.26790183 [1 8 9 ... 1 5 9]\n",
      "                           -12.25494453]                                                                         \n",
      "                          [ -4.33944017 -80.17101774 -26.23905846 ...  -8.52127845  -1.13562026                  \n",
      "                           -11.61606058]                                                                         \n",
      "                          [ -4.95105853 -27.47820667 -30.18537209 ... -40.02632489  14.92914101                  \n",
      "                             7.16595557]                                                                         \n",
      "                          ...                                                                                    \n",
      "                          [-30.16761169 -75.92589607 -36.23657961 ...  31.29352049 -48.14133102                  \n",
      "                           -31.77492922]                                                                         \n",
      "                          [-15.50455613 -38.06246858 -15.92850699 ...  -2.17938558 -15.08643598                  \n",
      "                           -19.74470713]                                                                         \n",
      "                          [-23.81811518 -53.72199872 -32.31312852 ...  11.15603229 -43.27437609                  \n",
      "                           -42.49867921]]                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max = [[6.89471594e-128 3.24446722e-138 4.26253939e-149 ... 8.63645266e-140\n",
      "             8.71115816e-133 1.61631222e-134]                                   \n",
      "            [4.42777069e-131 5.16356238e-164 1.36553155e-140 ... 6.76138756e-133\n",
      "             1.09040147e-129 3.06188608e-134]                                   \n",
      "            [2.40194269e-131 3.95500022e-141 2.63900073e-142 ... 1.40464963e-146\n",
      "             1.03376923e-122 4.39455599e-126]                                   \n",
      "            ...                                                                 \n",
      "            [2.68628912e-142 3.60232046e-162 6.21489037e-145 ... 1.32246233e-115\n",
      "             4.20015798e-150 5.38398225e-143]                                   \n",
      "            [6.26955717e-136 1.00105968e-145 4.10314958e-136 ... 3.83959049e-130\n",
      "             9.52410428e-136 9.03155825e-138]                                   \n",
      "            [1.53710681e-139 1.58347948e-152 3.14316102e-143 ... 2.37565464e-124\n",
      "             5.45704200e-148 1.18532698e-147]]                                  \n",
      "log =  2907044.2615598976\n",
      "loss , grand (prediction) =  2907044.2615598976 [[ 6.89471594e-128 -1.00000000e+000  4.26253939e-149 ...  8.63645266e-140 \n",
      "                                                   8.71115816e-133  1.61631222e-134]                                      \n",
      "                                                 [ 4.42777069e-131  5.16356238e-164  1.36553155e-140 ...  6.76138756e-133 \n",
      "                                                  -1.00000000e+000  3.06188608e-134]                                      \n",
      "                                                 [ 2.40194269e-131  3.95500022e-141  2.63900073e-142 ...  1.40464963e-146 \n",
      "                                                   1.03376923e-122 -1.00000000e+000]                                      \n",
      "                                                 ...                                                                      \n",
      "                                                 [ 2.68628912e-142 -1.00000000e+000  6.21489037e-145 ...  1.32246233e-115 \n",
      "                                                   4.20015798e-150  5.38398225e-143]                                      \n",
      "                                                 [ 6.26955717e-136  1.00105968e-145  4.10314958e-136 ...  3.83959049e-130 \n",
      "                                                   9.52410428e-136  9.03155825e-138]                                      \n",
      "                                                 [ 1.53710681e-139  1.58347948e-152  3.14316102e-143 ...  2.37565464e-124 \n",
      "                                                   5.45704200e-148 -1.00000000e+000]]                                     \n",
      "Epoch 2, loss: 2907044.261560\n",
      "enter of the function =  [[  21.94051095   48.76202756  -73.18848232 ... -100.36755491 [3 7 3 ... 8 2 2]\n",
      "                             38.70623899   24.34646558]                                                 \n",
      "                          [  -0.57136865  -53.41057269  -83.19001157 ...  -22.48675078                  \n",
      "                             -0.20838372  -22.35837238]                                                 \n",
      "                          [ -14.74517162   14.29130271  -57.60281762 ...  -68.56470775                  \n",
      "                              7.41886718   20.74532668]                                                 \n",
      "                          ...                                                                           \n",
      "                          [ -26.66852403 -139.88084056  -69.75833951 ...   60.41741199                  \n",
      "                            -36.18806292  -41.30907734]                                                 \n",
      "                          [ -32.60633534  -34.13820157  -70.95835517 ...  -14.8884172                   \n",
      "                            -26.67852793  -22.71798493]                                                 \n",
      "                          [  -3.94166685  -38.1001778   -46.87944343 ...  -48.57823773                  \n",
      "                             17.2864211     6.26462487]]                                                \n",
      "soft max = [[1.03756717e-185 4.61798884e-174 5.03523073e-227 ... 7.91221586e-239\n",
      "             1.98280220e-178 1.15055939e-184]                                   \n",
      "            [1.73472698e-195 1.95639430e-218 2.28249803e-231 ... 5.26625159e-205\n",
      "             2.49386812e-195 5.98763940e-205]                                   \n",
      "            [1.21234636e-201 4.94318986e-189 2.95656581e-220 ... 5.12978491e-225\n",
      "             5.12090199e-192 3.14017815e-186]                                   \n",
      "            ...                                                                 \n",
      "            [8.04230531e-207 5.46885247e-256 1.55493224e-225 ... 5.32502955e-169\n",
      "             5.90333388e-211 3.52426894e-213]                                   \n",
      "            [2.12139661e-209 4.58501562e-210 4.68329259e-226 ... 1.05054812e-201\n",
      "             7.96225202e-207 4.17905309e-205]                                   \n",
      "            [5.96388449e-197 8.72321157e-212 1.34242425e-215 ... 2.45534655e-216\n",
      "             9.88031249e-188 1.61460167e-192]]                                  \n",
      "log =  inf\n",
      "loss , grand (prediction) =  inf [[ 1.03756717e-185  4.61798884e-174  5.03523073e-227 ...  7.91221586e-239 \n",
      "                                    1.98280220e-178  1.15055939e-184]                                      \n",
      "                                  [ 1.73472698e-195  1.95639430e-218  2.28249803e-231 ... -1.00000000e+000 \n",
      "                                    2.49386812e-195  5.98763940e-205]                                      \n",
      "                                  [ 1.21234636e-201  4.94318986e-189  2.95656581e-220 ...  5.12978491e-225 \n",
      "                                    5.12090199e-192  3.14017815e-186]                                      \n",
      "                                  ...                                                                      \n",
      "                                  [ 8.04230531e-207  5.46885247e-256  1.55493224e-225 ...  5.32502955e-169 \n",
      "                                   -1.00000000e+000  3.52426894e-213]                                      \n",
      "                                  [ 2.12139661e-209  4.58501562e-210 -1.00000000e+000 ...  1.05054812e-201 \n",
      "                                    7.96225202e-207  4.17905309e-205]                                      \n",
      "                                  [ 5.96388449e-197  8.72321157e-212 -1.00000000e+000 ...  2.45534655e-216 \n",
      "                                    9.88031249e-188  1.61460167e-192]]                                     \n",
      "Epoch 3, loss: inf\n",
      "enter of the function =  [[  16.14223407 -107.53194176 -108.03653401 ...  -19.80455817 [4 7 4 ... 3 6 0]\n",
      "                            -11.73670136  -35.44233049]                                                 \n",
      "                          [ -15.01736882   -3.07971438  -29.58986281 ...  -99.10500191                  \n",
      "                             34.64924636   26.45739076]                                                 \n",
      "                          [   3.02163681    0.18504531  -81.53540827 ... -102.14758162                  \n",
      "                             72.68730943   34.87780451]                                                 \n",
      "                          ...                                                                           \n",
      "                          [ -21.35571817  -58.66018848  -55.83698004 ...  -46.3075079                   \n",
      "                            -20.27934065  -11.3773545 ]                                                 \n",
      "                          [ -24.50586219  -88.0313437   -58.64773185 ...  -15.38296423                  \n",
      "                            -39.90896434  -33.80349124]                                                 \n",
      "                          [ -15.29682509  -87.93761548  -59.23745682 ...  -99.65519472                  \n",
      "                             35.64293856    4.34893833]]                                                \n",
      "soft max = [[2.69500692e-254 5.24261263e-308 3.16523630e-308 ... 6.59274647e-270\n",
      "             2.10325566e-266 1.06578142e-276]                                   \n",
      "            [7.90890378e-268 1.20941052e-262 3.70990308e-274 ... 2.39508673e-304\n",
      "             2.93800952e-246 8.13533640e-250]                                   \n",
      "            [5.39954164e-260 3.16548937e-261 1.02258357e-296 ... 1.14273536e-305\n",
      "             9.72241357e-230 3.69245103e-246]                                   \n",
      "            ...                                                                 \n",
      "            [1.39767483e-270 8.79590514e-287 1.48041810e-285 ... 2.03695554e-281\n",
      "             4.10082241e-270 3.01268984e-266]                                   \n",
      "            [5.98847137e-272 1.54365377e-299 8.90615777e-287 ... 5.48706649e-268\n",
      "             1.22414739e-278 5.48791007e-276]                                   \n",
      "            [5.98067184e-268 1.69533507e-299 4.93828422e-287 ... 1.38157842e-304\n",
      "             7.93612021e-246 2.03608836e-259]]                                  \n",
      "log =  inf\n",
      "loss , grand (prediction) =  inf [[ 2.69500692e-254  5.24261263e-308  3.16523630e-308 ...  6.59274647e-270 \n",
      "                                    2.10325566e-266  1.06578142e-276]                                      \n",
      "                                  [ 7.90890378e-268  1.20941052e-262  3.70990308e-274 ... -1.00000000e+000 \n",
      "                                    2.93800952e-246  8.13533640e-250]                                      \n",
      "                                  [ 5.39954164e-260  3.16548937e-261  1.02258357e-296 ...  1.14273536e-305 \n",
      "                                    9.72241357e-230  3.69245103e-246]                                      \n",
      "                                  ...                                                                      \n",
      "                                  [ 1.39767483e-270  8.79590514e-287  1.48041810e-285 ...  2.03695554e-281 \n",
      "                                    4.10082241e-270  3.01268984e-266]                                      \n",
      "                                  [ 5.98847137e-272  1.54365377e-299  8.90615777e-287 ...  5.48706649e-268 \n",
      "                                    1.22414739e-278  5.48791007e-276]                                      \n",
      "                                  [-1.00000000e+000  1.69533507e-299  4.93828422e-287 ...  1.38157842e-304 \n",
      "                                    7.93612021e-246  2.03608836e-259]]                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss: inf\n",
      "enter of the function =  [[-127.30013118 -311.91408094  -76.22914074 ...  212.87600174 [0 2 5 ... 6 4 2]\n",
      "                           -242.79663494 -179.29024915]                                                 \n",
      "                          [ -19.62587674  -69.00228905  -89.356754   ...  -51.44384053                  \n",
      "                             -7.55539693  -35.36141146]                                                 \n",
      "                          [ -15.99911157  -37.3964249   -46.7857141  ...  -76.37230043                  \n",
      "                            -13.62952456  -13.99424619]                                                 \n",
      "                          ...                                                                           \n",
      "                          [ -74.19853874  -96.60787673  -34.30116628 ...   -5.02162078                  \n",
      "                           -117.69400058  -79.86490761]                                                 \n",
      "                          [  15.9669023   -23.10327731  -74.48113941 ... -118.54872148                  \n",
      "                             59.49274401   31.03323298]                                                 \n",
      "                          [ -59.74133073  -92.50522273  -92.70586468 ...  -71.98641278                  \n",
      "                            -41.27278703  -17.23082691]]                                                \n",
      "soft max = [[0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 6.25921146e-235\n",
      "             0.00000000e+000 0.00000000e+000]                                   \n",
      "            [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "             0.00000000e+000 0.00000000e+000]                                   \n",
      "            [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "             0.00000000e+000 0.00000000e+000]                                   \n",
      "            ...                                                                 \n",
      "            [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "             0.00000000e+000 0.00000000e+000]                                   \n",
      "            [1.90511713e-320 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "             1.52411303e-301 6.65592743e-314]                                   \n",
      "            [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "             0.00000000e+000 0.00000000e+000]]                                  \n",
      "log =  inf\n",
      "loss , grand (prediction) =  inf [[-1.00000000e+000  0.00000000e+000  0.00000000e+000 ...  6.25921146e-235 \n",
      "                                    0.00000000e+000  0.00000000e+000]                                      \n",
      "                                  [ 0.00000000e+000  0.00000000e+000 -1.00000000e+000 ...  0.00000000e+000 \n",
      "                                    0.00000000e+000  0.00000000e+000]                                      \n",
      "                                  [ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...  0.00000000e+000 \n",
      "                                    0.00000000e+000  0.00000000e+000]                                      \n",
      "                                  ...                                                                      \n",
      "                                  [ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...  0.00000000e+000 \n",
      "                                    0.00000000e+000  0.00000000e+000]                                      \n",
      "                                  [ 1.90511713e-320  0.00000000e+000  0.00000000e+000 ...  0.00000000e+000 \n",
      "                                    1.52411303e-301  6.65592743e-314]                                      \n",
      "                                  [ 0.00000000e+000  0.00000000e+000 -1.00000000e+000 ...  0.00000000e+000 \n",
      "                                    0.00000000e+000  0.00000000e+000]]                                     \n",
      "Epoch 5, loss: inf\n",
      "enter of the function =  [[  50.93856772   21.78837834  -84.0388259  ... -216.60843339 [3 4 1 ... 4 1 1]\n",
      "                            118.07326973   68.82734783]                                                 \n",
      "                          [ -75.58797656 -122.83782102  -51.40068834 ...  -75.70367666                  \n",
      "                            -14.17372553   -3.88578161]                                                 \n",
      "                          [ -52.31399073 -303.45492127 -112.9746942  ...   73.89771654                  \n",
      "                            -78.06396595  -78.16327982]                                                 \n",
      "                          ...                                                                           \n",
      "                          [ -85.04745953 -253.8273546   -31.08721944 ...    3.53744899                  \n",
      "                            -27.04610005  -21.37568341]                                                 \n",
      "                          [   4.2014608   -90.77717808  -79.12626286 ... -171.90157607                  \n",
      "                            103.09612402   39.45513313]                                                 \n",
      "                          [  -6.76567301  -44.06072077  -78.93951581 ... -125.83560247                  \n",
      "                             45.95502068   11.00741593]]                                                \n",
      "soft max = [[0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            ...                     \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.]]\n",
      "log =  inf\n",
      "loss , grand (prediction) =  inf [[ 0.  0.  0. ...  0.  0.  0.]  \n",
      "                                  [ 0.  0.  0. ...  0.  0.  0.]  \n",
      "                                  [ 0. -1.  0. ...  0.  0.  0.]  \n",
      "                                  ...                            \n",
      "                                  [ 0.  0.  0. ...  0.  0.  0.]  \n",
      "                                  [ 0. -1.  0. ...  0.  0.  0.]  \n",
      "                                  [ 0. -1.  0. ...  0.  0.  0.]] \n",
      "Epoch 6, loss: inf\n",
      "enter of the function =  [[ -92.90453954 -229.16607554 -197.4663998  ...  117.42642993 [2 5 8 ... 3 5 2]\n",
      "                           -159.95760219 -146.40306943]                                                 \n",
      "                          [ -42.28366575  -92.40852605 -102.55751151 ... -125.0266993                   \n",
      "                            -25.1408687   -13.69359446]                                                 \n",
      "                          [-162.31651935 -383.97203978 -115.40065766 ...  221.44768985                  \n",
      "                           -252.10188943 -202.44815402]                                                 \n",
      "                          ...                                                                           \n",
      "                          [ -50.60207547 -185.35112639    5.94851625 ...  -34.00721337                  \n",
      "                            -28.38600799  -45.62589562]                                                 \n",
      "                          [  28.12032731    8.41419021 -152.62827428 ... -137.29166185                  \n",
      "                              5.93662008   -4.25882421]                                                 \n",
      "                          [  42.18017256 -200.46922489  -18.81411893 ... -198.26402361                  \n",
      "                            136.31108209   72.40540545]]                                                \n",
      "soft max = [[0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            ...                     \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.]]\n",
      "log =  inf\n",
      "loss , grand (prediction) =  inf [[ 0.  0. -1. ...  0.  0.  0.]  \n",
      "                                  [ 0.  0.  0. ...  0.  0.  0.]  \n",
      "                                  [ 0.  0.  0. ...  0. -1.  0.]  \n",
      "                                  ...                            \n",
      "                                  [ 0.  0.  0. ...  0.  0.  0.]  \n",
      "                                  [ 0.  0.  0. ...  0.  0.  0.]  \n",
      "                                  [ 0.  0. -1. ...  0.  0.  0.]] \n",
      "Epoch 7, loss: inf\n",
      "enter of the function =  [[-149.78275392 -449.47060559 -127.73262059 ...  200.30877167 [0 0 2 ... 2 1 4]\n",
      "                           -192.51354342 -161.63694049]                                                 \n",
      "                          [-108.43688843 -207.34497383  -45.73156937 ...  -71.27205034                  \n",
      "                            -98.76318998  -42.78434313]                                                 \n",
      "                          [-129.15497728 -301.02349843 -135.08684784 ...  162.89827195                  \n",
      "                           -238.43295365 -162.34468207]                                                 \n",
      "                          ...                                                                           \n",
      "                          [  53.64518076  106.92034971 -154.64827936 ... -337.68110618                  \n",
      "                            198.24649215  125.28764523]                                                 \n",
      "                          [ -84.72424184 -409.90998192   36.99977263 ...   88.50137691                  \n",
      "                            -74.75986347 -117.50238974]                                                 \n",
      "                          [-105.68129739 -319.46994209  -77.56292195 ...   49.07415033                  \n",
      "                           -200.8757478  -136.51337844]]                                                \n",
      "soft max = [[0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            ...                     \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.]]\n",
      "log =  inf\n",
      "loss , grand (prediction) =  inf [[-1.  0.  0. ...  0.  0.  0.]  \n",
      "                                  [-1.  0.  0. ...  0.  0.  0.]  \n",
      "                                  [ 0.  0. -1. ...  0.  0.  0.]  \n",
      "                                  ...                            \n",
      "                                  [ 0.  0. -1. ...  0.  0.  0.]  \n",
      "                                  [ 0. -1.  0. ...  0.  0.  0.]  \n",
      "                                  [ 0.  0.  0. ...  0.  0.  0.]] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: inf\n",
      "enter of the function =  [[  75.12422054   91.99483269 -141.94011188 ... -425.95093671 [1 7 5 ... 0 5 1]\n",
      "                            276.37235804  165.3765982 ]                                                 \n",
      "                          [  39.05173559  -15.46208947 -244.58431126 ... -224.56346362                  \n",
      "                            107.70954726   41.1122469 ]                                                 \n",
      "                          [-110.42845617 -289.09229006 -111.81918736 ...  111.90119516                  \n",
      "                           -185.565825   -141.55318464]                                                 \n",
      "                          ...                                                                           \n",
      "                          [ -89.41506661 -205.84144885 -182.77571268 ...  -27.49924338                  \n",
      "                           -101.63232073 -101.57556045]                                                 \n",
      "                          [-158.38714279 -389.13893359 -168.83867801 ...  226.80016675                  \n",
      "                           -313.74540533 -214.66164455]                                                 \n",
      "                          [  26.07299391 -165.87535361 -191.902086   ... -104.87052956                  \n",
      "                             64.84183124    7.26339301]]                                                \n",
      "soft max = [[0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            ...                     \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.] \n",
      "            [0. 0. 0. ... 0. 0. 0.]]\n",
      "log =  inf\n",
      "loss , grand (prediction) =  inf [[ 0. -1.  0. ...  0.  0.  0.]  \n",
      "                                  [ 0.  0.  0. ... -1.  0.  0.]  \n",
      "                                  [ 0.  0.  0. ...  0.  0.  0.]  \n",
      "                                  ...                            \n",
      "                                  [-1.  0.  0. ...  0.  0.  0.]  \n",
      "                                  [ 0.  0.  0. ...  0.  0.  0.]  \n",
      "                                  [ 0. -1.  0. ...  0.  0.  0.]] \n",
      "Epoch 9, loss: inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'THERE WERE 10 EPOCHS   !!!!!\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function \n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "'''THERE WERE 10 EPOCHS   !!!!!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102666.1607642957, 1418103.0433875346, 2907044.2615598976, inf, inf, inf, inf, inf, inf, inf]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21b64f98>]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "print(loss_history)\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.16.4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
